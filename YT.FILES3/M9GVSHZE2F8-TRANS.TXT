{
  "transcript": "[Music] oh [Music] [Music] [Music] [Music] [Music] welcome to mongodb TV we've got some really great content lined up for you today normally we only have one live stream on Tuesdays but today we have a special double feature first we're going to kick off a monthlong challenge and that is going to be centered around drum roll can you guess artificial intelligence I know we hear all about AI this and AI that uh but AI is really changing how we work how we build applications and how we use applications so it's super important for us to understand these challenges and and uh and use cases Etc so that's what we're going to talk about first and then at 1M Eastern Ana rice andani is going to be hosting the manga be podcast live and she'll be joined by the CEO of nurel shr sha and they'll discuss uh how his product was built and have a cool demo to show so be sure to stick around for that one as well now before we get into the main material I want to let you know about mongodb dolo NC so if you're in the mongodb or if you're in the New York area join us on May the 2nd and you'll hear a bunch of uh new announcements you'll learn even more about how mongodb is revolutionizing the AI industry uh so use the coupon code Community 50 to get 50% off your tickets uh and if you're not uh able to join us in person uh we will be live streaming the keynote as well as uh the rest of the day we'll be live streaming the uh some interviews with he speakers Engineers partners and customers so make sure that you don't miss out on that as well so let's move on to the main material today artificial intelligence um it's just a fad right it's probably just going to blow over like a blockchain um well actually I don't think so it's uh it's a revolution revolutionary change and it's it's helping businesses solve real problems and making employees in individuals even more productive uh so mongodb is a document database so how does that fit into AI right so it is actually it's a it's a document database it's also a Time series database it's many other types of databases actually including a vector database and so that is what sets manga to be aart from many of the other Vector databases out there uh one of of the main differentiators is the fact that you can store your operational data along with your vector data so there's no need to sync data between separate databases and this eliminates a lot of the overhead that comes with many of the bolt-on solutions that you see out there um and if you're wondering who I am I'm Jesse Hall senior developer Advocate at mongodb and so throughout this stream we're going to talk about the demand for intelligent apps practical use cases limitations of llms overcoming those limit ations some of the tech that we can use to build a smart app and how to integrate GPT make it smart and optimize for the user experience so this is all in preparation uh for our rag to riches challenge that we're going to be running all month long that's this month the month of April 2024 there is a link in the video description so after this stream uh you'll be able to go through this Quest and build your first AI chatbot now this Quest is powered by wico we've teamed up with Wilco because they build amazing Interactive Learning courses so I took this course and it was fun and engaging a very unique way to learn so highly recommend after this to go check that out um but why should you complete this Quest well for one it's free uh you'll learn about Ai and Vector search with mongodb two you'll uh get a certificate of completion and three uh by completing this Quest this month you'll get 25% off the manga be associate developer certification exam so with this exam you can become manga Tob certified and on top of that you can get another 50% off of that exam um if you take off free manga to be no. JS developer path so by completing the rag to riches Quest and the node.js developer learning path you'll get a total of 75% off of that exam so um you might be asking why nodejs well it's true that a lot of AI developers use python and uh that's great for data analysis but when it comes to integrating AI into your applications especially web applications you might find it easier to have your entire stack written in JavaScript or typescript so our rag to Rich's Challenge and our open- source chatbot framework which we're going to talk about a little bit later are nodejs based and so let's get into preparing you for these the rest of this stream is going to be an introduction to Ai and rag retrieval augmented generation why it's essential uh to a great user experience in your applications so there is a huge demand for building intelligence into your applications in order to make these uh modern highly engaging applications and making uh differentiating experiences for each of our users so you could use it for fraud detection for chatbots uh personalized recommendations and so on uh to compete and win we need to make uh our applications smarter and surface insights faster the smarter our apps are uh they these smarter apps they use AI powerered models to take action autonomously for our users um and then the results are twofold uh first your apps will drive competitive Advantage by deepening user engagement and satisfaction as they interact with your application and secondly your apps will unlock higher efficiency and profitability by making Intelligent Decisions faster on fresher more accurate data so most almost every application going forward is going to use AI in some capacity and so in order to stay competitive uh we need to build intelligence into our applications in order to gain these rich insights from our data uh so AI is being used for both uh powering the uh the user-facing aspect and uh utilized on those fresh insites the fresh data on the back end as well uh to help us to make Better Business decisions so there are a lot of use cases um and here here's just a few here retail Healthcare Finance manufacturing uh although these are very different use cases they're all unified uh in their critical need to work on the freshest data in order to achieve their objectives in real time uh they all consist of AI powered apps that drive the user facing experience and predictive insights making use again of that fresh data to automate and drive more efficient business business processes uh but let's take a look at the the evolution of AI where did where did we come from and how did we get here so we started out with an analytics and so in the early days of computing applications primarily relied on analytics to make sense of data now this involved analyzing large data sets and extracting insights that um could inform business decisions and uh as computing power increased it became easier to analyze larger data sets in less time and so next became batch AI so as computing power continued to increase the focus shifted uh towards machine learning traditional uh batch machine learning involves training models on historic data and using them to make uh predictions or inferences about future events about how your users might interact in the future so the more data over time that you feed your model the better it gets and the more you can tune it and the more accurate these these future predictions become and so as you can imagine this is really powerful because you can uh predict what's going to happen tomorrow if we can do that we can make some really great business decisions today and so batch AI as the name implies is usually run offline on a schedule and so it's analyzing historic data to make predictions about future events and that is kind of where the problem is with this specific um type of AI it's working on historic data and it can't react react to events that happen quickly in real time now this could still be beneficial for certain industries and certain use cases um but we need data on things that are happening now uh and so this is where realtime AI comes in so real-time AI represents a significant step forward this approach involves training models on live data and using them to make predictions or inferences in real time now this is particularly useful for uh fraud detection for instance where decisions need to be made quickly based on what's happening in real time uh so I mean what good is fraud detection if the person defrauding you has already gotten away with it right and so next up is generative Ai and so uh generative AI really represents The Cutting Edge U as we know it today and this could change tomorrow who knows but this approach involves training models to generate new content this could be images text music video and it's not simply predicting uh future future events anymore it's creating the future and the fun fun fact here is all these images here were created using Dolly and so over the years we've seen AI evolve from analytics to real-time machine learning and now generative Ai and these are not incremental changes each one of these were transformative in their own way they shape how we interact with uh technology every single day so you've probably heard of something called generative pre-trained Transformers or gpts these are large language models that perform a variety of Tas tasks from natural um language processing to content generation and even some elements of Common Sense reasoning uh they are kind of like the brains that make our applications smarter uh but there's a catch to these These gpts are incredible but they aren't perfect uh one of their key limitations is their static knowledge based they only know what they've been trained on and there are some Integrations now with some models that can search the internet for newer in information but how do we know that that information that they're finding is on the Internet is accurate um they can hallucinate as well uh very confidently uh so how can we minimize this uh they can't access or learn from real-time proprietary data either your data and that's a big limitation don't you think the the need for realtime proprietary domain specific data is why we can't rely just on llms as they are uh and this is especially true in the business context where up-to-date information can be a GameChanger and so what's the solution here how do we make these models adaptable real time more aligned with our specific needs well this brings us to the focus of uh this stream today it's it's not merely about leveraging the power of gpts it's about uh taking your applications to the next level by making them intelligent and content or context aware uh we're going to explore how to augment applications with smarter capabilities using large language models and then boosting those capabilities even further with retrieval augmented generation or rag and so we're not just integrating AI into our applications we're optimizing it to be as smart and context aware as possible so what is involved in retrieval augmented generation first up vectors what are vectors so these are the building blocks that allow us to represent complex multi-dimensional data in a format that's easier to manipulate and understand so the simplest way to explain this is a vector is a numerical representation of data and even more simple it's an array of numbers so these numbers here a quick brown fox uh the embedding or the vectors of that is just an array of numbers and these numbers oops go forward these numbers are coordinates in an in dimensional space where n is the array length so however many numbers we have in the array is how many dimensions we have you also might hear vectors referred to as vector embeddings or just embeddings so here is a real life example of vectors in use so when you go to a store and you ask a worker where to find something many times they'll say go to Isis 30 Bay 15 so that is a two-dimensional Vector you'll also notice in the store that similar items will be placed near each other for ease of searching and finding uh for instance the light bulbs they're not scattered all over the store they're specifically placed to be easy to find another example is in games games use 2D and 3D coordinates where uh to to keep track of objects in a game's world and these coordinates can be used to compute proximity between objects for Collision detection for instance uh so the same kind of math is used to compute the similarity between vectors during Vector search and if you're a Stargate fan the gate addresses are made up of at least seven dimensions which are like vectors and to locate Stargates in other galaxies you add an eighth or a ninth Dimension and uh just like you would phone's area code or country code and so this really shows how adding Dimensions significantly increases the size of the virtual space in which our data is organized so again what makes vectors so special they enable semantic search in in simpler terms they let us find information that is contextually relevant and not just a keyword search and the data source is not limited to just text it can be be uh images video audio and these can all be converted to vectors and so how do we go about creating these vectors well this is done through an encoder the encoder defines how the information is organized in this virtual space there are different types of encoders that can be uh used to organize these vectors in different ways depending on your use case there are encoders for text for audio for images and so on and so the the mo the majority of the popular encoders uh are accessed through hugging phase open Ai and many others so now let's tie all of this back to retrieval augmented generation rag leverages vectors uh to pull in real-time context relevant data to augment the capabilities of the llm uh Vector search capabilities can augment performance of and accuracy of gpts uh by providing a memory or ground truth to reduce hallucination providing up-to-date information and allowing access to private data so first we we have our private data our custom data whatever that might be uh and we use or regenerate our embeddings using the embedding model and then store those embeddings in the vector database so again this data can be uh documents from our site blog articles videos images Etc PDFs and so uh in this example here we have Lang chain facilitating all of this it's very helpful um there are many other ways to go about this you could write it yourself but Lang chain is actually very helpful very useful and so uh we have that orchestrating everything here in this diagram so once we have our embeddings uh in our Vector database we're now uh able to accept user queries to find relevant information within our custom data so to do this we send the users natural language query to an llm which vectorizes their query and then we use vector search to find the information that's closely related semantically related to that user's query and then return those results uh and then we go uh we we can do anything that we want with those results we can summarize the answer to their question based on our Uh custom data we could respond with links to specific documentation pages and so on so imagine your application has an intelligent chatbot and with rag the and Vector embeddings this chatbot can pull in real-time data Maybe the product inventory and then offer it during a customer service interaction so with rag rag and Vector search or vector embeddings your application isn't just smart it's adaptable real time and Incredibly context aware because it has that extra information so now again uh you can uh add you can have you you have an advantage by using manga Tob because not only do you have your vector store but you also have your operational store uh with which has even more metadata that can be used to enrich your queries so this is something that we built at uh mangad with this same architecture architecture uh this is the mangad documentation website we have a mongodb AI chatbot that allows you to ask questions about mongodb and it only responds with answers from our own documentation and other internal resources so the question here is how do I get started with mongodb node.js and it answers us with a summary and some links to specific pages in the documentation and other resources to help us further uh we also have some more learnings if you want to uh do a deep dive into how we built this uh there is a uh podcast episode as well as a blog post from Ben pearlmutter um and this talks about how we actually built this chat bot you can check those out there's links in the video description below and so all of this has resulted in our open sourcing a chatbot framework that you can use in your next application so this framework comes with several pieces an ingest CLI which helps you to ingest whatever data uh you want to embed and store in your vector uh database uh it also comes with the chatbot server which is the back end it's basically the brains it it uh controls the vector search and the queries from your users and that is built on no JS and then the chatbot UI is the front end um and it's built in react so that accepts the the user queries from the front end and then we have an evaluation CLI in this framework as well for testing uh before you deploy to production so you can go check that out at GitHub mongodb chatbot uh again it's open source and you can use that in your next uh project uh but what is next so next go complete the rag to Rich's Quest and build your first AI chatbot powered by mongodb again there's a link in the description uh and then after that book your calendars April 18th Ben prom letter is going to be back on and uh we're going to have another live stream where he's going to walk us through using that open source chatbot framework in a real application and then we're going to go into some question and answers as well uh to help you out so again go back and complete that rag to witches rag to riches um Quest and um and then it it's very informative it's actually very fun quest to go through uh you're I think you'll definitely enjoy it um and so this is the the end of the slides here and I want to leave a little bit of time for some question and answer so let me go back and look at the chat and see if there are any uh questions um I'm not seeing any questions right off the bat so again go check out the rag to riches Rich's Quest that looks like there is a question that came in uh rather than doing the attribution at the end of the response how do llms do inline attribution um I think maybe you're talking about the rag uh architecture and so basically what we do is inject our custom Data before actually going to the llm so we do a vector search we find relevant data we uh enrich the the question then send it to the L it comes back and we'll learn a you'll learn a lot more about that through the rag to Rich's quest uh and then again on the 18th come back and we'll see it in real life we'll build an application and with with this architecture and leave some time for questions at the end of there as well all right yes thank you uh thank you Alexandro for that um I like analogies uh I try to make things simple and easy to understand all right so that's going to be the end of today's stream remember at 1M Eastern we have the manga toe podcast live with annah rice andani she has a special guest so be sure to come back for that um and we'll see you uh next time thank you all",
  "duration": 24,
  "comments": []
}
