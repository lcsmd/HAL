{
  "transcript": "if you want to build great AI agents you have to master prompt engineering and a lot of people are sleeping on prompt engineering I mean agents use llms or large language models as their brain what llm does predicts the next token by setting the system prompt you influence everything so there aren't many things more important than prompt engineering it's one of the most important skills to have in 2024 and 2025 in fact every big company right now is hiring prompt engineers and the salaries are really really generous with the right prompting you can automate virtually anything with AI agents so watch until the end and you will learn how now what even is prompt engineering it's the art of understanding the AI and knowing how to communicate with it you experiment with different phrases and words to get the best output possible you improve what the model can actually do for your specific task or use case just by changing a few words now zero shorting is bad this is the fundamental concept one of the most key Concepts in PR engineering zero shorting means no examples so provide examples when possible if you say give me five movies it will give some random garbage but if you give some you know recommendations of movies that you like the output will be way more personalized and you can apply this to any task Chain of Thought is another key prompt concept it's asking the llm to explain how it got an answer in other words telling it to work in steps like by the way if you just take this single prompt and put it after at the end of all of your problems in agents you will improve your agent instantly like it's it's really that powerful so here is an example of GPT 3.5 otherwise the free version of CH GPT if you give this a simple math problem with no coot or chain of fa it fails it cannot do a simple calculation right but with chain of fa it can do it the same llm the same AI model is suddenly much more useful self-consistency is another Concept in prom engineering first we give the model examples of questions and answers which is few shorting and together with these questions answers we also provide EXP explanations for how we got the solution and then the model tries several times to solve the problem you give it and the answer that comes up most often so you do multiple generations and the most often answer is considered a solution so here is example prompt right and by the way this is you can do this in a single prompt right the context window on the latest LMS is crazy and then at the very bottom you ask your final question and then you see the answer so here is an example of what the output might look like because we did more Generations like we're not that troubled by a wrong generation now if you did only one generation only one you know try of the llm of your agent you might get unlucky and get output free which is incorrect and then you might be like oh my God the llm sucks no your prom engineering sucks so here is example of a template right and you can take this for any application and use this right just use these templates I created and this will be linked below the module so if you want more prom templates just like this then make sure to join my community access to the smartest people at the cutting ede of AI complete guide on how to prepare for post AGI world how to build and deploy AI agent a full workshop on that and so much more so if that sounds interesting to you make sure to join it's the first link in the description now what is prompt chaining in other words chain of prompt it's using previous outputs for the next inputs so you're building up a conversation right coot is Chain of Thought cop is chain of prompt this is the very basic premise of AI agents and it's super important to understand and it can also help you with flow engineering but more on that later here's an example of prom chaining right let's say you want a summary of a document in this case you know I put in um a summary of our weekly calls we do in The Mastermind and you say summarize the three most important points in this document and the llm does that now you say what are the goals of the group based on these points and it gives you an answer and the difference here is that if you just said this directly it would give you an answer but it would take everything in inside like everything in the document a lot of which might not be relevant to to this description if you just say take the three most important points and based on that generate an answer it will be much more relevant now flow engineering this is a term that's becoming super popular over the last few weeks and I think honestly it's going to be the next evolution of promt engineering so it's basically designing the workflow of the agents you designed the diagram and then you build it with agents that is flow engineering so you decide which agent has which role you basically decide all of that you design all of that and that is flow engineering and if you combine that with great prompt engineering of every agent you'll be surprised of what you can do with agents here's a simple example of flow engineering from Microsoft's Auto autod def so you see the user on the top right then there are rules and actions and it goes into conversation manager there are multiple agents there's a parer goes in tools basically the workflow you design this you you know decide how to do this you can do this in paper on pen and paper you don't have to be fancy and you know once you have a workflow that makes sense you build it you test it you know maybe it works maybe it doesn't you adjust it but this is flow engineering tree of thought is another key concept it's similar to Chain of Thought but a bit more complex it's very powerful when combined with agents so here are you know this is beautiful illustrations of the the main uh prompting um Concepts right so on the very left this is like input output there is nothing you know no Advanced technique most of you probably are doing this when using chbd and there is nothing wrong with that but there are better ways right the level two is chain of fault you basically tell the llm to you know describe what it's doing then you have self-consistency with Chain of Thought So you generate multiple variations of these and then the majority vote the majority the most common output is the one that's that's decided that's chosen but tree of thought is for every step U the agent or the llm generates multiple options and you can then go back and see like which might have been better right so as you can see on the right it hit a dead end so It reversed and chose a different path and it achieved an output so this is why it payed to go deep when it comes to agents and PR engineering you can get way better results at complex tasks but it's also harder to implement so here are examples of like just how powerful Trio can be right and especially the middle middle chart you can see how it out outperforms uh just basic input output and a Chain of Thought like it it can be crazy if you done if you do it right now here are another two concept now these might sound intimidating but AR is tools PA is programs this is much easier to think about it like this so these are systems designed to help llms deal with complex tasks it essentially means allowing the llm to use external tools or programs to its Advantage this is like the difference between an llm and an agent basically now how does art and P work when the model reaches a point where a tool is needed let's say you know you have a language model running locally on your computer and you ask it for the weather well obviously it cannot do that it's a pre-trained model that's running locally on your computer doesn't have access to the internet but if you give it a tool that calls an API for weather it can achieve much more it can achieve things that normal llms cannot do so it calls to a tool and rewrites the prompt into format which the tool can use which is usually Json the tool gives it an answer and the answer is then processed by the llm and it gives you in natural English so you know if the API call returns uh Json that's not really readable the llm takes care of that and gives you plain English so where can we find this right well Paul is probably the most prevalent in ch gbd4 and at any time you use code interpreter you are using pal so it's doing something that normally the agent would the llm would suck at and when can we find Art well web GPT or perplexity are basically that right jgpt also use a gbt for vision to analyze images which is another example of tool usage honestly art is probably more widespread than pul but the concept is almost the same so now how can we utilize this in our o own agents well agents on their own cannot do a lot of tasks however with the power of tools the possibilities are insane Paul actually allows agents to have agency because if if your agent doesn't have agency then it's not really agent and that just means it can do things it can interact with the outside environment which in this case is the digital environment but soon enough with the advances in robotics my be the physical environment as well and art helps the agents with tasks that would otherwise be impossible due to the how llms work by generating the next token another concept is automatic promt engineer this isn't very useful for the average User it's more of advanced concept but it's super fascinating ape is used mostly by researchers to engineer the most effective prompts right so here is example right this is what I saying let's work is how in a step-by-step way to be sure we have the right answer produce the most uh accuracy and it it was uh found out by ape automatic promp engineer and as you can see it outperformed any human designed prompt so how does it work right first an agent is given a task for which it is supposed to find the best prompt another agent tests the prompts and judges the accuracy and based on the accuracy it creates a score for the proms and the best prom is chosen and this is repeated hundreds thousands tens of thousands of time probably millions of time let's be honest until you know the very best prompt is found this is another prompt engineering concept directional stimulus prompting this style of prompting is used to create summaries instead of doing a zero shot summary a person or agent gives the model A Hint this is usually not needed with the current llms the best ones only in specific scenarios and but you can use this to minimize API cost so let's say instead of using you know cloth free Opus use cloth free high cou and you give it a hint which is like what like 100 times cheaper than cus so yeah if you don't have a lot of money pay attention here's an example right the text at the top is the same for both situations but with directional stimulus prompting you give it a hint and then the output is much better as you can see accuracy of 48 versus 34 reflection this is super important in prom engineering it's one of the most effective ways to use llms and to improve their capability and intelligence so it combines a lot of the previous Concepts so make sure to understand the basics you know as Naval rikan says learn the fundamentals deeply now reflection is mostly used in agentic workflows you know it's like too complex to do with a single LM so the components of reflection you have an actor that generates outputs utilizes Chain of Thought react and long-term memory then you have an evaluator agent which gives a reward score to the actor based on how it performed and then you have a self-reflection agent which based on the output and score by the evaluator gives varable feedback to the actor which implements it again and the loop continues so here's a quick recap it's the basic concept to create something create it give feedback and create again I mean if you think about it this is how humans work how companies work the best use case for reflection are programming reasoning sequential decision making stuff that you know doing zero shot is bad and reflection is limited by memory but it performs great when utilized well and by memory you know you can think of as Conta window because there's only so much you can fit however context Windows have been growing massively over the last 6 months so it's only a matter of time before every llm has like 10 million token window and then we can just abuse reflection to the Finish you can see the increased performance just by using a bit of reflection as you can see the success rate is much higher when we use reflection and honestly this is just the beginning this was just the first little snippet first little sneak peek of my upcoming Advanced promt engineering Workshop which will be focused on agents but on llms as a whole so if that sounds interesting to you and if you want to learn how to build and deploy AI agents they make sure to join the community it's the first thing in description and honestly it's one of the best investments you can make see you inside",
  "duration": 12,
  "comments": []
}
