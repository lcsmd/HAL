{
  "transcript": "so one big tech company that's been sort of lagging on the AI front was Apple early this year it scrapped its autonomous electric car project and said that it's PIV 2 AI and there's been a lot of talk about what it could do improving Siri the M3 chips on device large language models etc etc but nothing too concrete nothing that we could quite syn our teeth into quite yet until today Apple researchers develop AI that can see and understand screen context another headline was Apple researchers achieve breakthroughs in multimodal AI as company wraps up Investments some people Haven even mentioned that this new model can beat GPT 4 and here's a paper that Apple published just a few days ago R reference resolution as language modeling and this paper that they published along with some of the other rumors that are floating in the AI rumor space got me from almost kind of dismiss in apple as a potential giant competitor in the AI space to all of a sudden being really interested if they are indeed building what I think they are building because what I think they're building is Agents they're skating to where the puck is going but let's rewind just a little bit here's the thing there are rumors that apple is in talks about potentially purchasing perplexity we've covered perplexity it's kind of a different approach to doing online web search right so so similar to how Google and Bing do it but with a large language model here's what that kind of looks like so let's say we ask it we're going to say Apple buying perplexity and we click search so it looks through all the various sources that are available online whether that's YouTube videos Reddit posts various new sources and then uses a large language model to answer that question for you by summarizing what it found so it's not sort of making stuff up it's not taking a stab at it it's not guessing it sort of reads over the sources for you and summarizes them now of course the answers are as good as the sources right so if the sources have the wrong information this will give you the wrong information but it's built to be similar to how scientific papers site their sources right so if you say we're building this paper based on the assumption that X and they link to a paper where they discuss why X is true right so they're sort of citing another paper to say this is why we believe it if you notice these little things here one and for example here's a good example so about perplexity AI right it tells you what it is and it gives you sources 2 4 5 Etc and the model that you use to sumarize it you can change it you can use Claude you can use gp4 you can use mix you can use pretty much whatever they're pretty good they're really fast at getting the latest models to be available for use if you wanted to switch now a lot of people are saying that this could potentially be the next iteration of how online search works so instead of typing in something into a search bar and then getting 10 Blue Links the way that we search online is this we can do image search video search we can even generate images if we wanted to if we needed our answers in a certain format like if you're doing a report perplexity has that built in you can create a custom search that organize that information into whatever subheadings or columns or however you want it it will organize it that way so when they're saying that there's rumors and speculation that apple is allegedly in talks to acquire perplexity AI however there's no definite confirmation of this potential acquisition so this I would say is a pretty good summary of what's happening there are rumors there's no confirmation of whether they're true or not but certainly something like this would make sense it would give Apple more power with Google because they would have basically their own search engine they also would not need to develop their own llm model they wouldn't have to necessarily build one from scratch it seems like they are working on it potentially but the big thing that a lot of other people are struggling with is the vision aspect the computer vision so similar to how GPT 4 with Vision Works or Gemini has their own version of vision I mean they're pretty good at recognizing images they make mistakes but when we try to use them as agents to navigate the web they're pretty bad everything that I've seen so far it's not anything that you would use to do any sort of complex tasks if you're taking a screenshot of this page for example and you're saying something like start a new search right maybe it might click on home and then click on here and start a new search or it might fail and if you give a complex task where multiple things have to be kind of strung together to achieve a certain outcome like go and do research on the best headphones and then check pricing on those headphones across various sites and then purchase those headphones right any mistake anywhere along the line means that it's not able to complete that task so even if it accomplishes 90% of those subtasks correctly right it kind of compounds to where it just it won't get to the f line so to speak and that's one area where a lot of different people are proposing different solutions that's one area where we need to have some progress some improvement to have very capable autonomous agents all right so that's one piece of the puzzle there are also rumors that potentially maybe Apple iOS could pull in anthropic to help them run some of the on device features some of the on device technology meaning that you would have a large language models like a GPT or Gemini inside the Apple device that you're using instead of having always be connected to the internet to the cloud that's also why kind of some people are saying that Apple could have an advantage because of the chips that they use the on device chips could run those on device images they could run those on device large language models why is on device so important isn't the phone or the computer isn't it always connected to the internet why do they need to be on device well that brings us to this paper that they just published this is uh end of March 2024 it's by Apple so Apple published this one this is the realm reference the resolution as language modeling this is the thing that some people are saying outperforms GPT 4 which while maybe not quite true it's certainly not hyperbole it's in the same ballpark but here's what I would be keeping in mind so imagine something like Siri a voice assistant that you talk to imagine something like this that's able to basically read all the text online and understand what it's looking at to be able to navigate effectively alongside with some on device large language model I'll come write out and say it what if in the next few years Apple devices all the Apple phones iPhones all the laptops all the stuff that they have what if they do in fact develop the best performing AI agents that run completely on device meaning that instead of clicking on things and typing things in instead of writing emails or adding things to your calendar or going to Amazon and doing shopping or any that you do like how you interact with any device instead of that you have your own AI agent it sounds like Siri you tell it what to do and Siri then goes and does it you to it hey email my friends see when they're available add it to my calendar then send me a text reminder when it's time to go to that event or whatever or do research for the best headphones go on Amazon and purchase them I mean we've seen this idea before there's multiple startups that are kind of going after that right now and they're all very exciting people are throwing tons of money at it but could Apple beat everybody to the punch and launch their own on device as an on Apple device autonomous AI agents certainly that would feel like it would give them a massive massive Edge in this sort of War for AI Supremacy but let's get started they start with saying basically it's important but quite hard to get these agents to understand what's happening on the user screen as well as on the various screens that are running in the background like if you have multiple apps open and they're saying while llms have been shown to be extremely powerful for a variety of tasks they're using reference resolution particularly for non-con conversational entities remains underutilized so what does this mean let's break it down just a little bit because this is kind of the the big point of all this basically what it means is this if let's say I show you this screen that you're looking at right now and I say what does the speaker want in table one right now it might take you a second to understand what I'm talking about but I feel like most people you know will figure out okay I'm talking about table one right so this is table one and so you always see okay speaker so speaker is either the user or the agent so you might translate that to mean okay so he's asking what does the user want right but the point is you'll kind of figure out that this is what I'm talking about right with the vision models of GPT 4 Gemini all the other ones this is where it misses quite a bit it might not be able to find this or it might not understand the reference the context of what I'm talking about because what might understand what this means when we're talking about text right so in llm large language model if you just asking you questions in text form and spitting out text form answers right it's it's pretty good at that right not perfect but you know that's kind of what they do but then you also have like the eye where it like looks at something tries to understand it it tries to translate what it's seeing so the pixels on a screen into words and then answering with words right and translating into some action on a screen I hope that makes sense right because to us it's almost like the same thing if I say you know click on the ey right so you click here like you don't have to think about it too hard but with language models and vision it's kind of like this back and forth translation it has to look at the image translate that into text reason about it using text and then translate that into clicking somewhere in the image and this is where we ran into a lot of problems with it this is where it screws it up like a lot of the time so the question becomes how do we help it translate visuals into words and so this apple paper continues the paper demonstrates how LMS can be used to create an extremely effective system to resolve references of various types by showing how reference resolution can be converted into a language modeling problem despite involving forms of entities like those on screen that are not traditionally conducive to being reduced to a Texton modality so I got to say I feel like in a lot of these papers like the sort of style of academic writers where they write for other sort of academics or experts in the field feel like that's slowly going away a lot of the more modern papers in the AI space at least I'm seeing they're simplifying things where they can be simplified if you can say something in a easier to read way when you can say it without losing some meaning you should do so I feel like here they're using too many big words to try to where maybe they're not as needed I mean they're basically saying like how do we take a picture of something and turn it into words so that an LM model can more easily interact with it and they're saying The Benchmark against GPT 3.5 and GPT 4 saying with our smallest model achieving performance comparable to that of GPT 4 and our larger models substantially outperforming it and I think this is the big thing because stuff like this if it works as advertised could become kind of the go-to way of running these Vision SL language models and this would be a pretty big breakthrough for agents for robotics as well perhaps and just in general for computer vision so they start with human speech typically contains ambiguous references such as they or that right and we as humans we kind of you know figure it out based on context and this is what's needed for users to be able to naturally communicate the requirements to an agent an AI agent and it's crucial if we want to have a true handsfree experience in a voice assistants right so they're kind of approaching these AI agents first and foremost as voice assistants right so they can have Siri be that autonomous agent table one what does the speaker want here's what the speaker wants user the speaker saying show meaes near me right the agent returns a list let's say it's you know 10 Blue Links or whatever and so here the user can say a number of things now if you're she says call the one on Rainbow Road right that's pretty straightforward right the LM model can probably understand that call the bottom one that's also should be fairly obvious but I've seen certain examples where the vision models kind of managed to screw that up as as well and of course call this number a number that's present on screen this could be probably the most vague one potentially I would say I would feel like this one might fail the most especially if there's multiple numbers present right maybe the user you know if this if this is the screen right and the user Scrolls down and then here's one number and here's another he's one right he might be referring like the one on the center of the screen like call this number right but there's like several numbers on the screen like that could be confusing and so they're saying here it's immediately apparent that it would not be possible for the agent understand or complete the user's query without the ability to use and comprehend context to answer that question to do that action and there are multiple context needed the conversation plus the onscreen context they talk about how having a model that can do this reference resolution right they're saying that having be a little bit more modular like having B its own Standalone model like a module that you can swap in and out with improved versions so similar to I mean with an agent right you might have like this llm model that's doing like the reasoning thinking about what's supposed to be doing and then on this side you have your kind of the action model that like executes and clicks on things and this reference resolution let's say RR somewhere in the middle right so V LM and the RR they're figuring out kind of the context and what needs to be done right then the llm says what needs to be done to the action module this would kind of be like how that agent would work how to perform all the task you need while interacting with the screen that you're using I mean they're not necessarily saying this but the point is like they're saying having this thing be modular would be a good thing it could be beneficial and in this work they're advocating for the use of relatively smaller language models that are fine-tune for specifically and explicitly for the task of reference resolution right so this idea of reference resolution this sort of understanding what's happening in the screen and kind of uh tying into the context of what's Happening they're saying this would be a small Standalone large language model doing this and that one might be communicating with the larger the smarter sort of the bigger large language model that's doing the reasoning which this is kind of a theme that we've been seeing in a lot of different paper the idea of stacking and combining different size language models different AI models each fine tune with their own thing like stacking them and combining them in certain ways seems to be very very effective because something like this could be much faster at certain tasks much better but completely useless for the tasks so having it be like almost a tool for the large language models that is doing kind of like the decision- making that could work very well here's where they're talking about more about Vision right and the problems of vision so they're saying however the biggest challenge with adopting this technique for the general reference resolution task in the context of a voice assistance so again this thing that figures out what does the user mean when they say call this number right given a list of numbers on the screen like what does that mean that's context resolution or reference resolution so the challenge there is just resolving the references to entities on the screen and using their properties in other words getting the LM to informally speaking see wow okay they could have said that a lot easier I feel like but the point is just like how we see things on screen and we tap it right when when can a child start doing that at two three while kind of interacting intentionally with buttons on an iPad right if you tell them okay like pop the balloons in this game like at what age can a child do that maybe two right they see the balloon they smack the balloon the balloon pops right they're saying you know the biggest challenge for the humans adopting this technique for a general reference resolution task in the context of popping balloons lies in resolving references to entities on screen AKA balloons and using their properties AKA they can pop in other words getting the child to informally speaking c yeah so again I feel like this could be said easier without losing context so uh why not do that so I'm actually not going to be reading this stuff out loud cuz man do they make it complicated I'm just going to try to kind of summarize it they're saying that they're going to take the screen that you're looking at and they'll try to translate it into text right so you can think of as that screen being sort of represented visually as text and those things are going to be represented or tagged right so that the large language model has or the Lang language model right because they're saying a smaller language model could perform very well at these tasks so that that language model has context so for example if you say call the business number right it understands where on the screen that is what it should click on just based on looking at the various things on the screen that are represented to it as text they have some related work where they talk about like what has been done already and what the some downsides are they talk about using some synthetic screens rather than natural real world images which could be easier to parse obiously kind of in the wild the screens could be much different than when they a synthetic collection of screens that they that they have because they have you know different distributions meaning different people will choose to put certain buttons in wildly different places or put those annoying popups asking for your email everywhere which by the way is one of the big Reasons I'm so excited about AI agents I mean it's not a big reason it's just like the frosting on the cake cuz all those stupid pop-ups will be completely useless if you're interacting with the web through agents and before of course you know how has this approach been done in terms of uh you know computer vision so they're they're using bounding box detection right so trying to kind of like put a box around certain things so there's a button somewhere right if we kind of like outline that button then the computer knows kind of the location of that button like this is probably a good example of it right so you have these bounding boxes over various cars as they're moving so it's tracking them as they move and OCR Optical uh character recognition so basically being able to recognize characters on screen uh letters and numbers and of course Vision Plus text models like GPT 4 Vision they're substantially more expensive with respect to parameters and computation cost so that's that's a good point you know if you're trying to run something on device like on an iPhone and you needed to be doing like a million different things per day using the biggest most expensive model imaginable might obviously limit how much things you can do right this to be super effective needs to be very very cheap and the smaller the better obviously if it can fit on device without needing an internet to run that would be a big big plus and then they of course uh separate the task into on-screen entities right obviously so this is what you see on screen buttons to click on numbers to call you also have background entities the examples that they give is you know like an alarm that starts ringing right your phone might be making a noise but there's may not be a visual representation of that on screen or probably a better example of that is music that is playing in the background right so the vision model has to be like aware of that even even though there might might be a A visual representation of that on the screen right as well as they call conversational entities but it's just any other context related to the user like if you say Call mom whose mom is it going to call I would do a yo mama joke here but I just I'm not coming up with anything and then data set so of course to train these models we need large amounts of data to train them on and so here they're saying they're using either a you know annotated human data created with the help of annotators or synthetically created what's really interesting to me is that you know years from now 5 10 years from now cuz you got to keep in mind a lot of this technology is very new the large language models Transformers they haven't been around for decades I mean really like 2022 is where a lot of this like really started scaling up like a lot of this obviously was around beforehand but that's when it kind of hit that inflection point and just rapidly started increasing as all of these things get better our ability to produce highquality synthetic data to train these models on will get better and better opening eyes feather that that thing that they haven't talked about yet but I think that's their AI assistant annotators like you give it just a bunch of videos and the AI goes through and annotates everything they're saying you know if it's like a hospital footage of of a patient in the hospital bed right it looks at each frame and goes here the patient is getting a shot here the patient is getting food you know the patient's wife is uh holding a pillow over his face right and you know in the next frame the patient appears to be lying still with the hardbeat machine flatlining right imagine how useful that would be right if you can just annotate all of that video automatically cheaply at any scale with something like opens the eye you know their their model obviously could lead to some also pretty scary scenarios cuz I mean in terms of surveillance boy would surveilling every single person get really really easy also not to go on a tangent here but pretty much everything you've said for the last 10 plus years you've said in the presence of some sort of a device like a Google assistant or a Siri or Amazon Alexa whatever actually you know what I'll just stop it right there point being we'll be able to annotate massive quantities of data with with AI Tools For Better or For Worse and so a couple models you got to be aware of so of course their proposed model is I'm going to I'm just going to say realm right because I guess you could say it as re Al but I'm going to say R the r was like one of the first MMO RPGs on online that has nothing to do with what we're talking about just just thought you should know then there's Mars which is a non LM based and there's uh Chad GPT so they're they're testing GPT 3.5 and GPT 4 as well so Mars is a system that proposed in another study so I'm guessing it's the best available non llm system that that we have and here are the results if you want to see all the details that went into doing all this stuff I'll I'll I'll link the paper in the show notes so take a look at that but basically they have multiple different data sets and different sort of testing data they've used and so they separate into four different ones the first one con so this is the conversational data set synth is the synthetic one screen is the onscreen one so kind of like picking the right number to call on screen and unseen is a conversational data set pertaining to a held out domain so I think what they're saying here is they're testing it you know how well it can like generalize right so if we train it on a bunch of stuff in one area how well can it kind of generalize to another area if we have a dog that we TR train to run three different obstacle courses and we just keep training them on those three obstacle courses and then we take him out into the while and we give them some other obstacle course that it's never seen before in its life how well is it going to be able to do on this obstacle course in other words does the dog being trained on these three does it generalized to every other one that it hasn't ever seen before in its life right so I think that's what that's saying here and so Mars does you know 92 99 8384 and you can say you know gpt3 does worse gp4 does better right so for almost every single one of these right that that that holds uh consistently except for the synthetic data set where gbt both gpt3 and GPT 4 completely drop off so there's some weirdness there but anyways here's the r at various different sizes right from 80 million which is Tiny all the way up to 3 billion which is still not big compared to some of the newer models we believe GPT 4 is 1.7 trillion grock I think 300 billion mixture of experts I think they're 7 billion each so combined they're like 56 billion or something like that I might be getting the number slightly wrong but it's in that ballpark right so this is 3 billion it's it's not big and 80 million is just n it's it's tiny which again the point is the smaller it is the easier it would be to run on device without you know needing an internet connection if a phone can run these models that's a big big deal once this gets past a certain threshold it won't be able to run on device or at least not effectively not cheaply but as you can see right away the tiny tiny 80 million model it's not even a large language model right it's just a language model it beats out or it's it's it's almost the same as GPT 4 it's in the same ballpark now for some reason GPT 4 like failed this synthetic test but if we ignore that for a second and compare kind of like apples to apples uh I think conversational data set so I mean the tiny model comes close to being GPT 4 so gp4 is 97 the tiny model is 96.7 so very very close but the very next step the 250 million parameter model it's 97.8 so it beats GPT 4 the 250 million parameter model beats gp4 on the screen tasks and the Unseen kind of how well it's able to generalize the real 3 billion comes close so again this feels kind of big in terms of finding kind of a new approach to building these models instead of using a massive model like GPT 4 with vision we create a smaller language model that is able to do something like this these tiny small models can be very effective for this specific approach for this specific set of Tas s all right so realm is better than GPT 4 for domain specific queries so for example if you ask the model can you make it brighter right of course gb4 might think Oh you mean this the screen brightness right where is a model that is specifically made for let's say the iOS devices might be like oh it probably means like the home bright brightness right so the home automation accessory that makes the room that you're in brighter because it knows which room you're in ETC so in summary tldr what does this thing mean some of you might have just skipped the video to get to this point this kind of means that Apple might be trying to create a new approach to having these on device autonomous AI agents or maybe not autonomous fully but basically a super smart Siri that is able to understand a lot of context that is able to go online do some research for you interact with websites but also change the brightness in your home as well as play music on your various devices just having a much greater understanding of what you want it to do and because they're able to do it with tiny tiny models it can run on device meaning doesn't doesn't have to be connected to the internet it's going to be very very inexpensive and these tiny models are either similar to or in some cases better than GPT 4 at understanding what needs to be done and these models beat GPT 4 out of the box for domain specific queries and it's roughly similar to to each other realm and gbt 4 right so Mars the other kind of state-of-the-art non- llm approach to doing this it kind of sucks right but for that but GPT 4 and realm this Apple's approach they're similar for new use cases right for for doing tasks on an unseen domain whatever the case I think it's exciting to see apple publishing AI research papers because other people can use this approach maybe they can build a small model to help computer vision right kind of incorporate into their own devices so everybody publishing this details and kind of talking about them is good for the field in general for the field as a whole in terms of how big this could be for Apple it's too early to see certainly this is promising and certainly they have a lot of things going for them if they execute on this if this works if they can create on device agents that run your life that can act as sort of assistance for you well I feel like that would give Apple a big big boost certainly they have enough cash to make a lot of this a reality and the fact that they dro the autonomous electric vehicles kind of shows that they're serious about AI I think probably similar to how meta slf Facebook and M and Zuckerberg they dropped VR right as soon as people started realizing that AI is going to be a big deal right it's like let's dump all these little projects we've been working on let's throw out the autonomous cars let's throw out the uh virtual reality devices or not not throw them out but at least reshift our Focus to AI but this seems promising this seems interesting let me know what you think in the comments are you guys excited about Apple potentially kind of moving in the direction of AI trying to become a bigger player do you think it will succeed do you think this is a big nothing burger and it won't really help them or do you think this might be the new sort of approach to doing computer vision for AI agents certainly I'm impressed with how tiny this is the 80 million parameter model how well it does at those specific tasks compared to right if I'm doing my math correctly so this model is 0.4% the size of GPT 4 so it's measured in like the thousandth of a per the size of a GPT 4 I hope I'm doing that math right I'm sure someone in the comments will correct me if I'm wrong but that is impressive the fact that it can reach for specific tasks and similar results to a model that is like gargantuan in size is is certainly impressive I mean and I think we're going to see this play out in the future too Microsoft has been showing some some some of the similar things that small models can be incredibly effective for if they're trained for specific tasks on you know synthetic data even so with that said thank you for watching and I'll see you in the next one",
  "duration": 30,
  "comments": []
}
