{
  "transcript": "With ChatGPT API, the possibilities for innovative\u00a0 projects are practically endless, but simply\u00a0\u00a0 prompting the model is often not enough. What many people don't realize is that under\u00a0\u00a0 the hood, ChatGPT has a dozen of API parameters\u00a0 that all affect how the outputs are generated.\u00a0\u00a0 Each of these perimeters can get you closer to\u00a0 the perfect results that you are looking for,\u00a0\u00a0 yet choosing the right values can be tricky,\u00a0 and without deep understanding, tweaking them\u00a0\u00a0 is likely to only degrade your results. So, in this video, I'll guide you through\u00a0\u00a0 each of the 12 ChatGPT API parameters\u00a0 and explain what each of them does,\u00a0\u00a0 so you can get the best possible\u00a0 performance for your specific use case. \u00a0 Model When it comes to\u00a0\u00a0 using the ChatGPT API, the model itself\u00a0 is the first parameter to consider. \u00a0 Currently, there are only two available options:\u00a0 gpt-3.5-turbo and gpt-3.5-turbo-0301. The main\u00a0\u00a0 difference between them is that gpt-3.5-turbo\u00a0 will always be updated to the latest version,\u00a0\u00a0 while gpt-3.5-turbo-0301 is a specific\u00a0 version released on March 1, 2023. \u00a0 However, since there's currently\u00a0 only one version available,\u00a0\u00a0 both models are essentially the same. That being said, if you always want to use\u00a0\u00a0 the latest and greatest version of ChatGPT in your\u00a0 requests, go ahead and set it to gpt-3.5-turbo and\u00a0\u00a0 forget about it. Messages \u00a0 Messages are the backbone of your conversation\u00a0 with ChatGPT, as they provide the input that\u00a0\u00a0 the model uses to generate its response. To use this parameter, you'll need to provide\u00a0\u00a0 an array of message objects, each with a role\u00a0 field and a content field. The role can be\u00a0\u00a0 set to system, user, or an assistant, depending\u00a0 on who is speaking, and the content is the actual\u00a0\u00a0 message itself. It's important to note that the\u00a0 system message should be provided first, as it\u00a0\u00a0 sets the stage and gives context for the model. Nevertheless, make sure to put the most important\u00a0\u00a0 instructions in your latest message, as the\u00a0 current model sometimes forgets the initial\u00a0\u00a0 system prompt. Temperature \u00a0 Range: [0, 2] Type: Float \u00a0 Default value: 1 Temperature is the first\u00a0\u00a0 and most important numeric parameter for\u00a0 controlling the output of the model. \u00a0 In essence, temperature controls the creativity\u00a0 of the model. To understand how it works,\u00a0\u00a0 consider the following example: if\u00a0 temperature is set to 0, ChatGPT will\u00a0\u00a0 always generate the same response for a given\u00a0 input. However, as we increase the temperature,\u00a0\u00a0 the output becomes increasingly diverse. So, try increasing temperature for more\u00a0\u00a0 creative tasks such as writing poetry\u00a0 or song lyrics and decreasing it\u00a0\u00a0 for a more technical or scientific application. I recommend playing with it in almost every\u00a0\u00a0 request. Top_p \u00a0 Range: [0, 1] Type: Float \u00a0 Default value: 1 top_p is an alternative to\u00a0\u00a0 temperature that can also help you fine-tune\u00a0 the randomness of your AI-generated text. \u00a0 top_p controls the scope of randomness., so if you\u00a0 set it to 0.1, only 10% of the random responses\u00a0\u00a0 will be considered for completion, while\u00a0 setting it to 0 will make the model completely\u00a0\u00a0 deterministic, generating the same response\u00a0 every time, even when temperature is at 1. \u00a0 It is a good practice to change either temparature\u00a0 or top_p and keep the other one at 1, as these\u00a0\u00a0 parameters are correlated. Max_tokens \u00a0 Range: [1, 4096] Type: Int \u00a0 Default value: inf Max_tokens sets the maximum number of tokens that\u00a0\u00a0 the model can use for both input and output. At 4096 tokens your model will be able to process\u00a0\u00a0 at most around 3000 words. Keep in mind that\u00a0 this value includes both tokens used for input\u00a0\u00a0 and output. So, if your previous conversation\u00a0 already takes up 3000 tokens, the model will\u00a0\u00a0 have only 1000 tokens left for a response,\u00a0 which equals to only about 750 words. \u00a0 It's a good idea to set the ax_tokens a bit higher\u00a0 than your expected response length, unless you're\u00a0\u00a0 specifically looking for shorter responses\u00a0 like in the use case of summarization. \u00a0 Presence penalty Range: [-2, 2] \u00a0 Type: Float Default value: 0 \u00a0 presense_penalty is perhaps the second most useful\u00a0 dial after temperature and it instantly solves\u00a0\u00a0 one of the most common complains that I hear\u00a0 from my clients which is repetative text. \u00a0 By increasing the presence penalty, you\u00a0 can make the model more likely to talk\u00a0\u00a0 about new topics, which is extremely useful\u00a0 for generating long texts. On the other hand,\u00a0\u00a0 lowering the presence penalty will make the model\u00a0 focus more on elaborating on specific points,\u00a0\u00a0 which is great for Q&A or\u00a0 other focused use cases. \u00a0 Presence penalty is a powerful dial that\u00a0 gives you a lot more control over the\u00a0\u00a0 generated text. I highly recommend experimenting\u00a0 with it to see how it can improve the quality of\u00a0\u00a0 your output for your use case. Frequency penalty \u00a0 Range: [-2, 2] Type: Float \u00a0 Default value: 0 Frequency penalty is a sibling to presence\u00a0\u00a0 penalty, and it affects the likelihood of the\u00a0 model repeating the same line word for word. \u00a0 Higher frequency penalty values result in new\u00a0 sentence structures, but not necessarily in new\u00a0\u00a0 topics, and lower values make the model\u00a0 more repetitive. Generally, it is a good\u00a0\u00a0 idea to experiment only presense_penalty\u00a0 or frequency_penalty but not both. \u00a0 So, if you want to improve the variety of your\u00a0 generated text, consider experimenting with\u00a0\u00a0 these coefficients, but be careful when setting\u00a0 these values higher than 1 or lower than 0.1. \u00a0 N Range: [1, inf) \u00a0 Type: Integer Default value: 1 \u00a0 N is an integer value that controls how many\u00a0 chat completions your model will generate. \u00a0 This parameter can be useful if you want to\u00a0 present your users with a few different options\u00a0\u00a0 or if you want to generate multiple texts at\u00a0 once without making multiple API calls. However,\u00a0\u00a0 setting N too high can quickly drain\u00a0 your tokens, as you will be paying the\u00a0\u00a0 price of N requests for a single call. When using N bigger than 1, don\u2019t forget to\u00a0\u00a0 increase your temperature, so you don\u2019t get N\u00a0 exact same completions for the extra price. \u00a0 Logit_bias Type: map \u00a0 Default value: null Logit bias is an advanced\u00a0\u00a0 parameter that can be used to decrease\u00a0 or increase the likelihood of a\u00a0\u00a0 selection for a particular token sequence. For example, if we set the 'Paris' bias to -10\u00a0\u00a0 and ask the model what the capital of France\u00a0 is, it will try to answer but will be forced\u00a0\u00a0 not to use the word 'Paris.\u2019 This parameter is rarely used,\u00a0\u00a0 but can be nonetheless a lifesaver\u00a0 in some very rare use cases. \u00a0 Stream Type: Boolean \u00a0 Default value: false The 'stream' parameter is a boolean\u00a0\u00a0 that allows for a nice typing effect, similar\u00a0 to what you see on the ChatGPT interface. \u00a0 When this parameter is set to 'true', partial\u00a0 message details will be sent, allowing you to\u00a0\u00a0 display your completion as the model generates it.\u00a0 This can significantly enhance the user experience\u00a0\u00a0 by reducing input latency, or the amount of\u00a0 time between when a user initiates an action\u00a0\u00a0 and when the system responds to that action. While setting up the 'stream' parameter may\u00a0\u00a0 require some additional effort, it's completely\u00a0 worth it for the improved user experience. \u00a0 User Type: String \u00a0 Default value: null The 'user' parameter is a new\u00a0\u00a0 addition that represents the end user's ID and\u00a0 can help OpenAI monitor for potential abuse. \u00a0 If your app has some form of authentication,\u00a0 it's recommended to add the user's identifier\u00a0\u00a0 to this field. This can help OpenAI\u00a0 provide your team with more actionable\u00a0\u00a0 feedback in case of any violations. And if\u00a0 you don't have user authentication set up,\u00a0\u00a0 you can use a session ID instead. While this parameter is optional, it's a good\u00a0\u00a0 idea to add it in advance, so you don\u2019t run into\u00a0 any issues when you start scaling your app. \u00a0 Stop Type: String or Array \u00a0 Default value: null And finally, the 'stop' parameter\u00a0\u00a0 allows you to define up to four stop sequences\u00a0 for the model to stop generating output. \u00a0 As soon your model has generated one of\u00a0 the stop sequences, it will immediately\u00a0\u00a0 stop producing new tokens. It's very commonly\u00a0 used with fine tuned GPT-3 models, however,\u00a0\u00a0 without fine-tuning, this\u00a0 parameter has little use. \u00a0 I'm hopeful that the fact that the\u00a0 new ChatGPT API has this parameter\u00a0\u00a0 available means that fine-tuning for\u00a0 ChatGPT will soon become an option. \u00a0 ChatGPT API has already paved way to\u00a0 thousands of startups around the world,\u00a0\u00a0 yet I believe its truy power is still untapped. Just imagine if you could train ChatGPT on your\u00a0\u00a0 previous customer support conversations to\u00a0 help it learn more about your business and\u00a0\u00a0 make it act like a trained support agent. When this feature becomes available, it will\u00a0\u00a0 completely transform customer service, so if you\u00a0 don\u2019t want to miss out, don\u2019t forget to subscribe.",
  "duration": 6,
  "comments": [],
  "metadata": {
    "id": "HFYTbGMtqeU",
    "title": "ChatGPT API Parameters for Beginners Explained",
    "channel": "VRSEN",
    "published_at": "2023-03-17T19:12:14Z"
  }
}
