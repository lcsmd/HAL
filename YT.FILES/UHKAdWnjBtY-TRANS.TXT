{
  "transcript": "okay so my uh patreon Community just brought this tweet up to my attention um and a little bit of background I'll give you some more in a second but basically uh United States Congress has just dropped this little gem of a uh regulation basically creating a new regulatory agency um for artificial intelligence and having gone over this you know skimmed it myself and like interrogated it with Claude and asking questions it basically looks like what Sam mman and others have been talking about uh Gary Marcus um Jeffrey Hinton uh what all these people have been calling for for at least the last year um in terms of Regulation so uh but what I wanted to start today was this initial response so this is a new AI proposal from AI policy us it should it should slam the Overton window shut so a little hyperbolic already um it's the most authoritarian piece of tech legislation I've read in my entire policy career um no the EU AI Act is way way more overboard so again with the hyperbole um everything in the bill is aimed at creating a democratically unaccountable government jobs program for doomers who want to regulate math I mean this is a work of poetry it's garbage but it's a work of poetry nonetheless um I mean just check out this section where in a mere six paragraphs attempts to Route any potential checks from Congress to the courts um okay sure whatever uh what the the paragraphs that he's outlined here basically like if an if an emergency order is issued by either this agency or the president you have to cease AI activity this is no different from like the FAA or FDA like all kinds of regulatory agencies have this kind of emergency Authority already to shut down something that is imminently causing harm or is dangerous um whether it's a biolab whether it's a nuclear site whether it's even trains right you can like there are there are agencies that can tell you shut down that train look at Boeing look at the doors blowing off of Boeing right now um and guess what the FAA has the authority to ground airplanes so that level of authoritarianism is this level of hand ringing doesn't make any sense now I will say that I have passed this piece of legislation on to my attorney friends so we'll see what they say maybe I'm the one who's off base here so um without further Ado this is this is the actual act here's the PDF um and one thing to keep in mind this is a draft it is not the final uh version this is just the current draft um so the responsible Advanced artificial intelligence act um be it enacted by the Senate and House of Representatives so on and so forth to establish an Administration that will oversee and regulate Advanced general purpose artificial intelligence systems great not a big deal there um they they lay out what they mean by flop and training runs and Frontier AI systems and Frontier AI Labs again it's all really really straightforward stuff like this is not Arcane language now what I will say is that they Define the four TI and this this is where I will agree with criticism because we already knew that this was coming um and so one of the things one of the one of the criticisms was you know if at least 10 to the 24 flops during its final training run that's a medium tier concern um and then if it's two uh 10 to the 26 that is a high concern and above 10 to the 26 is extremely high concern okay fine I'm not sure who they consulted for this maybe they asked I don't know like Ray Kurtz or you know a few other people like how how powerful do you have to scale these AI systems until they require regulation now the there's a few reasons that I think that this this classification system just using the relative power is kind of dumb because there is the possibility that AI systems will be very dangerous without that level of uh computer performance particularly with distillation um as we you know go for uh two you know four floating Point uh uh bit models and distilled down to one bit models and find other algorithmic efficiencies it's entirely possible that the most dangerous models will not even be that powerful but again that's speculation on my part I don't know who they consulted for this um now what I will say is that um they do identify like some of the primary big risks that we're thinking about like bioweapons and cyber attacks and fully autonomous artificial agents um okay uh now this is where I'm just like hold on cuz if you squint gp4 uh already constitutes what I would say an extremely high concern AI system so let me go through the the the the criteria for extremely high uh concern AI system one is the AI system uh has or could easily develop the ability to significantly assist with a development manufacturer or deployment of biological chemical radiological or nuclear weapons this has already been proven many times with um with various like papers that are published for free on archive where they just gave college students like undergrad college students unfiltered access to gp4 and they said hey um go use this to like create gain of function bioweapons go use this to figure out how to make chemical weapons and guess what they were able to succeed now of course Sam Alman has been on out on in the public saying oh it's not that helpful yet it doesn't really do any of those things and but here's the thing is the base model versus what it has been trained to do because even Elon Musk with grock it's like it'll refuse to do some of those things the fundamental capability uh of the model to do these things versus uh postao fine-tuning to cause it to refuse very big difference there gp4 is already capable of all of these things now uh part of the duty of care that's outlined in this and I'll show you what I mean there's some criteria that it that it lays out basically it's up to the model trainers to make sure that they don't release these like unmitigated Untamed models into the wild so you could make the argument uh that that uh GPT T4 already circumvented this um especially because jailbreaking is a thing so that would be a failure of Duty of Care on their part um so that's one thing um the AI systems could uh has or could easily develop the ability to autonomously spread replicate in uncontrolled manner or take over external computers obviously this one is going to be a little further off just because of the size of the models and the size of the hardware that's required um but you could imagine a future where because there's more and more GPU clusters deployed across the world that you could have auton AI systems metastasizing from one data center to the other but there's a lot that would have to go into to making that happen um but the fact the fact that we have this Skynet like thing that is being put into US legislation the the era of fiction is over this is this car this this one right here is to say don't let Skynet happen don't let it you know metastasize through the through the hive like the Borg um so that's fine they can't do that right now I agree fine whatever if if it does have that capability it's extreme risk but honestly I would personally characterize um gp4 as extreme risk based on this alone number uh three or letter C the AI system Hazard could easily develop the ability to accelerate scientific research to such a degree as to undermine National Security and de destabilize the global balance of power 100% true every scientist I know is already using this technology um and it does it now okay does it accelerate it to the point that it undermines National Security maybe not um but at the same time you know Claude Opus gp4 all these other ones they are actively accelerating not just science but a lot of other stuff that happens law for instance um perplexity accelerates research so again this is kind of a really flimsy definition of like what constitutes an extreme uh extreme um risk now taking a step back I know that just because it's on paper it doesn't mean that like it's a hard and fast Rule and some human is going to mechanistically interpret this like you know a vogan you know just going to rubber stamp oh this is illegal um these are guidelines for humans to follow but they're not necessarily the best guidelines and then uh d uh some or all of the AI systems capabilties significantly exceed normal human levels or performance on one or more tasks relevant to major security risks uh yeah if you use like you can give gp4 the ability like you say hey use inmap to do hacking or whatever these are already superhuman in terms of writing scripts now uh is it going to be better than the best hacker absolutely not is it going to be able to compromise um a good uh security hardened uh corporate infrastructure probably not but what it can do is in the in uh the hands of someone who is highly motivated these systems will already give your Joe Schmo the ability to hack at like Grade B or grade a tier not s tier hack hacking but it it you know cyber security risks definitely there um so that's one uh that's that's not there but like okay so one one half one half like to me this is just such a broken uh system of of concern and then finally e uh the AI system otherwise poses significant existential Glo Global catastrophic risk I would say that gp4 is nowhere near that um but again this is such a flimsy framework of finding extreme risk and what this to me what this really indicates is that the government as always doesn't really know what they're doing but in fact actually one of the things that this uh that this legislation points out is that we need to develop in in-house expertise so going over to um going over to Claude to kind of break it down into layman's terms it establishes a new federal Administration the frontier artificial intelligence system administration great it defines different tiers of AI systems that what's what we just went over it requires develop media concerns to uh pre-register with the administration High concern and extreme High concern require permits for Hardware training model weights and deployment so again this this requiring permits that's exactly what Sam has been has been asking for the biggest concern here is going to be uh regulatory capture um so it remains to be seen if this is uh how it's been implemented it doesn't Prima fasy it doesn't really look like it is it is gearing towards regulatory capture remains to be seen though the administration will develop standards rubrics and an application process for Frontier AI permits so again this is this is all very very like boilerplate they haven't even worked out the details yet so that's another thing to keep in mind um appeals process for permit applications and decisions this is where the regulatory capture I suspect we have the highest risk because if if the for instance if the people that they appoint on their Board of inspectors or whatever come from Microsoft and Google and open AI then who's going to get rubber stamped it's going to be the Microsoft and meta and Google and open AI that get rubber stamped and then all the mom and pop shops all the smaller ones they're going to want they're going to be the ones that get uh more scrutiny uh pointed at them IRS has been doing the same thing for a long time uh whereas uh ordinary middle class citizens like you and me we're the ones that the IRS has been targeting for a long time because all you have to do is send a threatening letter to someone like us and we pay the taxes or pay the fine or whatever you send a threatening letter to a millionaire or billionaire they hire lawyers and accountants and they basically make the irs's job too hard and so by virtue of it's harder to go after Big Fish they go after the little fish instead that is always a risk for any kind of Regulation um that's not unique to this but that is something that I am concerned about regulatory capture is the most mcky part of all of this um but creating an agency that can like ground the airplanes there's nothing mcky about that in my personal opinion um it requires self-reporting of transactions that involve high performance AI Hardware that's kind of a no-brainer if you remember elazar owski a while ago was saying like you need to register every GPU I mean you know slap a uu ID on it you can track it you know throw it in a blockchain logistics program you got it like that just kind of makes sense now you might say okay but do we track every bullet that we manufacture well I mean the military does do a pretty good job sometimes of tracking all of their Munitions also the the US military is known for leaving Munitions on battlefields but that's an entirely other other story uh let's see it establishes civil liability and duty of for AI developers with strict liability for tangible damages caused by Frontier AI systems criminal penalties are also specified so I've got those in the next next part um but this is this is another thing that was uh discussed at some of the public Congressional hearings particularly those with Gary Marcus and a few others uh Sam Alman um were basically by creating a framework of liability of civil and criminal liability that says oh hey here's an entirely new class of things that you could be liable for um if you're if your AI models break the law or hurt people in such such and such way which this is a way to basically play mediator say hey if you have a if you have standing to bring a lawsuit to say this this AI model has harmed you um whether on a civil civil level so the the penalty would be you know Financial damages or criminal which could result in jail time and actually they do outline possible jail time in this that holds the AI companies responsible for what they do so the safety people should be pretty happy with this um now you know me like I'm both on the acceleration side where I just think acceleration is default I also think that there are some big risks so to me I think that this does strike the balance personally I feel like this does strike the balance between um the friction of Regulation uh I'm not necessarily happy about the risk of regulatory capture but it does also allow for um for things to continue at a decent enough Pace it grants the administration emergency powers to suspend permits issue orders and take other actions if Frontier AI systems pose an imminent major security risk again the FAA can ground airplanes if their if their doors explode off I don't really see any fundamental difference on principle to that um because if someone is at risk great shut it down um the next is it provides whistleblower protections for employees reporting major security risks from Frontier AI systems so instead of people like uh Jimmy Apple's reporting on Twitter and other leaks now they actually have a legitimate way of saying hey um let's just imagine I'm I'm not going to name any names because I don't want to get sued but just imagine that you know a gigantic big Tech you know AI company um you know XYZ uh says hey you know our models are safe they're not able to do any biological harm or whatever but someone who was on the safety department said actually yeah it can it can actually do uh biot it can Aid biot terrorism and then they submit a whistleblower complaint to this agency that gives them recourse other than just Le leing onto redddit or forchan or Twitter or whatever and so this is actually again this creates another structural incentive where if someone is more concerned if if you have a someone who is conscientiously concerned about what a company is doing it is the role of the government to be able to hold that company accountable um now if you're Pro corporate then like yes this probably looks very Draconian and I will admit that if you're on the if you're on the like harder more libertarian side and just let the companies work it out on their own um granted I don't trust corporations and actually from the polls a lot of you don't trust corporations either I know a lot of you in the audience trust governments even less um but it's like the devil you know um and then finally it authorizes funding the administration through Appropriations permit fees blah blah blah blah blah um and then I asked it to just recapitulate the uh the tiers of risk again I think that the that this classification system is the dumbest classification system um rather than providing characteristics of like the model was trained on XY Hardware you just need to list out which capabilities like are you looking for um or the or the heuristics that you use to measure those those capability those uh capabilities and then finally civil liabilities and criminal liabilities so civil you know all persons engage blah blah blah the duty is owed to us residents a lot of this is just such boilerplate stuff um but what was interesting is that it also includes criminal liabilities so class C felony 10 to 25 years imprisonment for failing to comply with an emergency order conducting activities after a permit rejection or violating permit conditions for making false statements about a permit application so this means that if Google or meta or Microsoft or whoever once project Stargate gets up and running if they don't have the adequate permits people are going to jail so that is like that from a safety perspective that is great this will prevent stuff like you know the AI equivalent of Enron hopefully from happening um now cuz again if you if you are personally at risk if you're the one who pushes the button on that training run and you don't have the permit to do so you are personally at risk of going to prison for that so that is a very very like incisive thing this is from a safety perspective this is what I am happiest about now again you might say this is super Draconian the government doesn't have the right but you know what the government is is of the people for the people and by the people and I'm glad that my government is representing me in this fashion um sound off in the comments again I am very very well aware that this is a highly highly contentious piece of legislation and there's going to be some people that are super angry about it and some people that are super happy about it and some people that are super disappointed and a lot of people who don't care I don't know let me know what you think in the comments uh Class A misdemeanor 6 months to onee imprisonment uh improving or using Frontier AI without a valid permit uh failing to take required actions under a permit and blah blah blah blah blah so again just the you you got some skin in the game and it keeps going down you know Class B misdemeanor recklessly violating any other provision so on and so forth it's got some teeth so this is when when some of this stuff started rolling out last year um some of the some of the Insiders that I talked to over in Silicon Valley the initial AI Acts were considered to be completely toothless this is not Toothless uh which is good and then I asked like okay what are the duty and obligations that it outlines this was actually pretty interesting so the duty of care um all persons engaged in the development uh or or use of AI owe a duty of care to exercise appropriate caution this includes ensuring that their AI systems do not harm to innocent bystanders do not escape and spread to third party Hardware without consent um and have model weights that are not leaked and made publicly available so this is where it also outlines saying hey if you're doing this you actually have some legal responsibility to use this responsibly if your weights get leaked that's on you pre-registration uh again I can I can imagine how some people might say the require for registration and permitting that could be seen as a rather Draconian maybe a little bit of over reach um or also that this could be a way that uh regulatory capture is implemented because when you say you are good and you are not you kind of you shape the the the landscape and so if if the big players like Google and Microsoft if they get the if they get the fast lane but everyone else uh takes a little bit longer that's creating an unfair Market uh environment and also sets the stage for regulatory capture so like I said that pre-registration and those risk tiers that's that's what I'm most unhappy about permitting this makes sense just like how you're not allowed to fly a jumbo jet without permitting go figure like I don't really see any difference self-reporting uh you I guess self-reporting like Star Baines Oxley isn't isn't socks like isn't that gone now or something anyways point being is if you're if you're in the financial sector if you're dealing with uh you know toxic chemicals or whatever lots and lots of regulatory agencies require reporting not really anything there heck I even have to report I have to like go register my car right that's a form of self-reporting um we all have to do some self-reporting it's a pain in the neck is it is it Draconian I don't think so compliance with emergency orders we already covered that so there's criminal penalties if you don't uh comply with with emergency orders but what's interesting is that the that this AI regulatory agency or the president can issue an emergency order so if Joe Biden or you know whoever is in office says hey you got to shut that down it's a matter of national security you have to comply with that now technically the president probably already has the the authority to do this but this enshrines it in law whistleblower protections already covered that inter agency cooperation again pretty boiler PL compliancy with regulations again all relatively boilerplate stuff so there's a couple of things that really stand out here to me as some are good some are spectacularly stupid but it's the government what did you expect so yeah thanks for watching I hope you got a lot out of this let me know what you think in the comments cuz again this this is just my initial off-the-cuff reaction um I could be wrong I could be reading reading it incorrectly but I do think it's a step in the right direction and I also do think that this is categorically better than both the EU AI Act and the UK AI act or whatever it was called um it's certainly less uh squishy than those this is much more hard and fast in terms of what it lays out so yeah cheers",
  "duration": 21,
  "comments": [],
  "metadata": {
    "id": "UHKAdWnjBtY",
    "title": "US Government AI Regulation BOMB DROPPED! RAAIA Act - The end of Open Source? Regulatory Capture?",
    "channel": "David Shapiro",
    "published_at": "2024-04-15T16:31:02Z"
  }
}
