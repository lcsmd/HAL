{
    "qdWHONYm1l8": {
        "title": "OpenAI Unlocks Powerful Coding Workflows with \"GPT Mentions\" Feature",
        "thumbnail": "https://i.ytimg.com/vi/qdWHONYm1l8/hqdefault.jpg",
        "author": "AI Lucas",
        "date_of_release": "2024-01-29T12:00:45Z",
        "duration": "PT18M9S",
        "view_count": "609",
        "like_count": "29",
        "comment_count": "10",
        "description": "ChatGPT's new \"GPT Mentions\" feature allows you to talk to multiple custom GPTs in the same conversation. This enables powerful coding workflows with multiple GPTs acting in harmony to create complex code.\n\nInterested in more deep-dives into ChatGPT capabilities?\nWant more information about my projects?\n\nSubscribe to my Substack, where I post all my latest updates and any AI news that I think is important (NO hype cycle news):\nalucas.substack.com",
        "transcript": "so this morning I got an early start I sat down at my computer and was getting ready to write a medium article and then I notice this little popup in the corner turns out the little popup was in my chat gbt window uh and is a new feature they're calling it GPT mentions is kind of a funny name because it's actually a huge feature and it sounds like a small feature so basically if you've used custom gpts before you'll know you open that GPT in a new conversation you can talk to it if you want to use a different custom GPT with different custom instructions and different context then you have to open up a new conversation but now with this new feature you don't have to have separate conversations anymore uh you just type the at symbol and then you mention the custom GPT that you want to bring into the conversation and then it is able to actually reply to you not only that every custom GPT that you bring into the conversation can see the entire conversation content context which means they can actually technically talk to one another so now without further Ado let's try it out I've always wanted to do this so this is a vanilla cha gbt prompt and now if we type at in the input box down here you can see different custom gpts that you've tried recently here for my agentic Minecraft Project I've created a coder GPT a reviewer GPT and a tech lead GPT another thing to note before we get started is make sure you clear any custom instructions that you normally have for chat GPT uh I've noticed that these can interfere with the instructions of the custom gpts so I'm going to start by enabling the tech lead GPT this is a GPT that's responsible for creating tasks for the other gpts um not sure if this is 100% necessary but since I'm talking to multiple gpts in the same conversation I've started the practice so just starting with a tag indicating who I'm talking to just in case the models don't really see which GPT I'm directing each question to so Tech lead step and then the first question I'm going to ask is we need to implement the full class diagram and sequence diagram what is the first task so these are diagrams that I've already provided to each of these gpts and basically that's the the software design now you can see it's constructing the first task the task that the tech lead GPT has chosen to tackle first is initializing the agent class you see here implement the basic structure of the agent class right JavaScript uh agent. JS and it describes what that class is going to look like so here's the the real magic of the feature I'm going to type at again and select the coder now and now all we have to do to get the coder to work on this task is implement the task says it's going to provide us with the JavaScript that we need to create the agent class and there it goes so now here's the the full code that the coder GPT created you can see it's a very basic shell actually not much in there um I've also noticed it said in testing test this to test this class you can create an instance of agent in another Javascript file and call its methods to ensure they exist and do not cause errors really we want the coder agent to actually generate the test code so that we don't have to do that it's very simple code for the model to generate um so I'm interested to see if the reviewer GPT picks up on that so I'm going to type at and then go to agentic Minecraft reviewer again all of these gpts have access to the diagrams uh so they know what the design of the software should be review the code just generated and now it's giving us its observations and feedback in a code review so now looking through the full code review you can see at the end here it has a specific suggestions section so this is something that I provided in the custom gpt's instructions is that it needs to provide specific inst specific suggestions um not just like General ideas um um so the the three things that it points out and I I found that this is often the first three things that it notices in GPT generated code uh more documentation error handling uh and testing and debugging so that's what I talked about before it's saying you actually need to implement the unit tests for each method to ensure that they function as expected so that's what we want from the coder GPT um so let's just go ahead and send it right back to the coder implement the review suggestions says it's going to add more detailed documentation placeholders for error handling and validation comments for future unit test integration so in this case it may not actually generate the unit test um I need I may need to to play a role as the human here and actually tell it to generate the integration test sorry unit test yeah so I've noticed here here at the bottom the coder says when further implementation is done write unit tests for each method to ensure they work as expected uh basically it's saying that this class is so simple that it doesn't really need a unit test at this point um which which I generally agree with so I would say let's continue with this and we'll go back to the tech lead um to generate another task please create the next task to be implemented so I just realized I completely forgot to add the tags that I talked about to each step in this conversation but it seems to be understanding the conversation just fine so clearly that was just a bit of paranoia on my part um here's the next task from the tech lead agent um um implement the basic structure of the sensory State class so this is now basically going to create another shell class for sensory State like it did for for agent. JS I like the fact that it's splitting it up into files um when I've tried this in the past it it always hasn't hasn't always tried to split it up into files it's sometimes put it all in one file um that said it's it's giving me a little less detail on this run than than it has in the past so we'll see if we can get it to go a bit more deep uh bit deeper all right so we're going to pass this task back to the coder implement the task again pretty basic Shell Code here let's go ahead and do the review pretty similar suggestions to before let's just jump right into code cing so here's the new implementation after the code review of stage two um it actually did make some improvements so it it saved some of this environment data into its U member variables however it still has not decided to actually write the unit tests uh so obviously it's trained to act like a real developer in that in that instance um so I'm going to go ahead and just tell it to write the unit tests it does learn within the context of this this conversation itself so an example of that would be this second code that the coder created you can see it automatically with without being told by the reviewer added the dock strings whereas the initial code that it had created in the first step did not have dock strings so it's learned based on the initial review feedback that each time it needs to add doc strings and so it it will learn similarly if I tell it to actually write the unit tests let's go ahead and do that all right so we finally have some unit test code here I did copy it into a file and actually run the test and there is a syntax error so rather than pasting in the output which sort of puts me in the loop um I'm going to see if it will run this code in its internal code interpreter and figure out what the issue is for itself I'd rather it be able to work through problems on its own rather than rely on me for the testing so I'm going to say please run this test in your code interpreter and debug so its response to me was basically hey dummy I can't run JavaScript code which of course I should have known uh it's internal environment can only run python code um so it can't run this unit test um I then pasted in the actual output from my command window and it explained what the issue is um missing a particular test framework that it's using for the unit test I like that it's using this unit test it's teaching me about the unit test structure uh the unit test uh framework which I didn't know about before um so I'll have to look into this later and get it all installed don't have time to go through it now now that we've sent the conversation down this Sidetrack let's see how easily the model is able to uh get back on course let's go back to the tech lead we're going to say create the next task all right the next task is to create the other class that was in the sequence diagram policy policy state. JS uh let's go ahead and Implement that again the coder agent chose not to implement the unit test automatically um I think this is probably just something that I need to update in the custom instructions for the GPT um I either need to tell the tech lead agent that it needs to specify write the unit test uh as part of the task acceptance criteria um that's probably what I would try first otherwise I could update the uh coder agent to always um write unit tests for anything that it writes that's slightly less preferable to me um I would rather that it was actually part of the task all right so I realized if the model is going to create all these shell classes first this is going to be a very long boring video uh so I went ahead and and went through a bunch of different tasks uh so generating tasks implementing generating implementing um to save on usage I cut out the the review step basically I can act as the reviewer in this case I didn't really need to um but so I'll just walk through what the tech lead and the coder working together were able to achieve um first they created a shell for the Harvest wood policy so this is a particular policy that the agent can choose uh choose to act according to within the the Minecraft environment um next it was the uh the tech lead said to integrate the agent. JS and sensory State um so this is important because now it's remembering that it needs to go back to agent JS and fill in the details um so I was happy to see this as the next step rather than just continuing to create shell classes indefinitely which it could have chosen to do um so here you see we're now uh creating a sensory State object inside the agent class class uh and put in some some mock data in here that it can then update the sensory state with eventually ideally it would actually use the M player API um to sense the environment of the bot within Minecraft um but obviously that can be a later implementation step next up uh now that we've integrated the sensory state with the agent is to integrate the policy state with the agent again at each step here I'm just saying create the task implement the task like very little effort from my part other than reading through and understanding the code and making sure that it's staying on track and and it's doing a great job I actually like that it's going through so slowly um a it allows us to understand what it's doing um and B humans have to take time to think through these things so it makes sense that that AI would also need to take time to think through these things um we're not necessarily at a place where you can just tell the AI to generate all the code in one go and it can create it that's that's not really how the models are designed it's designed to go through this conversational structure with a back and forth uh and make iterative improvements on the code just like humans so before we move on one thing I noticed here was that in this latest code in agent. JS where they created the apply policy method um previously in this version of the file there was a choose policy in between the sense environment function and the apply policy function so I was sort of afraid that the models had forgotten the choose policy function um because you'd expect it to sort of go in order like that however the next task that the tech lead created was to implement that policy selection um and then we can see down here it put the policy selection back where it was originally in this file prior to apply policy and after sense environment so interesting little Quirk there where it momentarily removed the stub for choose policy um but it does always add this little other methods down at the bottom which is sort of a catchall for anything that it's not implementing at the moment you can see that in play here in the sense environment as well it knows that it's already provided the code for sense environment um but it's sort of uh cutting back on the amount of tokens that it's spitting out by replacing uh pre-existing code with this this comment that says existing implementation so clearly open AI has trained it to do that additionally this choose policy me method is actually pretty smart um for for this stage in the implementation saying sensory if basically if it's night in the Minecraft world um then the current policy should be survival like you just want to survive the night um if the Minecraft world indicates that the agent is hungry that means it's low on it it's hunger metric um then it needs to to find food obviously to fill up that hunger metric um otherwise it implements this sort of default policy probably not going to have a default policy uh ultimately it'll always choose some policy whether that's exploration um or mining resource Gathering um but for now I think this is fine one thing I want to point out in this latest code is that since I've been skipping the review step um the the coder agent has stopped adding the doc strings and other documentation into the file um so again hilariously similar to a real developer uh if there if there's no review they start to get lazy all right so lastly here I just want to give you an idea of what what type of information I'm giving all three of these gpts um so that they understand the design of the software from the start you can use gpts to help you generate this documentation especially like the initial formatting but I found that it's really necessary for me to go in and make sure it's perfect um prior to beginning the actual coding I've started using plant uml syntax uh you can see it here on the left not not really important to to go over the details here but this is nice because you have the text version of the diagram which you can feed to these text based models it's very dense information so in a a small amount of tokens you're able to give it a lot of information about the design of the software and that's really key um but then the other nice thing is then we have sort of the human readable form of the diagram here in the the preview window vs code um so there's a there's vs code extensions for handling plant uml um so if you're familiar with with VSS code you can use that method thank you all for watching hopefully this gives you a glimpse into how powerful this new feature is for Chachi PT uh if you're interested in more updates on my projects or more videos like this one recommend subscribing to my substack I'll leave the link in the description uh thanks again and I'll catch you next time"
    }
}