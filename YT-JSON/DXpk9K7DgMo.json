{
    "DXpk9K7DgMo": {
        "title": "Using NEW MPT-7B in Hugging Face and LangChain",
        "thumbnail": "https://i.ytimg.com/vi/DXpk9K7DgMo/hqdefault.jpg",
        "author": "James Briggs",
        "date_of_release": "2023-05-18T15:36:05Z",
        "duration": "PT18M55S",
        "view_count": "57120",
        "like_count": "1097",
        "comment_count": "81",
        "description": "Let's take a look at Mosaic ML's new MPT-7B LLM. We'll see how to use any MPT-7B model (instruct, chat, and storywriter-65k) in both Hugging Face transformers and LangChain. By using MPT-7B in LangChain we give it access to all of the tooling available via the library, like AI agents, chatbot functionality, and more.\n\nüîó Notebook link:\nhttps://github.com/pinecone-io/examples/blob/master/learn/generation/llm-field-guide/mpt/mpt-7b-huggingface-langchain.ipynb\n\nüå≤ Subscribe for Latest Articles and Videos:\nhttps://www.pinecone.io/newsletter-signup/\n\nüëãüèº AI Consulting:\nhttps://aurelio.ai\n\nüëæ Discord:\nhttps://discord.gg/c5QtDB9RAP\n\nTwitter: https://twitter.com/jamescalam\nLinkedIn: https://www.linkedin.com/in/jamescalam/\n\n00:00 Open Source LLMs like MPT-7B\n00:50 MPT-7B Models in Hugging Face\n02:29 Python setup\n04:16 Initializing MPT-7B-Instruct\n06:28 Initializing the MPT-7B tokenizer\n07:10 Stopping Criteria and HF Pipeline\n09:52 Hugging Face Pipeline\n14:18 Generating Text with Hugging Face\n16:01 Implementing MPT-7B in LangChain\n17:08 Final Thoughts on Open Source LLMs\n\n#artificialintelligence #nlp #langchain #deeplearning #huggingface",
        "transcript": "today we're going to talk about using open source models in hooking face and Lang chain we're going to be focusing specifically on the MPT 7B model which I'm sure some of you have heard as one of these fine-tuned versions of this model actually has a context window of 65 000 tokens which is pretty huge the the amount of recording this video gpt4 the one that's generally available to people has a context Window 8 000 tokens and they have a version that goes up to 32 000 but I'm actually not aware of anyone that has access to that at the moment so basically we're limited with gpt4 to 8000 tokens now MPT 7B like I said we can have that super huge model but there are also a lot of other models that are available as well so let me just go ahead and show you those very quickly so just head over to hugging face which is where we're going to pull these models from and you can see actually straight away we have these these four models so the MPT 7B is the cord that's a pre-trained model that's Foundation model okay then we have sorry right to chat and instructor these are all fine-tuned models so story writer is the one you've probably heard about which has a max context window 65 000 tokens which is pretty huge and in reality it actually goes up to higher so I believe they say are here right so we demonstrate Generations as long as 84 000 tokens which is I would say pretty impressive and then if we as you can come over to here scroll down and we can see that other models as well so we have this chat model the instructor model and obviously the foundation model we're going to be using the instructs model because I mean most of the use cases I see kind of rely on us providing instructions to these models and therefore I think most most people out there actually are going to want to use this model okay because yeah we can give the instructions it's going to be able to follow them better than the others so yeah we're gonna see how we can use this both so initially in hugging face we're going to see how we can load that into hug and face and then we're going to see how we can take that and actually load it into line chain which obviously has a few more features on the agent side of things okay so the first thing we're going to want to do is actually do a few pip installs so we have Transformers accelerate so accelerate we need that in order to basically optimize how we're running this on our GPU we will want to run this on a GPU otherwise you're going to waiting a impossibly long time so yeah I if you if you don't have access to a GPU I would recommend you figure that out so right now I'm running this on co-lab and actually there will be a link to this notebook as well in the top of the video so from carolab you can run on GPU okay so you just go to runtime change runtime type you initially maybe are none so you click GPU GPU type so I'm using T4 which is the smallest one on here and the standard version of T4 you can get on the free version of colab but for me that wasn't actually big enough to run the the MPT 7B model unfortunately so I'm currently on colab Pro now thanks to this this model and with that I can switch up to the high Ram version now obviously you have to pay for that but you don't have to pay that much okay it's it's not a it's not a significant cost but of course I know this will be limiting for some people but this is the the best and cheapest option I I can find right now okay so back on the on the installs we have inops so this is again it's used by the MPT model naturally I'm using Lang chain and do I use Wikipedia here and actually I don't think I use this anymore and it's just certain optimization in our in our transform functions okay once we have all those installed we come down here and this is where we initialize the model okay so like I said we're gonna be using the instructor model okay one thing so if you do want to use story writer and you want to use that huge context window you would go story writer okay and then here you would write oh I don't know what is it 65 000. it's kind of nuts but in order to run that you're gonna definitely need more than a T4 GPU basically the highest Max sequence length is the bigger your GPU memory is going to need to be so yeah you need something big to run that but we're just going to stick with this instruct this 2048 is the typical or the standard sequence length for these other models so instruct or the base model the foundation model instruct and chat and this is also something important so the trust remote code we have to have that because essentially the MPT models are not fully supported by hooking face yet so we have to rely on this remote code that is basically stored in the in the model directory for this to set up the all the endpoints and everything for the model okay then we switch the model to the to evaluation mode so that just switches a few options within the within the model that says okay we're not training we're now performing inference okay we're now doing predictions and then we want to move our model to the device so the device we decided here okay so Cuda and we have Cuda current device if we scroll down to the end here we should see what that moved it to yeah so model loaded to Cuda at zero now just one thing this takes a little bit of time to run okay like here it just took a minute I think that's because most of the the model was probably already downloaded for me if you're downloading and initializing this expect to wait like five or ten minutes at least on colab but once that has been downloaded you you should be good to uh use it to basically initialize it and it will just take like a minute or so because you don't need to download it once okay and then we initialize our tokenizer so the tokenizer is actually using this uh Luther AIS GPT neops 20b this is just this is a tokenizer so when I say tokenizer it's basically the thing that will translate from Human readable plain text to transform it or large language model readable token IDs right so it's going to convert like the word the into the token ID 41 for example right and then they get fed into the into the large language model now the mpt-7b model was trained using this tokenizer here right so we have to use that tokenizer then what we need to do is Define a stopping criteria of the model so I should I don't know if I mentioned this but right now what we're doing is actually um initializing the hung face pipeline So within that pipeline we have the large language model the tokenizer both of those we've just created and also stopping criteria object right stopping criteria object let me come down to where we create it is this here okay so basically MPT 7B has been trained to add this particular bit of text at the end of its Generations when it's like okay I've I'm finished right but there's nothing within that model that will stop it from actually generating text at that point right it will just it will generate this right and then it will actually just continue generating times and the text that it generates after this is generally just going to be gibberish because it's been trained to generate this at the end of a meaningful answer right after generating this it's able to just begin generating anything okay it's gonna it's it's not going to be useful stuff so what we need to do is find this as a stopping criteria for the model we need to go in there and say okay when the model says end of text when it gives us token to us we stop right we need to specify that and we do that using this sopping criteria list object okay so that requires a stopping criteria object which we've defined here so I mean you can see this so these parameters are just the default parameters needed by this swapping criteria object and basically what it's going to do is say okay for sub ID so we have these sub token IDs maybe I can just show you these maybe that's easier so stop token IDs and it's just going to be a few integers right those integers actually it's one integer which represents this right so I mean I said before the tokenizer translates from plain text to the Token IDs that's what this is this is a plain text version this is the token ID version right and it's going to say okay for the sub ID here so actually just for zero if the input IDs so the the last imp ID is equal to that we're going to say okay it's time to stop right otherwise it's not time to stop you can keep going and that's it okay so that gives us our stocking criteria object and then we just pass that into our pipeline so the pipeline is basically the tokenization the model and the generation from that model and then also this stopping criteria or package into a nice little function So within that pipeline we pass in obviously our model our tokenizer and the second criteria but there's also a few other things we need as well so return for text so if we have this false it's just going to return the generated part or generate portion of some text and that's fine you can do that isn't actually no problem with that but if you want to use this in line chain we need to return the generator text and also the input text we need to return full text because we're going to be using line chain later that's why we set return for text equal to true if you're just wanting to use this in hanging face you don't need to you don't need to have this as true then our task here is text generation okay so this just says okay we want to generate text the device here is important we obviously want to use our Cuda enabled GPU so we set that and then we have a few other model specific parameters down here or we could call them generation specific parameters as well so the temperature is like the randomness of your output zero is a minimum it's basically zero Randomness and one is maximum Randomness okay so imagine it's kind of like how random the predicted tokens or the next words are going to be then we have P so top p is basically we're going to select from the top tokens on each prediction from whose probability adds up to 15 and I would recommend if you you want to read about this I'd recommend looking at this page from cohere so there'll be a link at the top of the video right now they explain this really nicely so yeah you can kind of see they use 0.515 here as well right so consider only top tokens whose likelihoods add up to that 15 and then ignore the others so with each step right each generation set you're predicting the next token or the next word you can think of it like that and by setting top P equal to 0.15 we're just going to consider the the possible next words because we're predicting for all of the words in that tokenizer uh we're going to consider the top words who's together their their likelihood adds up to 15 right of the total okay so you can you can see that there they visualize it very nicely I don't think I don't think my explanation can compare to this visualization okay and then we have top K this is another value kind of similar thing right so top K if we come up to here you and this is easy to explain we're picking from the top K tokens right so in this case if your top k equal to one it would only select United or it could only decide on selecting United if you had top k equal to two you could do United or Netherlands Top Gear equal to three you could choose any of these top three right that is what the top K is actually doing and actually you can visualize that here as well okay and okay what I've done here is set top k equals zero that's because I don't want to consider top K because I'm already defining the the limits on the number of tokens to decide from using top P okay so I don't activate the top K there and then we have the the max not Max Max number of tokens to generate in the output so with each generation I'm saying I don't want you to generate any more than 64 tokens you can increase that right so the the max context window so that's inputs and outputs for this model we've already set it to it's a Max sequence Zone from earlier 2048 so you can you can go much higher than 64 that I've set here and then also we have this repetition penalty that's super important because otherwise this is going to start repeating things over and over again and so the default value for that actually is one in that you know we can see more repetition we stretch that to 1.1 and we're generally not going to see that anymore okay so let's run this so we say explain to me the difference between nuclear efficient and fusion so this is from an example somewhere I think it was hugging face but I I don't actually remember where I got that from exactly anyone does know feel free to to mention that in the comments so we have the input okay so we said return return for text so we have the input here and then we also have the output so nuclear efficients process that splits heavy atoms into smaller lighter ones so and so on nuclear fusion occurs when two light Atomic nuclei are combined as far as I know that is is correct so that looks pretty good and then I've also added a note here on if you'd like to use the Triton optimize implementation so Triton in this scenario as far as I understand is the way that the attention is implemented it can be implemented either in pi torch which is what we're using by default it can be implemented with flash attention or using Triton and if you use Triton it's going to use more memory but it will be faster when you're actually performing inference uh so you can you can do that the reason I haven't used it here is because the insultates just an insanely long time so I just gave up with that but as far as I know this sort of setup here should work so you could install Triton and you go through and then this this should work okay just be wary of that added memory usage so yeah we've seen okay this is how we're going to use this in the hugging face so generating text now let's move on to the line chain side of things so how do we implement this inline chain okay so we're going to use this with the simplest chain possible so the LM chain for the LM we're going to initialize it via the home face pipeline which is basically local hooking face model and for that we need our pipeline which we have convenient currently already initialized up here so we just pass that into there we have our prompt template okay it's nothing right as you see instruction here so basically we have some input and that's it I'm just defining that so that we can Define this as a lamp chain okay we initialize that and then we come down to here and we can use the LM chain to predict and for the prediction we just passed in those instructions again okay so same question as before so in this case we should get uh pretty much the same answer so we can run that okay in the output we get there is this so as far as I can tell it's pretty much the same as what we got last time Okay so looks good and with that we've now implemented MPT 7B in both hugging face and also line chain as well so naturally if you just want to generate text you can use hunting face but obviously if you want to have access to all of the features that line chain offers all the chains agents all this sort of stuff then you obviously just take on this actual set and you have your originally hooking face pipeline now integrated with line chains which I think is is pretty cool and super easy to do it's not not that difficult so with that that's the end of this video we've explored how we can actually begin using open source models in line chain which I think opens up a lot of opportunities for us you know fine-tuning models just using smaller models maybe you don't always need like a big GT4 for all the all of our use cases so I think this is the sort of thing where we'll see a lot more more of going forwards a lot more open source smaller model is being used I of course I still think open AI is going to be used plenty because honestly in terms of performance there are no open source models that are genuinely comparable to GT 3.5 or GT4 at the moment you know maybe going forwards there will be eventually but right now we're not quite there so yeah that's it for this video I hope all this has been interesting and useful thank you very much for watching and I will see you again in the next one"
    }
}