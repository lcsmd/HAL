{
    "kCL4JuRPD6U": {
        "title": "The Easiest Way To Add Memory To Your Chatbot (Pinecone Tutorial)",
        "thumbnail": "https://i.ytimg.com/vi/kCL4JuRPD6U/hqdefault.jpg",
        "author": "VRSEN",
        "date_of_release": "2023-03-31T18:16:03Z",
        "duration": "PT6M41S",
        "view_count": "14244",
        "like_count": "602",
        "comment_count": "80",
        "description": "Supercharge your chatbot with limitless knowledge using Pinecone vector databases ‚Äì unlock the ultimate conversational experience!\nCode: https://colab.research.google.com/drive/1IHz_oCjICU-8e3EsWAQu0IEk8i1eXj8c?usp=sharing\n\nüöÄCustom AI Solutions: https://www.vrsen.ai/\n\nüìßFor all sorts of projects, reach out to me via email on the \"About\" page of my channel.\n\nü§ùConsulting: https://calendly.com/vrsen/ai-project-consultation\n\nüê¶Twitter: https://twitter.com/__vrsen__\n\nAbout: Discover how to expand your chatbot's knowledge with Pinecone, a popular vector database solution, in this tutorial. Learn about vector databases, embeddings, semantic similarity searches, and how to create a Q&A chatbot with OpenAI and Pinecone for a unique conversational experience. Unlock the full potential of ChatGPT and chatbots by integrating Pinecone, enabling users to chat with your documents for unparalleled knowledge access and an exceptional conversational journey.\n\n0:00 - Intro\n0:21- What are vector databases?\n1:55- Pinecone overview\n2:25 - Install\n2:50 - Getting the Documents\n2:54 - Splitting the Documents\n3:47 - Generate Embeddings\n3:56 - Upload to pinecone\n4:31 - Querying\n4:57 - Writing Prompts\n5:24 - Running the model\n6:04 - Demo\n6:30 - Outro",
        "transcript": "with the Advent of GPT models Vector databases have gained increasing popularity for one simple reason token limits we deliver value for your business large language models have to know as much as possible about your operations however most businesses have such a vast amount of information that incorporating it all within a single prompt is simply not feasible this is where Solutions like pinecon come in Vector databases allow you to expand your chatbot's default knowledge base with your own custom data almost indefinitely and I'm certain that most applications like charger VT plugins that are built on top of these models will utilize some kind of a vector database in the future so in this video Let's delve into one of the most popular Vector database Solutions called Pinecone so you can make the most out of your chatbots and offer your users a truly unique conversational experience but first let's do a quick overview of pinecon and Vector databases in general first and foremost Vector databases are designed to organize your embeddings for efficient semantic similarity searches embeddings are numerical representations of your data typically generated using large language models these representations encode the minion in the context of your text into a specific sequence of numbers called vectors semantic similarity search is a process that compares embeddings to each other for example you can compare the embedding of the user's query to all other embeddings in your database to quickly find most relevant items what sets this approach apart from traditional searches is that items are also matched based on their underlying context rather than on specific keywords for example if a question asks about the orbit of the Sun but substitutes the word sun with a massive ball of fire the semantic search is still likely to locate the relevant text defined in the sun's orbit in contrast traditional search algorithms would struggle with this type of query this is extremely useful in chat applications and conversations users do not typically ask queries based on specific keywords as they do with Google instead they ask questions that Express a particular meaning or intent this is why pinecon has recently become so popular pinecon provides a production already efficient solution that seamlessly integrates with AWS us or Google cloud and can be accessed with sdks or an API its pricing is based on a number of ports required by your application ports are essentially pre-configured Hardware instances running the pinecon servers for your needs as you create more pots you benefit from increased storage capacity lower latency and higher throughput fortunately for us pinecon also offers a free tier that is more than sufficient for the development of most projects now let's dive in in this tutorial we will build a q a chatbot with open AI that allows you to answer questions based on your documents from Google Drive the first step is to install the necessary packages we will be using only three additional packages which are openai pinecon client and python docs the first two are self-explanatory and the python docs package is needed to process the word documents next let's import them with some standard python packages that we will also need later on the next step is to Mount Your Google Drive and Define the path of the folder with your documents to ensure that our documents do not exceed the token limit we must divide them into smaller chunks in in this tutorial I will utilize 10 reports collected by a client for whom I am currently developing an application just like this the main goal of the app is to ask questions based on the strands and extract some hidden insights it is a good practice to split documents into semantically relevant chunks typically paragraphs work well since each paragraph usually makes a certain distinct Point therefore if the query is related to that point we can retrieve complete information that the model can use for the response in this example each trend is a new paragraph which is perfect for generating embeddings to do so we simply need to split these paragraphs and embed them using open AIS embedding model first we iterate over the folder and read each document then we extract paragraphs as separate chunks and store them in the list finally we remove any chunks that are shorter than 10 words as they are unlikely to provide much useful information we can now generate the embeddings using the open AIS text embedding r.002 model which is extremely cost effective and outperforms all other previous embedded models to upload our data to pinecon we first need to create an account and login then navigate to API Keys click create API key and copy the value next pass the API key as the parameter in the following method to initialize the python client then let's create our first index and connect to it finally we can upload our data to the index in batches of 64. for each batch we need metadata IDs and embeddings in our case the metadata will simply be the next chunk however for more complex use cases additional Fields such as the amount of tokens that this text Will consume might be useful once the batch is ready for upload we can simply call the index absurd method with the vectors that we have defined once our data is uploaded we can attempt to query the results with some example questions to find the most relevant documents to our question we must first create an embedding of the question itself then we can perform a semantic similarity such decompass the embedding of the question to all other embeddings in our index the top key parameter defines how many close matches will be returned and include metadata parameter is needed to get our text back with the response this should give us some documents that are most similar to our query the final step is to use these documents for the context in our prompt we first query the database with the method that we just defined and then we extract the text from the metadata a good starting point for the prompt is answered the question as truthfully as possible using the context below and if the answer is not within the context say I don't know this way we prevent the model from answering random unrelated questions and from college students who want to write essays for free using our app then we simply insert the text chunks below and return the result finally we can run the model with our newly constructed prompt for contextual q a applications I found that gpt3 provides better responses than charger PT although it is not fine-tuned on human feedback gpt3 is larger and therefore it is better at processing long texts if you would like to use charge GPT simply change the endpoint and set the prompt as the user message the system message is not necessary make sure to set the temperature to zero to prevent the model from generating any made up information now let's run the model and analyze the response since the reports contain information about social media let's ask what the top social media platform will be in 2023 it's seems like the top platform according to trance is expected to be Tick Tock unfortunately the entire code base can also be easily integrated into a web application here is the example that I have delivered to my client this app will later on be embedded on client's website as an iframe I also made similar examples for stripe open HubSpot production versions usually involve a lot more data processing to get the best results if you would like to create an app like this for your own website feel free to reach out to me that's it for today thank you for watching and if you want to know more about how to leverage the power fi don't forget to subscribe foreign [Music]"
    }
}