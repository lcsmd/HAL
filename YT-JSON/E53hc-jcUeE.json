{
    "E53hc-jcUeE": {
        "title": "How I Built a Medical RAG Chatbot Using BioMistral|Langchain | FREE Colab|ALL OPENSOURCE #ai",
        "thumbnail": "https://i.ytimg.com/vi/E53hc-jcUeE/hqdefault.jpg",
        "author": "DataInsightEdge",
        "date_of_release": "2024-02-23T02:14:44Z",
        "duration": "PT24M14S",
        "view_count": "4320",
        "like_count": "137",
        "comment_count": "11",
        "description": "In this video, we delve into the BioMistral model, a cutting-edge development aimed at constructing robust applications using LangChain and free Colab resources. Our focus will be on leveraging heart health data to generate insights through the Large Language Model (LLM), enabling users to pose specific questions across various medical domains. Having personally tested the model, I can attest to its promising capabilities. BioMistral stands out as an open-source LLM tailored for biomedical tasks, utilizing the Mistral foundation model and trained extensively on PubMed Central data. Through a meticulous evaluation involving ten medical question-answering tasks in English, BioMistral showcases its superiority over existing open-source medical models and competes admirably against proprietary alternatives. Furthermore, the study delves into techniques for creating lightweight models through quantization and model merging. Addressing the challenge of limited data availability beyond English, the study pioneers a multilingual evaluation by automatically translating and assessing the benchmark in seven other languages, marking a significant milestone in the evaluation of LLMs within the medical domain.\n\n#langchain #opensourcellm #llm #mistral #largelanguagemodels #opensourcemodels #chatbot \n\nLINKS:\nCode:https://colab.research.google.com/drive/1Jk7M4N8O4kUEhHQSk5-J7bKFg0nod43k?usp=sharing\nHuggingface Model:https://huggingface.co/BioMistral/BioMistral-7B\nHuggingFace Embeddings:https://huggingface.co/BAAI/bge-large-en-v1.5\nhttps://www.youtube.com/watch?v=7lP7fune0Gw&t=16s\nhttps://www.youtube.com/watch?v=vBC6Ym0cb0Y&t=2136s\nhttps://www.youtube.com/watch?v=ZKW6tWNaJRI&t=74s",
        "transcript": "[Music] hey everybody so in this video we are going to explore a new model called the biom MRA model to build a rag application using L chain and free cab so we are going to use a PDF document that contains a Hearts data right so we are going to F this data into the large language model to answer questions that relates to the heart so with this kind of application you can include any medical data from any specific medical domain to answer a specific question right so I've already tested this out and it looks promising so the biom is an open- Source large language model specifically designed for biomedical application is built upon the Minal Foundation model and pre-train on the pet Central data through a rigorous evaluation on 10 established medical question and answering TXS in English the biom myal demonstrate a superior performance compared to existing open-source medical models and competitive performance against its counterpart so the study explores the creation of lightweight models through quantization and model merging techniques so IT addresses The Limited availability of data Beyond English and to assess the multilingual generalization of medical llms so it means that it automatically translated and evaluated this Benchmark into seven other languages so this marks the first large scale multilingual evaluation of llms in medical domain all right enough of that so let's get into the collab and see how this model performs all right so before we do that I'm going to briefly take you through how this works so the architecture we are going to upload a PDF in this case we are going to upload this Health Data so it is a 95 page healthy heart data so we are going to specifically upload this data to be passed to the LM to answer question that relates to the heart health all right so it has information like the blood pressure the congestive heart F so it has contain a lot of information with the heart okay so we are going to in let me take you to the so this going to be the PDF uplo uploaded in this case going to be the health health data we are going going to extract the formation from the PDF and we're going to split that into chunks and we are going to create Chun one Chun two Chun three Chun four like that so for each of the chunks you're going to create embedding for that to refer into the light language models so we are going to store that into a knowledge base so we creates a semantic index in here so so we are going to find relevant document based on the similar ities and information from the original question all right so that so for the knowledge base the vector store we are going to use the chroma so we can use the files and other open source Vector storage all right so so that when the user asks any questions it comes in here and creates a burden just like this one and try to create understanding into the knowledge base it gets a similarity and it passes that to be ranked so it ranks let's say the top four the top three and it passes that to the LM so in this case the llm is going to be the bio mistal okay so it then creates the best answer and passes that to the user okay it's pretty simple all right so in here is the the collab right so we are going to need to P install L chain the phas CPU in this case need this one because we are using the chroma change sentence Transformers chroma DB to create a vector store the Lama CPP we are going to load the model to the Lama CPP and we are going to use the L chain community and the P PDF to read a PDF all right uh let me rerun this again so we import the libraries we import the pi PDF directory loader to load a PDF so we load out from the Lang chain community and we do the car tax split Tex splitter okay this case we only need the all right so we you can either use any of these the character text splitter or the cive character text plator so we have to display the text so we use the handing phase embedding to load the to create the embedding so this case you you can use any of these Vector storage we are going to use the chroma and the Lama CPP to load our llms in this case the bial and we are going to create the chain using the retrial keyway chain so I'm going to run this right so so we import the part flip so the text wrap we we want to wrap the the text into a markdown so that it be the outputs become so nicer all right so let me run this right so we want to store the get the API key I'm going to show you in a second so in order to get that we import OS we from get pass import get pass all right so let me run this I'm going to show you how this works all right so in order to get the API key from Huggy face you first go to your Huggy face go to settings and you go to assess tokens in here it's going to be your assets token so you can just copy that and you come in here okay so you click on the key sign and you can see the here is tells you add new secret key so you can put a name here for your key so in this case is the hacking face H API token and you paste your API key here all right and you can choose any of this to turn it on or off all right so we import OS and we say os. embar is equal to the Hang phub API token so you can grab the API key just by running this code so the next thing we need to do is we import the document the PDF document in this case I've already uploaded it right I've created so we just copy the folder in which is which contains the PDF document so you say copy path and you paste it in here all right so so we use the pi PDF directory loader to load the PDF file and we create a variable called the docs it we load it using the loader. load so let me run it right so I already have the documents here so when I run the documents you can see all the documents can be seen here so when you move it up move it up you can see the documents here so what we want to do next is to split that into chance so we create variable called a text splitter use a c C splitter to split that so we make a sign size of 300 and the will have to be 50 so when I run this so in here so when we split the documents we fed that into a variable called chunks so when we run this this is the length of so we have 747 chunks right so let's look at the first index so as you can see these are the chunks so this is the first index so you can see the your guide healthy heart Department of Health and Human Services so this is the index and let's look at the next one you can see the next one it's the next sank and the next s and the next sank right great all right so now we create the embance so we are going to say we are using the huging face embance from huging face and we are using this embedding model all right so you can get in here and choose a model um embedd model in this case I'm using the B AI the V1 one .5 version so I'm using this one so you can just copy that and you can paste in here okay so if run that all right it's creating okay we are done with that and now we want to create a vector store so for each of the text as you can see here chroma is going to be the vector store from document so for each of the chunks we've created up we want to create embedding for that embedding is nothing but converting each of the text into numbers so we can pass that to the llm in that case it's going to be the bio M okay it's running right so that is done so let's run it see right so we have the vector store created right so from Vector store. chroma so the next thing is let's perform a similarity search and see so I'm saying a query is equal to who is at risk of heart disease and when I run this right let's create a markdown so this is where the Contex can be found so an important risk factor for heart disease and other Medical this others okay so he try to find a similarity that is linked to the query okay so let's move on to the let's move on to create a retriever right so it's going to retrieve from the breor store it's going to retrieve the top four searches so in this case create a variable call a retriever and we pass the vector store so as a retriever we pass in the top four searches and when you run this right so we are saying that get relevant document from the query and let me run this so as you can see this is the query that I passed and I wanted to get the top four searches and you can see this is the document based content an important risk fact factor for heart disease and other means medical disorders right so it's giving me the top four searches so it say in document one one 2 3 4 all right so you can extend it anyhow you want it all right so in case I three like five let me see right so you can see one two three four five so these are the searches it contains the the top for of that relates to the query so the next thing is we now want to create the LM pull the llm so we are connecting that to the gole collab so I'm going to show you how you can get this model so I am running the biom 7B that is a key for GF so I'm running the ggf so if you want to pull that you can go to the m the bio on hugy face and you say bio M course I'm doing the ggf I'm going to go to the GG and you go to files so in here I'm using the km ggf so that would be 4 37 gab so you go here and you download it right so I have this in my Google Drive all right as you can see I have the model here so I'm going to load the model so I'm going to say google. cab I'm going to import test it's already imported so I'm going to say drive. Mount I'm going to I'm going to mount that okay so I'm going to say run this one so already connected to that right and now I'm going to do now is let me run the llm I've already run it so it's should not take a long time for the first time if you're running it it's going to take a long time so in here is my using the Lama CPP and this is the model path okay so in here is the model is St in my drive so as you can see my drive and I have the okay so you can see this is the model so in order to copy that you go to here you say copy path so you only need a path so you just come in here and you paste it okay it's really easy so actually it points to the direction where you have your model stored in this case I have it stored in my drive so I just have to copy the copy the path and paste it all right so the temperature is going to be 0.3 the max token is 24 the8 top P equal to one so you can just change it play around with it so it's already R so I'm going to create the RO pipeline or the chain so here I'm going to need the runnables so I'm using the L of schema runnables so runnable pass through it's just nothing but it's going to allow us pass the input or with the addition of P TR key so it's going to help us run the input and the output passer is going to be the pass the output from the model okay all right so let me run this and this is the template so the template we have the context so you are an AI assistant that follows instruction the streamly world so please do be truthful and give the direct answer so we are passing the user so this is the query going to pass the query and the assistant so this is the what we are going to get from the model that is the answers all right so when I run this so we create a prompt and we are passing the chat prom template so we pass the template that we've created pass the template into the chat prom template and now want to create the chain the r chain so in here we are passing the context the context is the retriever that we've created and the query is the runable pass through so it is going to use to pass the input all right and we are piping The Prompt that we've created and the llm and the output passer right to pass the output from the model all right and let me run this okay so the next thing we want to do is create a variable called the response and we want to pass the query into the r chain So when you say run run this it's going to take just a little time it's making everything ready so we can start asking questions chatting with the model all right so it's set up see you can load time sample time so now let's say response and let's see the output we saying why should I care about my heart health so when I run this right and is putting that into a markdown really nice so because because your heart is one of the most important organs in your body it's crucial to take care of it so a healthy heart can help prevent or manage many health problems like high blood pressure stroke or heart disease so there are many ways to take care of your heart like eating a healthy diet being Physically Active managing stress so taking care of your heart can also help you feel better now and on in the future I think I like this response really really awesome all right so let me say we just say what disease a disease affects the heart right so let me run this right so let me run this right so the heart is I say what disease affect the heart the heart is affected by many diseases some of which include the coronary artery diseases cardio myopathy endr enro citis all right guys U it has a lot of technical work so I wouldn't say that so I know hypertension and hard scenar stenosis well guys I really love this response so um so all so you can just do like uh put a while loop in here right so let me just run it and try something here so let's go into the PDF let me ask a question similar one from here okay all right so let me just say what is heart disease okay let me try this what is heart disease this disase it's enter all right so it's saying the Hat refers to a group of conditions that involves the hearts and blood vessels including the coronary artery disease heart failure arisis right guys I think it's perfectly aligns with what I we had just seen here which is a very great response yeah all right so let's get back to it you can see you can see this is awesome so it's saying the cenital heart defects can cause symptoms such as shortness of breath fatigue and swelling in the legs and can increase the risk of premature death right yeah so you can just continue asking several questions okay just keep playing with it and I'm going to T the link in the description so you can grab the file and play with it so let me know in this comment section how you think about this model and do well to subscribe and share and see you in the next one"
    }
}