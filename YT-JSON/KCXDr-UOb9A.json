{
    "KCXDr-UOb9A": {
        "title": "Large Language Models in Five Formulas",
        "thumbnail": "https://i.ytimg.com/vi/KCXDr-UOb9A/hqdefault.jpg",
        "author": "Sasha Rush ðŸ¤—",
        "date_of_release": "2024-01-30T16:01:43Z",
        "duration": "PT58M2S",
        "view_count": "33120",
        "like_count": "1192",
        "comment_count": "31",
        "description": "Tutorial on building intuition about LLMs. \nSlides: https://link.excalidraw.com/p/readonly/aBWlNjEckdUlrszwwo6V or https://github.com/srush/LLM-Talk/blob/main/Tutorial.pdf\n\n00:00 - Intro\n02:15 - 1: Generation (Perplexity)\n15:40 - 2: Memory (Attention)\n28:00 - 3: Efficiency (GEMM)\n38:40 - 4: Scaling (Chinchilla)\n46:37 - 5: Reasoning (RASP)\n55:33 - Conclusion\n\nDeveloped for an invited tutorial at the Harvard Data Science Initiative.\n\nNote: This tutorial is rather high-level and leaves out much of the scientific and citation history. There are other great guides that provide this in detail. My goal here was chalk-board level intuition.",
        "transcript": "hey everyone I'm good toing slightly different today normally I talk about new research topics I bring in a lot of citations and I go in really hardcore technical detail today I'm going to do something slightly different I'm going to present a tutorial on large language models it's called large language models in five formulas the tutorial is a bit casual I'm going to try to give you some intuition about how large language models work I'm going to do this by presenting intuition about five core formulas that help me understand language models in general so it goes without saying it this point that you've heard about language models you've heard about large language models you've heard about on device language models you've heard about solving math problems and generating code I can basically type any ridiculous thing into chat GPT and get a pretty coherent answer uh here I am asking to write a BC calculus question like it's a barber cutting my hair and frankly it does a relatively good job but I don't really have too much more to say about this topic instead I want to narrow in on the question of reasoning about larger language models my goal is to develop a language that lets me reason about how large language models work now the problem with this is that we do not really even understand how small language models work I'm not really an optimist at heart so I'm not going to tell you that we're going to figure this out very soon however I can say that there are some specific areas where we can reason about the behavior of large language models relatively precise so with that as a unifying theme the tutorial structure is to talk about large language models in five formulas now I won't leave you hanging I'll tell you the five that I chose in particular I'll have sections on perplexity attention gem chinchilla and rasp now if you haven't seen these before those will be pretty mysterious names if it helps these correspond to generation memory efficiency scaling and reasoning and just one more caveat before we begin I'm going to be focusing on uh conceptual understanding so I'm going to simplify a lot of details and probably get things wrong you should think about this as acting in a kind of frictionless environment this is a kind of idealized version of language modeling that focuses more on understanding how to think about the system than on the specifics okay let's begin the first section is about generation and we're going to focus on the formula for perplexity for this section we're going to use a simplified version of language we're going to assume that we have a collection of documents each of these documents is made up of exactly a thousand word tokens we're going to also use a simplified version of language this version of language will have 10,000 word types think of this as our dictionary we Define a language model as a probalistic model of a document it gives the probability of the tokens X1 through XT and it uses a set of parameters Theta I like this notation because it lets us isolate the parameters Theta separate from the probalistic model given that the Theta is going to be a giant neural network this lets us defer the problem of defining that Network to the next section of the talk given that the probability of the document is the joint probability of the word tokens we can utilize standard rules of probability to write the distribution however we would like a common way to WR write it is to split it into the product of its conditionals we do this by factoring it left to right as the prob ility of X1 X2 condition on X1 Etc until we have the entire probability distribution once we do this we can parameterize the individual conditionals in particular we form what is known as an auto regressive language model this is a predictive model where we predict the next token conditioned on the previous tokens we call it auto regressive where Auto refers to the fact that we're feeding back in previous predictions and regressive refers to the fact that we are predicting the next token more tangibly we can think of each of these conditional probabilities as producing a distribution over 10,000 different choices that is we are assigning a probability to every word in the dictionary I'll represent this as a histogram over all possible next word choices one thing that's nice about this joint distribution is that we can sample a document by sampling each of the words individually here's an example example where we sample words X1 through XT simply by sampling each word one at a time feeding it in as the conditional to the next step and then sampling the next word token this makes it a bit more concrete why we have an autor regressive process okay so we're about 5 minutes in and honestly I haven't told you anything new in fact basically everything I've said so far was figured out by Markov about 100 years ago this is a pretty old idea but it's important to get the basics down that being said this does allow us to talk about some of the assumptions that people used to make in language modeling that no longer hold in modern systems the first is an assumption that a language model has a fixed amount of history in particular it was very common until recently to assume that the probability of XT really only depended on a few of the previous words for instance we might assume that XT only depends on XT minus one this seems like an aggressive assumption but XT minus one definitely provides the most information about predicting XT and words further away outway less the second assumption I'll refer to as the categorical assumption this assumption is that the probability of the next word could be modeled roughly with a categorical distribution this assumption went away when people started using neural networks to model the probability of next word prediction in Shannon pioneering work in 1948 where he develops some of the first language models he actually produces a one-step categorical language model and samples from it in the way we have seen earlier in the talk this model is actually not so different from a lot of the language models that were developed before the modern Resurgence of neural networks and of course Shannon's model is not great but because language modeling is in some sense unsupervised learning it's kind of hard to quantify when a model is good or when it's bad to do this we have to basically give it unseen text and then check how close its predictions are to the predictions in that text we have to somehow compute a metric for this value and use it to compare different language models so the most naive thing that you might try is to Simply check the accuracy of your language model given a sentence like the dog walk to the blank we can look at the mode of our distribution and compare it to the true answer in this case we would have predicted Park where the true answer was lawn this means we get zero out of one which is I guess right but a bit unsatisfying we were pretty close we almost got the right answer but we get zero points this metric problem is Complicated by the fact that words and language follow what's known as a zipfian distribution roughly this means that very very common words make up of very large portion of the probability Mass but that very uncommon words are seen pretty frequently what is hard about this is that not every prediction is equal a lot of the time we're going to be predicting pretty common words like the or a but not too infrequently will we have to predict very challenging words like pizza or raincoat to motivate the system that's used in practice let's consider converting the probability distribution into a string of binary values for each possible work this conversion can be done deterministically but you can think about it as placing a B on each of the words you only have so many values you can allocate and so you have to utilize the probabilities to choose the length of the string that you place for each word so in this example here maybe we place a short string one 01 on the word part and a somewhat longer string 1 0 0 1 0 one on the word lawn words that are totally irrelevant might have very very long strengths but in general we have a binary number for each possible word that we predict you can show that the optimal length of these strings will be roughly negative log base 2 of the probability in the histogram that explains why words with very low probability will have very long strings and the closer we are to one the closer we are to a short string this is not just a theoretical conversion these strings literally give us a way to compress the underlying language and communicate it to a party that has access to our language model this conversion between probability and bits provides us with the main metric that's used in language modeling known as perplexity for historical reasons perplexity is given as two to the average number of bits per word in our heldout test set it can be comp computed with the formula below where we simply compute the probability of each of the true next words take a log base 2 average negate and then send to a power of two this puts it in a slightly easier form to work with for users of language models to commit you of this let's look at some examples first let's consider what it means if our perplexity is one this implies that the number of bits needed per word is actually zero how how's that possible don't we need to communicate something about the next word well in this case we actually don't if our perplexity was one the person we're talking with basically knows the next word it's always whatever had the highest probability we don't actually need to communicate anything alternatively let's say our perplexity is 10,000 this implies that we need a string of size log base 2 10,000 to communicate any word in our dictionary that means we're not really getting any advantage from language modeling at all it implies that our model is basically uniform if we were kind of trying to guess the next word we would basically have to roll a 10,000 sided dice I'll also note that our perplexity could be greater than 10,000 this in particular could happen if our model was overly confident about the wrong prediction it might assign a very short code to the wrong next word and an extremely long code to the correct next word this would lead to a very bad perplexity value as we would be spending a huge amount of bits communicating the next word which we thought could never actually happen for a long time people studied the problem of language modeling using a corpus known as The Wall Street Journal Corpus you can think about this as using a couple years of newspaper articles as your training data and then trying to assess your perplexity on today's newspaper if you do this with a uniform distribution you have roughly a perplexity of 10,000 if you give yourself access to the previous word you get down to around 600 if you have access to the previous two words you can get nearly around 200 if you give yourself the four previous words and uh about 40 Years of clever tricks you can get all the way down to about 140 when deep learning started looking at the language moding problem early Markoff neural networks got to a similar performance of around 140 when people started developing early non-m Markoff neural networks that looked at longer ranges they could get down to around 100 or even lower this is greatly simplifying uh very rich and interesting literature but it gives you a rough sense of about where things stood around 2015 but in 2015 why did people really care about this problem at all it first the answer was because language modeling was a good proxy that could be directly utilized for or other tasks that we did care about for instance the task of machine translation for example translating a sentence from French to English could be posed as a conditional language modeling problem instead of just conditioning on the previous words you could also condition on the Fred sentence you could then measure the conditional perplexity of generating the English words conditioned on the Fred input what researchers found is that measuring the perplexity in these systems correlated very strong ly with actual Downstream performance so in this table here we have a bunch of different perplexity values for translation experiment and we have the corresponding translation accuracy measured with a bespoke measurement known as blue score what researchers found was that as the perplexity got lower the blue score on this task would become better however if this was all that happened very few people would care outside of NLP the major result that people found next was that perplexity on the task of language modeling by itself could be used to produce models that would be really good at tasks that the model had never seen or had just seen a few examples of this table demonstrates an interesting result where just raw language modeling perplexity goes down from 5.84 to 3.23 over a series of experiments at the same time three other very different Downstream tasks all get significantly better these tasks were not included as part of the original training data but instead we're given as a small set of examples after the fact just by reducing the perplexity on general purpose language the model could then be used on these tasks this idea of course now underlies all of modern large language modeling research and in fact in modern papers like llama 2 sometimes they really just show the perplexity and people trust that things will work really well this table demonstrates the fact that four different lamama 2 models of varying sizes each have better perplexity we go from a final perplexity of about 1.8 down to about 1.5 the bottom model llama 2 70 billion Still Remains one of the best open- source large language models and we can trust that because it has an extremely good General perplexity as as we've seen earlier a complexity of 1.5 means that the model is really capturing the distribution of English well and while it's not totally a fair comparison we can go back and apply GPT 3 to the challenging Wall Street Journal test Corpus you get a perplexity of 20.5 um major leap from some of the earlier language models applied to this task but I'm getting ahead of myself uh I still have been told you how you actually get from Shannon's model to gpt3 and to do this we're going to need to remove the two major assumptions we're first going to have to move to the use of neural networks and then we're going to need to figure out how we can take into account all previous word tote this brings us to section two which focuses on memory and in particular the use of attention so for this section we're going to dive into the Theta we're going to try to understand better how to create a neural network that can power the probabilistic model that predicts the next word to do this let's go back to our markof assumption and assume we're only looking at the last previous two words but get to utilize a neural network to make the next word prediction do this we're going to use the following functional form we're going to run a neural network over XT minus 2 and X tus1 that will then produce a vector which will send to the softmax function the softmax function will convert that Vector into a distribution over 10,000 Poss classes this will then represent our language model distribution and tell us which words we think will come next well it took a while to figure out the best neural networks to use looking back the form of all these networks is relatively straightforward we're going to encode xt- 2 and XT minus1 as one hot vectors that is vectors that have zeros for every position and a one for the position of the word token they represent well then then feed those two vectors into a neural network that neural network will process them in the standard form and it will output another Vector of size 10,000 we learn the whole thing on language and all in all it's probably about five or 10 lines of P torch after going through the neural network we have to transform the output into a distribution over our vocabulary this is done by applying the softmax function which ensures that the output is positive and sums to one we do this by exponentiating each element of the vector and then it normalized this produces a histogram like we saw in the first section but obviously I'm being pretty casual about this process there were many forms of early neural network language models and they all had innovations that made it possible to get to this point a particularly famous one is known as word toac this model came out exactly 10 years ago and it demonstr ated a lot of techniques that became foundational to later models one thing to note as we'll talk about in the next section is that it was much harder to build models like this at that time uh particularly because the compute infrastructure and Hardware was less developed however once you have the language model infrastructure you can start putting in more data and start building larger networks in particular you could take the infrastructure of one of these marvian neural network based models and simply replace the internals with a much larger neural network however this alone doesn't really seem to be enough one of the problems is that while the post words are particularly important there are many bits that are hard to recover without looking at longer term context so in particular even if you have a very large neural network you might not have enough information in the last two words to really make a very good prediction about the next word that is coming up to make this more concrete let's look at get our running example if we're trying to predict the next word for the dog walk to the park and we're only allowing ourselves the last two words we get to a point where we have to predict the blank word only from to the this tells us that it's a location and a noun but it really doesn't tell us much about the semantics of the sance itself we don't know who is going or what the verb was and that information can really help us get some of the harder bits in this prediction problem one famous type of language modeling problem are examples where there is a proper noun that is going to fill a slot but that proper noun was mentioned much earlier in the document consider for example reading a newspaper article where you mention a person's name in the intro paragraph later you might bring up the same person and need to recall what their last name is in theory this is relatively rare and mostly you can get the easy bits just by saying some proper proun however to get the hard bits of exactly who that person was requires a very long-term memory these and similar examples really motivate the use of fully autor regressive models these are models that have the ability to utilize all previous tokens this obviously is a pretty simple idea and there have been many different models that have tried this we're going to focus though on one particular use of an approach known as attenion that allows us to build fully autor regressive models of relatively long range you might first ask why a simple neural network couldn't just do this can't we just take the Markoff neural network language modle that we saw earlier and make it much longer the problem is that if you do it this way you end up learning very specific information about absolute positioning for instance you might learn some particular information about position 7 but language doesn't really work that way there is no real specific information about position 7 it's going to depend on context and the dynamic structure that get constructed in the document we're going to focus in on one particular Solution that's at the center of all modern large language models this is an idea known as attention the way to think about attention is to think about a neural network version of random access memory or even simpler as a neural network version of a lookup table we're going to save all the previous information and then refer back to it as we need it as an example let's return to our simple sentence we're going to have the words the dog walk to the blank and we're going to want to use that history to predict the next word to do that we're going to need three different pieces of information we'll have one vector known as the query which has looked at the whole sentence so far additionally we'll have a lookup table which has a key and a value for each previous position based on the query we will match the key that we think will be most relevant to our next word prediction from that key we'll then extract the corresponding value that value will then be passed to a neural network which we can utilize to predict the next word in our sequence recourse steps the query matches the key the best match is selected and then we return the value of that key however this process has a foundational issue the problem is that we'd like the whole thing to be embedded within a neural network neural networks learn through the use of derivatives the problem is that the argmax operation that would be used to select the best key does it have a useful derivative if we write it as a one-dimensional function we can see that we get a flat structure with a derivative of zero instead we need another function that lets us softly select which key we would like to use the common approach to this is to Simply replace the artmax function with the softmax function that we saw before this softmax instead of producing a distribution over work word types produces a distribution over previous token positions this distribution is computed softly and has a non-trivial derivative at every location in 2D this softmax function can be drawn with a sigmoid shape which has a nice derivative at every location so here's our new process in step one we use the query to score our key instead of picking the highest scoring value we instead use a softmax to normalize the scores in step three instead of producing a single value we utilize the softmax to average over the different values waiting them by how well their key match the query here's what this looks like in practice we start with the same query key and value we then match the query to the keys to get a score for each location and we compute a softmax which gives us a histogram over these key locations we then use that histogram to average together the values this produces a new Vector that is some average of the different values weighted by the histogram the final step is exactly the same instead of using a single value we use the weighted value to predict the next word this process is fully differentiable and is a good way to learn a neural network that can decide which previous words are useful for the next prediction this attention operation was Central to the key work in language modeling known as attention is all you need this paper written in 2017 introduces a neural net architecture known as a Transformer that relies heavily on this attention step there are many extensions beyond the simple version that I've shown earlier but roughly the idea holds you have some way of computing Keys queries and values and then you use attention repeatedly until you're ready to predict the next word this parameterizes our language model and produces a nice neural network for predicting the next word distribution the diagram of the Transformer architecture has become quite iconic in the field it roughly consists of two stages the first stage is the attention that we just previously saw and the second stage is a rather large standard neural network these two stages are repeated many times before the final prediction is made this diagram looks a bit complex but note that this describes basically the entire GPT system given how important that is as a large language model it's actually surprisingly simple and I've told you what attention is but not why it's the best way to do this sort of long formed language model the tempting answer is to say that attention is kind of like memory and so it makes sense that this sort of architecture would actually win out in practice the real answer though is a bit different it turns out that the sort of attention that's used in the Transformer architecture happens to be very efficient and paralyzable it's a nice combination of long range dependency and something that runs fast on Modern Hardware to make this a bit more clear let me note that in practice we're actually going to be Computing several queries simultaneously at once these queries can be grouped together in a matrix in the same way the memory that has the key and Value Store can also be written as two separate matrices when we combine the query and the key and and compute the soft mags we end up with many different histograms representing all the combinations of the queries and the keys this set of histograms can be computed by running a softmax over the matrix product between the queries and the keys similarly these can then be combined with the value Matrix to compute the set of weighted averages simply by taking a matrix multiply between the softmax of the queries and the keys and the value Matrix this produces es each of the value outputs which are then used to predict the next word the punchline of this process is that the entire attention step can be written as two Matrix multiplies with a soft Max around the inner one going back to our three steps we can simply read off this mathematical formula them to see that the queries score the keys through a matrix multiply a softmax then normalizes the scores to produce the histograms and then the weighted average is taken with the values by Computing another Matrix multiply this whole thing is just a series of soft Maxes and Matrix multipli but then why is that actually efficient so far we've now described a full autor regressive language model this model is known as a generative Transformer but what I haven't told you yet is why this actually runs fast in practice in the next section we'll dive deeper into this Matrix multiply operation and see how do we actually make it run fast on gpus part three efficiency I've been bouncing around this question of why language models suddenly got so much better and if I'm honest the answer is pretty simple it's mostly because of gpus I can show you a graph of the speed of gpus over recent years but in some sense showing you the graph of Nvidia stock price gets the same point together over the last several years we've seen the core centrality of Hardware in the process of building bigger and more powerful large language models as a deep learning practitioner the rise of general purpose programming on gpus has fundamentally altered what sorts of models were possible to be built as we've seen earlier the softmax function is central for predicting the next word in the sequence as well as for its use in attention in order to compute the softmax function we need to normalize the distribution this means taking a a sum over every word type in our vocabulary the sum is surprisingly big it can be upwards of 10,000 different choices if we think about language models Circa 2010 there was lots of research into effectively approximating the denominator of the softmax function if we could come up with some way to efficiently approximate the denominator we could compute the softmax more efficiently on CPU Hardware but after the introduction and widespread use of gpus this totally changed it turns out that this denominator is pretty trivial to compute on gpus and all of a sudden we no longer had to figure out fancy ways of approximating this function a similar example is the calculation of matrix multiplication matrix multiplication is Central to every part of deep learning it's used within neural networks itself and we've seen that it's Central to the computation that's necessary for attention if we can map an operation into some form of matrix multiplication then we can certainly run it fast on new gpus I'm going to use this as a running example to teach you a little bit about how gpus work so first off what is a GPU at a kind of high level approximation you can think about this as just being a parallel computer GPU has many threads and they all run the same code simultaneously to make things less intimidating we'll think of each GPU thread as being a little robot the robot can do mathematical operations and it can read and write for memory within a GPU each one of these threads are grouped together into a block in this picture here we can see one GPU block and for this example it corresponds to 12 different individual threads each of these threads again has to run the same code but they can also read and write from a bit of memory that's seen by the entire block reading and writing to this memory is quite efficient finally the whole GPU consists of a grid this grid has all of the blocks that we've previously seen and each thread in the grid can additionally read from a set of global memory This Global memory is shared by the entire grid but it's quite inefficient to read and write from the main rules of GPU programming are that we're only able to have a limited number of threads per each block but the blocks will be quite important the blocks are essential because reading and writing from Global memory is much much slower than utilizing our block memory we're going to want to do as many operations as possible within the block as opposed to resorting to the global memory I think at this point you've got the main idea but let's go through some examples it would be very bad if each of the individual threads was reading and writing to Global memory by themselves you'd get parallelism but it would be really slow to calculate things in the ideal world we first load from the global memory into to our local block memory we then do some computation where we compute things within the block itself maybe read and write from the local memory several times and then when we're done write back out to Global memory with our final answers this is the main trick for GPU programming but it can be a little bit counterintuitive and seeing it applied in practice for the first time can be quite challenging to make things more tangible let's run through an example of 3x3 matrix multiplication for this example we're going to have two square matrices A and B both of these will be 3x3 matrices and we'll compute each element of a b by multiplying columns in a with rows in B and then summing up the results here's an example of computing the second row First Column here we multiply the First Column of a with the second row of B and then sum up the results if we were to do this naively we would have one thread compute each of the outputs in AB in order to compute this value we would do six Global reads the column of a and the row of B do each of the multiplications sum them up with the thread and then write it back out to Global memory note that doing this requires six Global reads for each thread a better method is going to be to first read from Global memory into the blocks memory we can then calculate important intermediate results within the block itself and then finally write back out to the final value let's look at how this works in practice in step one we read from Global memory into the block memory we'll do this by reading the whole Matrix into our block memory yielding 2 * 9 Global reads once we have the Matrix in our block memory we can have each thread do the same operations we saw before but now the read are from the shared memory not from the global memory this is much much faster in practice since the memory is shared different threads can reuse the same rows and columns to compute different positions in the output Matrix when we're done we can simply copy the shared AB calculated Matrix back out to Global memory for use in the next operation if we look at the number of reads with the naive method each thread did six Global reads and there were nine threads total yielding 54 Global reads if we do our Block Base method all of the reads happen in the first stack which yields 9 * 2 reads for a total of 18 this is many fewer per thread but that's the case where the entire Matrix fit into our block recall that I mentioned that blocks have to be a fixed size and so we can't scale this approach to arbitrarily large matrices you might ask then how you would do a 6x6 Matrix multiplic with blocks of the same size and the answer is that you end up having to do it in multiple steps for step one instead of reading in the entire Matrix we read in a 3X3 block of each of the A and B matrices again this yields 2x9 reads into our 3x3 blocks once we've done this we can calculate a part of the final value by multiplying together the three values of the column of the top Matrix and three values of the row of the bottom Matrix we then use a single thread reading from block memory to compute the cell once this part is done we copy in a new part of the two original matrices into our shared memory for the top Matrix we do the bottom part and for the bottom Matrix we do the right part we then use our threads to multiply together these components and sum them into the final value between the first and the the second part of this process we now have computed the full multiply between the row on the bottom and the column on the top this gives us the correct answer for the 3X3 Block in the a * B Matrix once this is done we can take our 3x3 block and write it back out to Global memory this gives us a 3X3 part of the full a by B output Matrix while this is happening other blocks are completing the rest of the a * B Matrix each of these are again only doing 2 * 9 reads each time in this case we end up doing 36 total Global reads per block in order to compute the full final Matrix and that's the main operation for this section I probably should have just called it matrix multiplication in practice this operation is often called gem when applied on gpus the GPU operation lets us to a generalized version of this Matrix multiplier that also allows us to add in an additional term and to scale the operation but you get the idea we can do this sort of low-level efficient matrix multiplication by exploiting all the power of gpus as I mentioned earlier it's really really hard to underestimate the importance of this operation for modern neural networks it's used in basically all the main parts of the system it's particularly important for the calculation of attension as well as the core neural network blocks that are utilized in a Transformer I've also only shown you the most basic form of matrix multiplication in modern gpus there's all sorts of specialized Hardware that is continued to make the calculation of Matrix multiplies even faster and faster with each release but you might wonder why we're focusing so much on speed isn't enough enough you can run a language bottle on your laptop isn't that good enough why do we have to optimize it so intensely you might wonder where all this compute is going and why people are fighting over buying up all the newest latest gpus part four scaling so we've talked about generative models and we've talked about Transformers in this section we're going to focus on the p pre-training in particular we're going to focus on the question of scaling these sorts of language models to be trained on lots and lots of data with very large neural network unlike some of the other sections of this talk the key decisions in scaling seem quite simple we have to decide how big of a neural network to use that means how many parameters should we try to fit in our neural network layers and we have to decide how much training data to use roughly how many documents or tokens should we train our model on training on more tokens allows the model to fit the data better and potentially have a better perplexity what's interesting about these two variables is that they form a multiplicative relationship if we take the neural network size and we take the training data the amount of total compute we need to dedicate to pre-training scales as the product of their two sizes you can think of this horis by thinking that each token that goes to the N Network needs to touch every one of the neural network parameters this is what forms the multiplicative relationship and form the total compute of the system we can see this relationship in three famous models in the Bert Bas model released in 2018 there were about 109 million parameters and the system was trained on 250 billion tokens this yielded a compute about 1.6 e to the 20 when we jump to a model like pal which was trained in 2022 it has 540 billion parameters and was trained on 780 billion tokens this yielded a total compute of about 2.5 * 10 24th so models are getting bigger they're being trained on more data and more importantly they're utilizing more and more compute if we can utilize the compute to the best purpose we can get better language FS so you might ask which of these variables we want to change in a paper on scaling laws researchers at open AI demonstrated that for each of these quantities the perplexity of the model is going to improve as a power law roughly this means that if we make a log log plot of perplexity versus each of these individual Powers we get a linear line showing the decrease in perplexity as we make a large increase in compute parameters or data size given this relationship a natural conclusion is just to increase all of these parameters as much as possible they all seem to help performance so let's just make them all as big as we can the the problem with this argument is that even if you're extremely GPU Rich you still have a compute budget and you have to determine how to best utilize the compute that you have available for instance these two diagrams both utilize roughly the same amount of compute or area in the diagram but the one on the left allocates more compute to utilizing more tokens to train on whereas the one on the right utilizes that compute for a model with more parameters how do we determine which one would yield the best perplexity in the end this is not really a theoretical problem for example the pal model which I mentioned earlier utilized a very large amount of parameters and actually relatively few tokens you might ask if this was the best thing they could have done or if they could have done better the approach we'll use to study this problem is to write down the formula for a power wall and then try to fit this formula to empirical curves that show perplexity if we can get a good fit we can maybe extrapolate onto how we should train a new model basically whether we should use more parameters or more tokens the formula looks a little bit complicated so instead let's draw it as a picture the formula tells us that our perplexity can be predicted as a function of some value a over the number of parameters we use to the alpha term plus a second term which is B over the amount of data we use to a beta term we then add in an additional e term which acts as a bias and corresponds to the best possible perplexity you could get for the language the key terms of Interest are the exponents on the model size and the data size this will tell us roughly how to scale our model this formula and its fit are explored in a paper known as chinchillo the main results of chinchillo is that the exponents for the model and the data the blue and the green box are roughly the same this implies that the best perplexity can be achieved with an equal scaling formula that is if we scale the data and the model in roughly equal proportions we'll be able to get the best perplexity for the least amount of compute the consequence of this result is that we can go back and look at some previous models so in particular we can note that the pal model which used a scaling that favorite parameters over data was maybe the wrong decision they ended up spending too much compute for the final perplexity and outcome of their Model A Better strategy would be to scale the parameters and the data equally you end up with something looking more like a square than a rectangle this allows you to get a very good model with less compute and often less dollars the notable figure of this work shows a graph with the continuing scaling of compute represented by flops in the line in the center of the graph the Gilla model which we've seen uh those equal scaling ends up being on this line where other models such as gpt3 or approaches like Megatron touring which is 530 billing parameters end up overparameterized for the amount of tokens that they use there of course lots of uh complexities here and other factors but in general it provides a very nice key for how people can produce better models a lot of the conversation in building large language models has centered around this notion of chinula scaling and how to take advantage of various constraints there are some really important caveats to this scaling property though the main one is that there is an asymmetry between model parameters and training data the training data is only utilized during the training process and is therefore a one-time cost however the model parameters have to be used for every every inference of the actual large language model so every time you actually call Cat GPT it has to basically use all the parameters of the model because of this it may actually be beneficial to use some of your training time compute to instead produce a model that's smaller and maybe utilizes more tokens this can lead to weird asymmetries where people produce models that may not be the best model they could compute for their constraints but end up leading to better actual applications the most notable example of this was the original L model this model was purposely suboptimal in terms of training compute but ended up producing a model that had many fewer parameters than some of the larger models with similar compute to do this they simply paid more upfront cost to train it on more tokens and then they distributed the smaller version of the model so just to summarize the key way we produce large language models is through this initial pre-training stage particularly when working with generative pre-trained Transformers compute is really the constraining factor and the best use of this compute can produce the model with the best perplexity even with gpus we are still limited to the amount of compute we have and so deciding on the best allocation both during training and for Downstream inference is a core problem and yet this is still a bit unsatisfactory we know that these models learn a ton from this pre-training stage but we're really just relying on the fact that perplexity goes down and therefore all sorts of tasks get better what people really seem to want to know now is what the models are actually doing what do they learn and how come they're able to perform so well on such hard tasks part five algorithms let's jump all the way back to this question of memory we discussed this example earlier in the talk where we have a New York Times article that mentions a person's name and then later refers to him in the article in order to solve this kind of problem you need a model that's both going to be really good at the easy stuff so able to figure out the syntax of the language and it structure but also able to do complex algorithmic tasks such as remember a previous name and then be able to use it based on context clues for many years this type of problem was actually considered very very very challenging for language models but we've seen that as perplexity goes down with GPT based models it is able to answer these questions correctly almost all at the time sometimes the complication of natural language can make these problems more challenging to study so people have proposed a bunch of synthetic tasks that simulate some of the interesting properties that language models display one interesting one is the problem of associative memory here the task is to look at the less than symbol in the context and generate whatever symbol came to the last of fact when you next produce it so in this case here we would generate the letter A which came before the first less than symbol this is obviously an abstraction of the original task um but it is a good representation of what makes the problem hard so you might ask how do language models learn to do this sort of task how are they able to make it work so well for so many different domains and honestly I actually have no idea uh there are lot of Papers written about this topic but I still don't feel like I've gotten a conclusive clear answer I'm a bit more comfortable with version of this question that look into how might language models do this so in particular they look to build abstractions that could represent similar properties just to show that it's either possible or impossible for a language model to accomplish certain goals there are many different formal systems for exploring the properties of attention based models but one I particularly like is known as rasp rasp is a formal language that allows us to write simple deterministic code that is guaranteed to be translatable into Transformer weights just as in the first section of this talk we looked at the probalistic properties of these models and abstracted Away the Transformer part in this section we'll look at the Transformer structure but abstract away the probalistic or stochastic part of the model basically we're going to be building these little finite State autom like machines that represent Transformer properties let me start by getting you acquainted with rasp the way it's going to work is we're going to write a very short program the program is going to describe the operation of one layer of attention if you recall attention has three parts a query key and a value those three parts are then combined to produce an output which then gets utilized to predict the next word or fed into another layer of attention in our WRA visualization we're going to have a square like the one shown on this Slide the key will be at the top the query will be at the left and the value will be at the bottom these three things will be simple lists of values and the key and the query will interact with each other through Boolean operations finally once we've combined the key and the query we'll use that to go over the values to produce the output the way this works is we simply see which keys and queries matched and then we sum up over the corresponding values this is obviously a simplification of the full attention process that we saw in section two but it can be used to build simple programs which can then be translated into Transformer weights okay so here's our first rasp program the way this program works is that it's going to sum up the number of words that came before each of the words in our sentence so our program says match each key that had an index that was less than the query index The Matrix on the bottom right shows everywhere that matched specifically for each row representing a query we match everything to the left of it and we get a matrix that looks like a lower triangular Matrix given that query key match we then pass in a value the value here is one and then we sum across all the gray boxes in that row that leads to the output on the right hand side consisting of 0 1 2 3 4 5 6 7 the number of words before each of the words in our example this obviously is a pretty simple program but it can be useful for building more complex programs and demonstrates at least that every element of a transformer can tell what its absolute position in a document is okay here's another program for this program what we'd like to do is we'd like to take our input and shift everything one word to the right to do this we match a key for the index to the query for the index minus one this leads to the Matrix on the bottom right which is an off diagonal under each of the words once we have that match we pass in the original tokens as the value this shifts them all to the right as we can see by the output the output consisted of summing across each of the rows which led to just selecting each of the words that was one off from its original position again we were able to show that a simple matching between the queries and the keys led to a matrix indicating the relationships between these and then finally when we pass in the values we were able to select a given portion of the original input that was useful for us the rasp language allows you to combine logic with these attention operations for instance in this match we were trying to match previous symbols that have the same token value as our current token we can you can see that the only thing that matches is the greater than operator that was used previously at the beginning of this sentence here attention is acting as a way to find different tokens that we may have used before in case we want their nearby values to fill in the next word with these three basic operators we can already start building interesting programs so this Ras program consists of two layers of attention the first finds the greater than operator earlier in the sentence and the second shifts one to pull out the value that was before that Operator by running this code we were able to get a final output of the letter A which was the value that was before the greater than operator arbitrarily far in the past with just two layers of these neural networks operating together we can start to build up pretty interesting programs here's that same program applied to a longer example here we're able to read this whole input and figure out that Q was the token that was before at the previous greater than symbol again this is a simple example but with the few lines of code we're able to simulate what a Transformer may have had the ability to do and get a better sense of what this inner circuitry may have looked like you can go online and find some really interesting examples of rasp programs for instance here's a six- layer model that is able to implement a complete adding circuit this can add arbitrarily long decimal digits and produce the correct answer while I'm particularly interested in the capabilities that a Transformer may have people have also explored some of the attacks that you could perform on large scale language models uh this particular example it's particularly neat they were able to take the adding circuit from the previous slide and show that they could add a back door to that circuit it's a relatively complex WRA program but they're able to show that for particular inputs they can cause the model to not add but instead output a mean message I do feel like this section to come with a major caveat though while rasp is really exciting and we're able to build interesting programs and even compile them to real Transformers we're really not very close to Turning realistic networks back into rasp code it is possible that Transformers in practice are learning very different or even complex or random versions of these sort of circuits and we may not be able to isolate them or pull them out from the system it's also possible they're learning to do a lot of these operations in completely different ways that are not intuitive to us but that being said it's still pretty neat and I think people should check it out so end with a short conclusion so we started with this wild example Chad GPT is able to take a pretty ridiculous question produce an amazing answer and it does this by basically just training a very large language model on a big machine for a very long time we do have a relatively good handil on how each part of this system works from generation to memory efficiency scaling and we're beginning to get a better grasp on its internal reasoning but in a global sense we still know very very little even for each of these individual Parts while I'm pretty comfortable each of the five formulas I've listed will continue to be pretty important there's already starting to be all sorts of different properties that people are exploring for the first section on perplexity we're seeing all sorts of work into incorporating human feedback into language models which Chang is the objective and the goals when we talk about attention there are all sorts of other models that are looking at different architectures or other ways to take into account long-term contexts some of which may be better and others which may just be faster we didn't really go into gpus in that much detail at all but there are all sorts of questions of new architectures or new features or specific new extensions for different machines when we talk about scaling there's this assumption that things will just keep on getting bigger and bigger but for a lot of Lang languages that's not possible it would be really amazing to produce models that could work with more human scale amounts of data finally I touched a bit on the rasp language and trying to understand how certain algorithms may be coded in Transformers but there's a massive area that looks at the circuits of how Transformers work and various ideas surrounding interpretability of large language models all of these are huge areas of research interest and you could explore almost any of them for many many years to come and then there's this whole other question of whether actually any of this matters we have this amazing technology and it just seems to work it seems to know language it seems to know how to do reasoning in all sorts of ways that are just surprising to us I think much of the interesting stuff of language models these days is simply finding applications or using them in various different domains maybe we won't get a good sense of their internals or or how to think about them uh even as we use them for many different tasks so and there uh thanks so much for listening and uh feel free to ask questions in the comments about further resources or other ways to get involved in any of these areas thanks so much"
    }
}