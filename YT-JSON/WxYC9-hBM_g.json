{
    "WxYC9-hBM_g": {
        "title": "Run your own AI (but private)",
        "thumbnail": "https://i.ytimg.com/vi/WxYC9-hBM_g/hqdefault.jpg",
        "author": "NetworkChuck",
        "date_of_release": "2024-03-12T17:03:38Z",
        "duration": "PT22M13S",
        "view_count": "1243010",
        "like_count": "50703",
        "comment_count": "2580",
        "description": "Run your own AI with VMware: https://ntck.co/vmware\n\nUnlock the power of Private AI on your own device with NetworkChuck! Discover how to easily set up your own AI model, similar to ChatGPT, but entirely offline and private, right on your computer. Learn how this technology can revolutionize your job, enhance privacy, and even survive a zombie apocalypse. Plus, dive into the world of fine-tuning AI with VMware and Nvidia, making it possible to tailor AI to your specific needs. Whether you're a tech enthusiast or a professional looking to leverage AI in your work, this video is packed with insights and practical steps to harness the future of technology.\n\n\nüß™üß™Take the quiz and win some ‚òï‚òï!: https://ntck.co/437quiz\nüî•üî•Join the NetworkChuck Academy!: https://ntck.co/NCAcademy \n\nVIDEO STUFF\n---------------------------------------------------\nOllama: https://ollama.com/\nPrivateGPT: https://docs.privategpt.dev/overview/welcome/introduction\nPrivateGPT on WSL2 with GPU: https://medium.com/@docteur_rs/installing-privategpt-on-wsl-with-gpu-support-5798d763aa31\n\n**Sponsored by VMWare by Broadcom\n\n\n\n\n\n\n\nSUPPORT NETWORKCHUCK\n---------------------------------------------------\n‚û°Ô∏èNetworkChuck membership: https://ntck.co/Premium \n‚òï‚òï COFFEE and MERCH: https://ntck.co/coffee \n\nCheck out my new channel: https://ntck.co/ncclips \n\nüÜòüÜòNEED HELP?? Join the Discord Server: https://discord.gg/networkchuck \n\nSTUDY WITH ME on Twitch: https://bit.ly/nc_twitch \n\nREADY TO LEARN??\n---------------------------------------------------\n-Learn Python: https://bit.ly/3rzZjzz \n-Get your CCNA: https://bit.ly/nc-ccna \n\nFOLLOW ME EVERYWHERE\n---------------------------------------------------\nInstagram: https://www.instagram.com/networkchuck/ \nTwitter: https://twitter.com/networkchuck \nFacebook: https://www.facebook.com/NetworkChuck/ \nJoin the Discord server: http://bit.ly/nc-discord \n\n\n\n\nAFFILIATES & REFERRALS\n---------------------------------------------------\n(GEAR I USE...STUFF I RECOMMEND)\nMy network gear: https://geni.us/L6wyIUj \nAmazon Affiliate Store: https://www.amazon.com/shop/networkchuck \nBuy a Raspberry Pi: https://geni.us/aBeqAL\nDo you want to know how I draw on the screen?? Go to https://ntck.co/EpicPen and use code NetworkChuck to get 20% off!! \nfast and reliable unifi in the cloud: https://hostifi.com/?via=chuck\n\n- Setting up Private AI on your computer\n- Offline AI models like ChatGPT\n- Enhancing job performance with Private AI\n- VMware and Nvidia AI solutions\n- Fine-tuning AI models for specific needs\n- Running AI without internet\n- Privacy concerns with AI technologies\n- Surviving a zombie apocalypse with AI\n- VMware Private AI Foundation\n- Nvidia AI enterprise tools\n- Connecting knowledge bases to Private GPT\n- Retrieval Augmented Generation (RAG) with AI\n- Installing WSL for AI projects\n- Running LLMs on personal devices\n- VMware deep learning VMs\n- Customizing AI with VMware and Nvidia\n- Private GPT project setup\n- Leveraging GPUs for AI processing\n- Consulting databases with AI for accurate answers\n- VMware's role in private AI development\n- Intel and IBM partnerships with VMware for AI\n- Running local private AI in companies\n- NetworkChuck's guide to private AI\n- Future of technology with private and fine-tuned AI\n\n\n**00:00** - Introduction to Private AI and Setup Guide\n**00:56** - VMware's Role in Private AI\n**01:50** - Understanding AI Models and Exploring Hugging Face\n**02:54** - Training and Power of AI Models\n**04:24** - Installing Ollama for Local AI Models\n**05:24** - Setting Up Windows Subsystem for Linux (WSL) for AI\n**06:53** - Running Your First Local AI Model\n**07:23** - Enhancing AI with GPUs for Faster Responses\n**08:02** - Fun with AI: Zombie Apocalypse Survival Tips\n**08:28** - Switching AI Models for Different Responses\n**09:04** - Fine-Tuning AI with Your Own Data\n**10:50** - VMware's Approach to Fine-Tuning AI Models\n**12:53** - The Data Scientist's Workflow with VMware and NVIDIA\n**15:23** - VMware's Partnerships for Diverse AI Solutions\n**16:26** - Setting Up Your Own Private GPT with RAG\n**18:08** - Bonus: Running Private GPT with Your Knowledge Base\n**20:55** - The Future of Private AI and VMware's Solution\n**21:28** - Quiz Announcement for Viewers\n\n\n\n#vmware #privategpt #AI",
        "transcript": "I'm running something called private\nai. It's kind of like chat GPT, except it's not. Everything about it\nis running right here on my computer. Am I even connected to the internet? This is private contained and my data\nisn't being shared with some random company. So in this video I\nwant to do two things. First, I want to show you how to set this up. It is ridiculously easy and fast to run\nyour own AI on your laptop computer or whatever. It's this is free, it's amazing. It'll take you about five minutes and\nif you stick around until the end, I want to show you something even\ncrazier, a bit more advanced. I'll show you how you can connect\nyour knowledge base, your notes, your documents, your journal entries to your own\nprivate GPT and then ask it questions about your stuff. And then second, I want to talk about how private AI is\nhelping us in the area we need help Most. Our jobs, you may not know this, but not everyone can use chat GBT\nor something like it at their job. Their companies won't let them mainly\nbecause of privacy and security reasons, but if they could run their own\nprivate ai, that's a different story. That's a whole different ballgame and\nVMware is a big reason. This is possible. They're the sponsor of this video and\nthey're enabling some amazing things that companies can do on-Prem in their\nown data center to run their own ai. And it's not just the cloud man,\nit's like in your data center. The stuff they're doing is crazy. We're\ngoing to talk about it here in a bit, but tell you what, go ahead and do\nthis. There's a link in the description. Just go ahead and open it and take a\nlittle glimpse at what they're doing. We're going to dive deeper, so just go ahead and have it open right\nin your second monitor or something or on the side or minimize. I\ndon't know what you're doing. I dunno how many monitors you\nhave. You have three Actually, Bob, I can see before we get started,\nI have to show you this. You can run your own private ai. That's\nkind of uncensored. I watch this, So yeah, please don't do\nthis to destroy me. Also, make sure you're paying attention\nat the end of this video, I'm doing a quiz and if you're one of\nthe first five people to get a hundred percent on this quiz, you're getting\nsome free coffee network. Chuck Coffee. So take some notes,\nstudy up. Let's do this now real quick, before we install a\nprivate local AI model on your computer, what does it even mean? What's\nan AI model? At its core, an AI model is simply an artificial\nintelligence pre-trained on data we provided. One you may have\nheard of is open AI's Chat GBT, but it's not the only one out\nthere. Let's take a field trip. We're going to go to a website\ncalled hugging face.co. Just an incredible brand\nname. I love it so much. This is an entire community dedicated\nto providing and sharing AI models and there are a ton. You're about\nto have your mind blown. Ready? I'm going to click on models up here. Do\nyou see that number? 505,000 AI models. Many of these are open and free\nfor you to use and pre-trained, which is kind of a crazy\nthing. Let me show you this. We're going to search for\na model named Llama two, one of the most popular models out\nthere. We'll do LAMA two seven B. Again, I love the branding. LAMA two is an AI model known as\nan LLM or large language model, open AI's Chat. GPT is\nalso an LLM. Now this LLM, this pre-trained AI\nmodel was made by meda, AKA Facebook and what\nthey did to pre-train. This model is kind of insane and the fact\nthat we're about to download this and use it even crazier, check this out\nif you scroll down just a little bit, here we go. Training data. It was trained by over 2 trillion\ntokens of data from publicly available sources. Instruction data sets over\na million human annotated examples, data freshness. We're talking\nin July, 2023. I love that term. Data freshness and getting\nthe data was just step one. Step two is insane because this\nis where the training happens. Mata to train this model put together\nwhat's called a super cluster. It already sounds cool, right?\nThis sucker is over 6,000 GPUs. It took 1.7 million GPU hours to\ntrain this model and it's estimated it costs around $20 million to train\nit and now made is just like, here you go kid. Download this\nincredibly powerful thing. I don't want to call it a being\nyet. I'm not ready for that, but this intelligent source of information\nthat you can just download on your laptop and ask it questions, no internet required and this is just\none of the many models we could download. They have special models like\ntext to speech, image to image. They even have uncensored ones. They have\nan uncensored version of a llama too. This guy George Sung, took this model and fine tuned\nit with a pretty hefty GPU, took him 19 hours and made it to where\nyou could pretty much ask this thing. Anything you wanted, whatever\nquestion comes to mind, it's not going to hold back. Okay, so how did we get this fine tuned\nmodel onto your computer? Well, actually I should warn you, this\ninvolves quite a bit of llamas, more than you would expect. Our\njourney starts at a tool called O Lama. Let's go ahead and take a field\ntrip out there real quick. We'll go to O lama.ai. All we'll have\nto do is install this little guy, Mr. Alama, and then we can run a ton of different\nLLMs Llama two Code Llama told you lots of llamas and there's others that are\npretty fun like Llama two Uncensored or Llamas. Tdrl. I'll show you in a second.\nBut first, what do we install alama on? We can see right down here that we\nhave it available on macOS and Linux, but oh bummer, windows coming soon. It's okay because we've got WSL,\nthe Windows subsystem for Linux, which is now really easy to set up. So we'll go ahead and click on\ndownload right here from os. You'll just simply download this\nand install like one of your regular applications for Linux.\nWe'll click on this. We got to fun curl command that will\ncopy and paste now because we're going to install WSL on Windows. This will\nbe the same step. So Mac OS folks, go ahead and just run that installer.\nLinux and Windows folks, let's keep going. Now, if you're on Windows, all you have to do now to get WSL\ninstalled is launch your Windows terminal. Just go to your search bar and search\nfor terminal and with one command it'll just happen. It used to be so much\nharder, which is WSL dash dash install. It'll go through a few steps.\nIt'll install Ubuntu as default. I'll go ahead and let that do\nthat. And boom, just like that. I've got Ubuntu 22 0 4 3 lts installed\nand I'm actually inside of it right now. So now at this point, Linux\nand Windows folks, we converged. We're on the same path.\nLet's install alama. I'm going to copy that curl\ncommand that alama gave us, jump back into my terminal, paste\nthat in there and press enter. Fingers crossed, everything should be\ngreat. Like the way it is right now, it'll ask for my pseudo password and\nthat was it. Oh, LAMA is now installed. Now this will directly apply to\nLinux people and Windows people. See right here where it says Nvidia\nGPU installed. If you have that, you're going to have a better time\nthan other people who don't have that. I'll show you here in a second.\nIf you don't have it, that's fine. We'll keep going. Now let's run an\nLLM. We'll start with llama two. So we'll simply type in, oh Lama run, and then we'll pick one llama\ntwo and that's it. Ready, set go. It's going to pull the manifest. It'll then start pulling down\nand downloading Llama two. And I want you to just realize this,\nthat powerful LAMA two pre-training, we talked about all the money and\nhours spent. That's how big it is. This is the 7 billion\nparameter model or the seven B. It's pretty powerful and we're about to\nliterally have this in the palm of our hands in like 3, 2, 1. Oh,\nI thought I had it. Anyways, it's almost done. And boom, it's done. We've got a nice success message\nright here and it's ready for us. We can ask you anything.\nLet's try what is a pug? Now the reason this is going\nso fast, just like a side note, is that I'm running A GPU\nand AI models love GPUs. So lemme just show you real quick. I did install alama on a Linux\nvirtual machine and I'll just demo the performance for you real quick. By the\nway, if you're running a Mac with an M1, M two or M three processor, it actually\nworks great. I forgot to install it. I got to install it real quick and\nI'll ask you that same question. What is a pug? It's going to\ntake a minute, it'll still work, but it's going to be slower on CPUs and\nthere it goes. It didn't take too long, but notice it is a bit slower. Now if you're running WSL and you know\nhave an Nvidia GPU and it didn't show up, I'll show you in a minute how you can\nget those drivers installed. But anyways, just sit back for a minute, sip your coffee and think\nabout how powerful this is. The tinfoil hat version of me\nstinking loves this because let's say the zombie apocalypse happens, right?\nThe grid goes down, things are crazy, but as long as I have my\nlaptop and a solar panel, I still have AI and it can help\nme survive the zombie apocalypse. Let's actually see how that would\nwork. It gives me next steps. I could have it help me with the water\nfiltration system. This is just cool, right? It's amazing. But can\nI show you something funny? You may have caught this\nearlier. Who is network? Chuck? What? Dude, I've always\nwanted to be Rick Grimes. That is so fun, but seriously,\nit kind of hallucinated there. It didn't have the correct information. It's so funny how it mixed the\nzombie apocalypse prompt with me. I love that so much. Let's try\na different model. I'll say bye. I'll try a really fun one\ncalled mytral. And by the way, if you want to know which ones you\ncan run with Llama, which LLMs, they get a page for their models right\nhere and all the ones you can run, including llama two,\nuncensored Wizard Math. I might give that to my kids\nactually. Let's see what it says. Now who is Network Chuck? Now my name is not Chuck Davis and my\nYouTube channel is not called Network Chuck on Tech. So clearly the data this thing was trained\non is either not up to date or just plain wrong. So now the question is cool, we've got this local private ai,\nthis LLM, that's super powerful, but how do we teach it the\ncorrect information for us? How can I teach it to know\nthat I'm network Chuck,\nChuck Keith, not Chuck Davis, and my channel is called Network Chuck. Or maybe I'm a business and I want it\nto know more than just what's publicly available because sure, right\nnow if you downloaded this lm, you could probably use it in your job, but you can only go so far without it\nknowing more about your job. For example, maybe you're on a help desk. Imagine if you could take your help\ndesk's knowledge base, your IT procedures, your documentation. Not only that, but maybe you have a database\nof closed tickets, open tickets. If you could take all that data and\nfeed it to this LLM and then ask it questions about all of\nthat, that would be crazy. Or maybe you wanted to help troubleshoot\ncode that your company's written. You could even make this LM\npublic facing for your customers. You feed information about your product\nand the customer could interact with that chat bot you make. Maybe this is all possible with a process\ncalled fine tuning where we can train this AI on our own proprietary\nsecret private stuff about our company or maybe our lives or\nwhatever you want to use it for, whatever use case is, and this is fantastic because maybe before\nyou couldn't use a public LLM because you weren't allowed to share your\ncompany's data with that LLM, whether it's compliance reasons or you\njust simply didn't want to share that data because it's secret.\nWhatever the case, it's possible now because\nthis AI is private, it's local and whatever\ndata you feed to it, it's going to stay right there in a\ncompany. It's not leaving the door. That idea just makes me so excited\nbecause I think it is the future of AI and how companies and individuals\nwill approach it. It's\ngoing to be more private. Back to our question though,\nfine tuning, that sounds cool. Training and AI on your own\ndata, but how does that work? Because as we saw before with\npre-training a model with mata, it took them 6,000 GPUs\nover 1.7 million GPU hours. Do we have to have this massive\ndata center to make this happen? No. Check this out, and this is such a fun\nexample, VMware, they asked chat GPT, what's the latest version\nof VMware vSphere? Now the latest chat GPT\nknew about was vSphere 7.0, but that wasn't helpful to VMware because\ntheir latest version they were working on chat hadn't been released yet. So it wasn't public knowledge\nwas vSphere eight update too. And they wanted information like this\ninternal information not yet released to the public. They wanted this to be available to\ntheir internal team so they could ask something like chat GBT, Hey, what's\nthe latest version of vSphere? And they could answer correctly. So to do what VMware is trying to do\nto fine tune a model or train it on new data, it does require a lot. First of all, you would need some\nhardware servers with GPUs. Then you would also need a bunch of\ntools and libraries and SDKs like PyTorch and TensorFlow, pandas, MPI side\nkit, learn transformers and fast ai. The list goes on. You need lots of tools and resources\nin order to fine tune an LLM. That's why I'm a massive fan of\nwhat VMware is doing right here. They have something called the\nVMware private AI with Nvidia, the gajillion things I just listed\noff. They include in one package, one combo meal, a recipe of\nai, fine tuning goodness. So as a company it becomes a bit easier\nto do this stuff yourself locally. For the system engineer you have on\nstaff who knows VMware and loves it, they could do this stuff, they could implement this and the data\nscientists they have on staff that will actually do some of the fine tuning,\nall the tools are right there. So here's what it looks like to fine tune\nand we're going to kind of peek behind the curtain at what a data\nscientist actually does. So first we have the infrastructure\nand we start here in vSphere, VMware. Now if you don't know what vSphere\nis or VMware, think virtual machines, you got one big physical server. The\nhardware, the stuff you can feel, touch and smell. You haven't smelled\nthe server, I dunno what you're doing. And instead of installing one operating\nsystem on them like Windows or Linux, you install VMware's, EA XI, which will then allow you to virtualize\nor create a bunch of additional virtual computers. So instead of one computer, you've got a bunch of computers all\nusing the same hardware resources. And that's what we have right here.\nOne of those virtual computers, a virtual machine. This by the way is one of their special\ndeep learning VMs that has all the tools I mentioned and many, many more\npre-installed, ready to go. Everything a data scientist could love. It's kind of like a surgeon walking in\nto do some surgery and like their doctor assistants or whatever have\nprepared all their tools. It's all in the tray laid out\nnice and neat to the surgeon. All he has to do is walk\nin and just go scalpel. That's what we're doing\nhere for the data scientist. Now talking more about hardware, this guy has a couple Nvidia GPUs assigned\nto it or pass through to it through a technology called PCIE Passthrough.\nThese are some beefy GPUs. I notice they are V GPU for virtual GPU\nsimilar to what you do with the CPU, cutting up the PU and assigning some\nof that to a virtual CPU on a virtual machine. So here we are in data scientists\nworld. This is a Jupiter notebook, a common tool used by a data scientist, and what you're going to see here is a\nlot of code that they're using to prepare the data, specifically the data that they're\ngoing to train or fine tune the existing model on. Now we're not\ngoing to dive deep on that, but I do want you to see\nthis, check this out. A lot of this code is all about getting\nthe data ready. So in VMware's case, it might be a bunch of the knowledge\nbase product documentation and they're getting it ready to be fed to the LLM.\nAnd here's what I wanted you to see. Here's the dataset that we're training\nthis model on. We're fine tuning. We only have 9,800 examples that we're\ngiving it or 9,800 new prompts or pieces of data. And that\ndata might look like this, like a simple question or a prompt and\nthen we feed it the correct answer and that's how we essentially\ntrain ai. But again, we're only giving it 9,800 examples, which is not a lot at all and is\nextremely small compared to how the model was originally trained. And I point that out to say that we're\nnot going to need a ton of hardware or a ton of resources to fine tune this model. We won't need the 6,000 GPUs we needed\nfor MATA to originally create this model. We're just adding to it, changing some things or fine tuning it\nto what our use case is and looking at what actually will be changed\nwhen we run this and we train it, we're only changing 65 million parameters,\nwhich sounds like a lot, right? But not in the grand scheme of things\nof like a 7 billion parameter model. We're only changing 0.93% of the model. And then we can actually\nrun our fine tuning, which this is a specific technique in\nfine tuning called prompt tuning where we simply feed up additional prompts with\nanswers to change how it'll react to people asking you questions. This process will take three to four\nminutes to fine tune it because again, we're not changing a lot and that is\njust so super powerful and I think VMware is leading the charge with private ai. VMware and Nvidia take all the guesswork\nout of getting things set up to fine tune an LLM. They've\ngot deep learning VMs, which are insane VMs that\ncome pre-installed with\neverything you could want everything a data scientist\nwould need to find tune an LLM. Then Nvidia has an entire suite\nof tools sensor around their GPUs, taking advantage of some really exciting\nthings to help you fine tune your lms. Now there's one thing I didn't talk about\nbecause I wanted to save it for last. For right now it's this right\nhere, this vector database, post gray SQL box here. This is something called rag and it's\nwhat we're about to do with our own personal GPT here in a bit. Retrieval,\naugment the generation. So scenario, let's say you have a database of\nproduct information, internal docs, whatever it is, and you haven't fine\ntuned your LLM on this just yet. So it doesn't know about it. You\ndon't have to do that with rag. You can connect your LLM to\nthis database of information, this knowledge base and\ngive it these instructions. Say whenever I ask you a question about\nany of the things in this database, before you answer, consult the database, go look at it and make sure\nwhat you're saying is accurate. We're not retraining the LLM, we're\njust saying, Hey, before you answer, go check real quick in this database to\nmake sure it's accurate to make sure you got your stuff right.\nIsn't that cool? So yes, fine tuning is cool and training\nan LLM on your own data is awesome, but in between those\nmoments of fine tuning, you can have rag set up where\nit can consult your database, your internal documentation and give\ncorrect answers based on what you have in that database. That is so stinking cool. So with VMware private AI\nfoundation with nvidia, they have those tools baked right in\nto where it just kind of works for what would otherwise be a very complex setup.\nAnd by the way, this whole rag thing, like I said earlier,\nwe're about to do this, I actually connected a lot of my notes\nand journal entries to a private GPT using RAG and I was able to talk\nwith it about me asking it about my journal entries and answering questions\nabout my past. That's so powerful. Now, before we move on, I just want to highlight the fact that\nNvidia with their Nvidia AI enterprise gives you some amazing and fantastic\ntools to pull the LLM of your choice and then fine tune and customize and deploy\nthat LLM. It's all built in right here. So VMware Cloud Foundation, they provide the robust infrastructure\nand NVIDIA provides all the amazing AI tools you need to develop\nand deploy these custom LLMs. Now it's not just Nvidia, they're\npartnering with Intel as well. So VMware is covering all the\ntools that admins care about. And then for the data\nscientists, this is for you. Intel's got your back data analytics, generative AI and deep learning tools\nand some classic ML or machine learning. And they're also working with IBM, all\nyou IBM fans. You can do this too. Again, VMware has the admin's back. But\nfor the data scientist, Watson, one of the first AI things I ever\nheard about Red Hat and OpenShift, and I love this because what VMware\nis doing is all about choice. If you want to run your own\nlocal private ai, you can. You're not just stuck with one of the\nbig guys out there and you can choose to run it with Nvidia and VMware,\nIntel and VMware, IBM and VMware. You got options. So there's\nnothing stopping you. It's not for some of the bonus section\nof this video and that's how to run your own private GPT with your own\nknowledge base. Now, fair warning, it is a bit more advanced,\nbut if you stick with me, you should be able to get this up and\nrunning. So take one more sip of coffee. Let's get this going. Now, first of\nall, this will not be using a lama. This will be a separate project\ncalled Private GPT. Now disclaimer, this is kind of hard to do.\nUnlike VMware private ai, which they do it all for you, it's a complete solution for companies\nto run their own private local ai. What I'm about to show you is not that\nat all. No affiliation with VMware. It's a free side project. You can try just to get a little taste\nof what running your own private GPT with rag tastes like. Did I do\nthat right? I don't know. Now L Martinez has a great doc on\nhow to install this. It's a lot, but you can do it. And if\nyou just want a quick start, he does have a few lines of code for\nLinux and Mac users. Fair warning, this is CPU only. You can't really\ntake advantage of RAG without A GPU, which is what I wanted to do. So\nhere's my very specific scenario. I've got a Windows PC with an\nNVIDIA 40 90. How do I run this? Linux-based project. WSL, and I'm so\nthankful to this guy Emelia Lance a lot. He put an entire guide\ntogether of how to set this up. I'm not going to walk you through every\nstep because he already did that link below, but I seriously need to buy\nthis guy a coffee. How do I do that? I don't know, Emil, if you're\nwatching this, reach out to me. I'll send you some coffee. So anyways, I went through every step from installing\nall the prereqs to installing NVIDIA drivers and using poetry to handle\ndependencies, which poetry is pretty cool. I landed here. I've got a private local working private\nGPT that I can access through my web browser and it's using my GPU,\nwhich is pretty cool. Now, first I try a simple document upload, got this VMware article that details\na lot of what we talked about in this video. I upload it and I start asking\nyou questions about this article. I tried something specific like show me\nsomething about VMware AI market growth. Bam, it figured it out,\nit told me. Then I'm like, what's the coolest thing\nabout VMware private ai? It told me I'm sitting here chatting\nwith a document, but then I'm like, let's try something bigger. I\nwant to chat with my journals. I've got a ton of journals on markdown\nformat and I want to ask you questions about me. Now this specific step\nis not covered in the article. So here's how you do it. First, you'll want to grab your\nfolder of whatever documents\nyou want to ask questions about and throw it onto your machine. So I copied over to my WSL machine and\nthen I ingested it with this command once complete and I ran private GPT. Again, here's all my documents and\nI'm ready to ask it questions. So let's test this out. I'm going\nto ask it what did I do in takayama? So I went to Japan in November of 2023.\nLet's see if you can search my notes, figure out when that was and what I did. That's awesome. Oh my goodness. Let's see, what did I eat in Tokyo? How cool is that? Oh my gosh,\nthat's so fun. No, it's not perfect, but I can see the potential here.\nThat's insane. I love this so much. Private AI is the future and that's why\nwe're seeing VMware bring products like this to companies to run their own\nprivate local AI and then make it pretty easy. If you actually did that private\nGPT thing, that little side project, there's a lot to it. Lots of tools you\nhave to install, it's kind of a pain. But with VMware, they kind of cover everything like that\ndeep learning VM they offer as part of their solution. It's got all the\ntools ready to go. Pre-baked again, you're like a surgeon just\nwalking in saying scalpel. You got all this stuff right there. So\nif you want to bring AI to your company, check out VMware private AI link below\nand thank you to VMware by Broadcom for sponsoring this video. You made it to\nthe end of the video time for a quiz. This quiz will test the knowledge you've\ngained in this video and the first five people to get a hundred percent on this\nquiz will get free coffee from Network Chuck Coffee. So here's how\nyou take the quiz right now. Check the description in your\nvideo and click on this link. If you're not currently signed into the\nacademy, go ahead and get signed in. If you're not a member, go ahead\nand click on sign off. It's free. Once you're signed in, it will take you to your dashboard showing\nyou all the stuff you have access to with your free academy account.\nBut to get right back to that quiz, go back to the YouTube video, click on that link once more and\nit should take you right to it. Go ahead and click on start now and\nstart your quiz. Here's a little preview. That's it. The first five to get\na hundred percent free coffee. If you're one of the five, you'll know because you'll\nreceive an email with free coffee. You got to be quick, you got to be smart.\nI'll see you guys in the next video."
    }
}