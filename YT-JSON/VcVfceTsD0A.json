{
    "VcVfceTsD0A": {
        "title": "Max Tegmark: The Case for Halting AI Development | Lex Fridman Podcast #371",
        "thumbnail": "https://i.ytimg.com/vi/VcVfceTsD0A/hqdefault.jpg",
        "author": "Lex Fridman",
        "date_of_release": "2023-04-13T16:25:45Z",
        "duration": "PT2H48M13S",
        "view_count": "1525011",
        "like_count": "26816",
        "comment_count": "4532",
        "description": "Max Tegmark is a physicist and AI researcher at MIT, co-founder of the Future of Life Institute, and author of Life 3.0: Being Human in the Age of Artificial Intelligence. Please support this podcast by checking out our sponsors:\n- Notion: https://notion.com\n- InsideTracker: https://insidetracker.com/lex to get 20% off\n- Indeed: https://indeed.com/lex to get $75 credit\n\nEPISODE LINKS:\nMax's Twitter: https://twitter.com/tegmark\nMax's Website: https://space.mit.edu/home/tegmark\nPause Giant AI Experiments (open letter): https://futureoflife.org/open-letter/pause-giant-ai-experiments\nFuture of Life Institute: https://futureoflife.org\nBooks and resources mentioned:\n1. Life 3.0 (book): https://amzn.to/3UB9rXB\n2. Meditations on Moloch (essay): https://slatestarcodex.com/2014/07/30/meditations-on-moloch\n3. Nuclear winter paper: https://nature.com/articles/s43016-022-00573-0\n\nPODCAST INFO:\nPodcast website: https://lexfridman.com/podcast\nApple Podcasts: https://apple.co/2lwqZIr\nSpotify: https://spoti.fi/2nEwCF8\nRSS: https://lexfridman.com/feed/podcast/\nFull episodes playlist: https://www.youtube.com/playlist?list=PLrAXtmErZgOdP_8GztsuKi9nrraNbKKp4\nClips playlist: https://www.youtube.com/playlist?list=PLrAXtmErZgOeciFP3CBCIEElOJeitOr41\n\nOUTLINE:\n0:00 - Introduction\n1:56 - Intelligent alien civilizations\n14:20 - Life 3.0 and superintelligent AI\n25:47 - Open letter to pause Giant AI Experiments\n50:54 - Maintaining control\n1:19:44 - Regulation\n1:30:34 - Job automation\n1:39:48 - Elon Musk\n2:01:31 - Open source\n2:08:01 - How AI may kill all humans\n2:18:32 - Consciousness\n2:27:54 - Nuclear winter\n2:38:21 - Questions for AGI\n\nSOCIAL:\n- Twitter: https://twitter.com/lexfridman\n- LinkedIn: https://www.linkedin.com/in/lexfridman\n- Facebook: https://www.facebook.com/lexfridman\n- Instagram: https://www.instagram.com/lexfridman\n- Medium: https://medium.com/@lexfridman\n- Reddit: https://reddit.com/r/lexfridman\n- Support on Patreon: https://www.patreon.com/lexfridman",
        "transcript": "- A lot of people have said for many years that there will come a time when we want to pause a little bit. That time is now. - The following is a\nconversation with Max Tegmark, his third time in the podcast. In fact, his first appearance was episode number one of this very podcast. He is a physicist and artificial intelligence\nresearcher at MIT, co-founder of Future of Life Institute, and Author of \"Life 3.0: Being Human in the Age of\nArtificial Intelligence.\" Most recently, he's a key figure in\nspearheading the open letter calling for a six-month\npause on giant AI experiments like training GPT-4. The letter reads, \"We're calling for a pause on training of models larger than\nGPT-4 for six months. This does not imply a pause\nor ban on all AI research and development or the use of systems that have already been\nplaced in the market. Our call is specific and addresses a very small pool of actors who possesses this capability.\" The letter has been signed\nby over 50,000 individuals, including 1800 CEOs and\nover 1500 professors. Signatories include Yoshua Bengio, Stuart Russell, Elon Musk, Steve Wozniak, Yuval Noah Harari, Andrew Yang, and many others. This is a defining moment in the history of human civilization, where the balance of power between human and AI begins to shift, and Max's mind and his voice is one of the most valuable and powerful in a time like this. His support, his wisdom, his friendship, has been a gift I'm forever\ndeeply grateful for. This is the Lex Fridman podcast. To support it, please\ncheck out our sponsors in the description. And now, dear friends, here's Max Tegmark. You were the first ever\nguest on this podcast, episode number one. So first of all, Max, I just have to say, thank you for giving me a chance. Thank you for starting this journey, and it's been an incredible journey, just thank you for sitting down with me and just acting like I'm\nsomebody who matters, that I'm somebody who's\ninteresting to talk to. And thank you for doing it. That meant a lot. - And thanks to you for putting your heart and soul into this. I know when you delve\ninto controversial topics, it's inevitable to get hit by what Hamlet talks about \"The slings and arrows,\" and stuff. And I really admire this. It's in an era, you know, where YouTube videos are too long, and now it has to be\nlike a 20-minute TikTok, 20-second TikTok clip. It's just so refreshing to see you going exactly against all of the advice and doing these really long form things, and the people appreciate it, you know. Reality is nuanced, and thanks\nfor sharing it that way. - So let me ask you again, the first question I've\never asked on this podcast, episode number one, talking to you. Do you think there's intelligent life out there in the universe? Let's revisit that question. Do you have any updates? What's your view when you\nlook out to the stars? - So, when we look out to the stars, if you define our universe the\nway most astrophysicists do, not as all of space, but the spherical region of space that we can see with our telescopes from which light has the time to reach us, since our Big Bang. I'm in the minority. I estimate that we are the only life in this spherical volume that has invented internet, the radio, has gotten\nto our level of tech. And if that's true, then it puts a lot of responsibility on us to not mess this one up. Because if it's true, it means that life is quite rare. And we are stewards of this one spark of advanced consciousness, which if we nurture it and help it grow, eventually life can spread from here, out into much of our universe, and we can have this just amazing future. Whereas, if we instead are reckless with the technology we build and just snuff it out due to stupidity or in-fighting, then, maybe the rest of cosmic\nhistory in our universe is just gonna be playing\nfor empty benches. But I do think that we are actually very likely to get visited by aliens, alien intelligence quite soon. But I think we are gonna be building that alien intelligence. - So we're going to give birth to an intelligent alien civilization, unlike anything that human, the evolution here on\nearth was able to create in terms of the path, the biological path it took. - Yeah, and it's gonna be\nmuch more alien than a cat, or even the most exotic animal\non the planet right now, because it will not have been created through the usual Darwinian competition where it necessarily cares\nabout self-preservation, that is afraid of death, any of those things. The space of alien\nminds that you can build is just so much faster than\nwhat evolution will give you. And with that also comes\na great responsibility, for us to make sure that\nthe kind of minds we create are the kind of minds\nthat it's good to create. Minds that will share our values and be good for humanity and life. And also don't create\nminds that don't suffer. - Do you try to visualize the full space of alien minds that AI could be? Do you try to consider all the different kinds of intelligences, instead of generalizing\nwhat humans are able to do to the full spectrum of\nwhat intelligent creatures, entities could do? - I try, but I would say I fail, I mean, it's very difficult for human mind to really grapple with\nsomething so completely alien. Even for us, right? If we just try to\nimagine how would it feel if we were completely indifferent towards death or individuality? Even if you just imagine that for example, you could just copy my knowledge\nof how to speak Swedish, (fingers snapping) boom,\nnow you can speak Swedish, and you could copy any\nof my cool experiences, and then you could delete the ones you didn't like in your own life, just like that. it would already change quite a lot about how you feel as\na human being, right? You probably spend less\neffort studying things if you just copy them, and you might be less afraid of death, because if the plane\nyou're on starts to crash, you'd just be like, \"Oh shucks, I haven't backed my\nbrain up for four hours, (Lex laughs) so I'm gonna lose this, all this wonderful\nexperiences of this flight.\" We might also start feeling more, like compassionate maybe with other people if we can so readily share\neach other's experiences and our knowledge, and\nfeel more like a hivemind. It's very hard though. I really feel very humble about this to grapple with it, how it might actually feel. The one thing which is so obvious though, which, I think is just\nreally worth reflecting on, is because the mind space\nof possible intelligences is so different from ours, it's very dangerous if we assume they're gonna be like us, or anything like us. - Well there's, the entirety of human written history has been through poetry, through novels, been trying to describe\nthrough philosophy, trying to describe the human condition and what's entailed in it. Like, just like you said, fear of death and all\nthose kinds of things, what is love, and all of that changes. - [Max] Yeah. - If you have a different\nkind of intelligence. Like all of it, the entirety of all those poems, they're trying to sneak up to what the hell it means to be human. All of that changes. How AI concerns and existential crises\nthat AI experiences, how that clashes with the\nhuman existential crisis, the human condition. - [Max] Yeah. - That's hard to fathom, hard to predict. - It's hard, but it's\nfascinating to think about also. Even in the best case scenario, where we don't lose control over the ever more powerful AI that we're building to other humans whose goals we think are horrible, and where we don't lose\ncontrol to the machines, and AI provides the things we want. Even then, you get into the questions you touched here, you know, maybe it's the struggle that it's actually hard to do things is part of the things that\ngives us meaning as well, right? So for example, I found it so shocking that this new Microsoft GPT-4 commercial that they put together, has this woman talking about, showing this demo how she's gonna give a graduation speech to\nher beloved daughter. And she asks GPT-4 to write it. It was frigging 200 words or so. If I realized that my\nparents couldn't be bothered struggling a little\nbit to write 200 words, and outsource that to their computer, I would feel really offended, actually. And so I wonder if eliminating too much of the struggle from our existence, do you think that would also take away a little bit of what- - it means to be human? Yeah. - [Max] Yeah. - We can't even predict. I had somebody mentioned\nto me that they use, they started using ChatGPT\nwith the 3.5 and now 4.0, to write what they\nreally feel to a person, and they have a temper issue, and they're basically\ntrying to get ChatGPT to rewrite it in a nicer way. To get the point across, but rewrite it in a nicer way. So we're even removing the inner asshole from our communication. So I don't, you know, there's some positive aspects of that, but mostly it's just the transformation of how humans communicate. And it's scary because so much of our society is based on this glue of communication. And if we're now using AI as\nthe medium of communication that does the language for us, so much of the emotion that's\nladen in human communication, and so much of the intent, that's going to be handled by, outsourced to AI, how does that change everything? How does that change the internal state of how we feel about other human beings? What makes us lonely? What makes us excited? What makes us afraid? How we fall in love? All that kind of stuff. - Yeah. For me personally, I have to confess, the challenge is one of the things that really makes my life feel meaningful, you know? If I go hiking mountain\nwith my wife, Meia, I don't wanna just press a\nbutton and be at the top, I want to struggle and\ncome up there sweaty, and feel, \"Wow, we did this,\" in the same way. I want to constantly work on myself to become a better person. If I say something in anger that I regret, I want to go back and really work on myself rather than just tell an AI, from now on, always filter what I write so I don't have to work on myself, 'cause then I'm not growing. - Yeah, but then again, it could be like with chess, and AI, once it significantly, obviously supersedes the\nperformance of humans, it will live in its own world, and provide maybe a flourishing\ncivilizations for humans. But we humans will\ncontinue hiking mountains, and playing our games, even though AI is so much smarter, so much stronger, so much superior in every single way, just like with chess. - [Max] Yeah. - So that, I mean, that's one possible\nhopeful trajectory here, is that humans will continue to human, and AI will just be a kind of, a medium that enables the\nhuman experience to flourish. - Yeah, I would phrase that\nas rebranding ourselves from Homo sapiens to Homo sentiens. You know, right now, if it's sapiens, the ability to be intelligent, we've even put it in our species' name. So we're branding\nourselves as the smartest information processing\nentity on the planet. That's clearly gonna change\nif AI continues ahead. So maybe we should focus\non the experience instead, the subjective experience that we have, Homo sentiens, and say that's\nwhat's really valuable, the love, the connection,\nthe other things, and get off our high horses, and get rid of this hubris that, you know, only we can do integrals. - So consciousness, the subjective experience\nis a fundamental value to what it means to be human. Make that the priority. - That feels like a\nhopeful direction to me. But that also requires more compassion, not just towards other humans, because they happen to be\nthe smartest on the planet, but also towards all our\nother fellow creatures on this planet. I personally feel right now, we're treating a lot of farm\nanimals horribly, for example. And the excuse we're using is, \"Oh, they're not as smart as us.\" But if we admit that we're not that smart in the grand scheme of things either, in the post-AI epoch, you know, then surely, we should value the subjective experience of a cow also. - Well, allow me to\nbriefly look at the book, which at this point is becoming\nmore and more visionary that you've written, I\nguess over five years ago, \"Life 3.0.\" So first of all, 3.0, what's 1.0, what's 2.0, What's 3.0? and how's that vision sort of evolve, the vision in the book evolve to today. - Life 1.0 is really dumb like bacteria, and that it can't actually\nlearn anything at all during their lifetime. The learning just comes\nfrom this genetic process from one generation to the next. Life 2.0 is us and other\nanimals which have brains which can learn during\ntheir lifetime a great deal. Right so, and you know, you were born without being able to speak English, and at some point you decided, \"Hey, I wanna upgrade my software, and so let's install an\nEnglish-speaking module.\" So you did. And Life 3.0, which does not exist yet, cannot replace not only its\nsoftware the way we can, but also it's hardware. And that's where we're\nheading towards at high speed. We're already maybe 2.1 because we can, you know, put in an artificial knee, pacemaker, et cetera, et cetera. And if Neuralink and\nother companies succeed, it will be life 2.2, et cetera. But the companies trying to build AGI, or trying to make is of course, full 3.0, and you can put that intelligence in something that also has no, biological basis whatsoever. - So less constraints\nand more capabilities, just like the leap from 1.0 to 2.0. There is nevertheless, you speaking so harshly about bacteria, so disrespectfully about bacteria, there is still the same\nkind of magic there that permeates life 2.0 and 3.0. It seems like maybe the\nthing that's truly powerful about life, intelligence,\nand consciousness, was already there in 1.0. Is it possible? - I think we should be\nhumble and not be so quick to make everything binary and say either it's there or it's not. Clearly there's a great spectrum and there is even controversy about whether some unicellular\norganisms like amoebas can maybe learn a little\nbit, you know, after all. So apologies if I offended\nany bacteria here. (laughs) It wasn't my intent. It was more that I wanted to talk up how cool it is to actually have a brain. - [Lex] Yeah. - Where you can learn\ndramatically within your lifetime. - [Lex] Typical human. - And the higher up you\nget from 1.0 2.0 to 3.0, the more you become the\ncaptain of your own ship, the master of your own destiny. And the less you become a slave to whatever evolution gave you, right? By upgrading your software, we can be so different\nfrom previous generations and even from our parents, much more so than even a bacterium, you know, no offense to them. And if you can also swap out your hardware and take any physical\nform you want, of course, it's really, the sky's the limit. - Yeah, so the, it accelerates the rate\nat which you can perform the computation that\ndetermines your destiny. - Yeah, and I think it's worth commenting a bit on what \"you\" means in this context. Also, if you swap things out a lot, right? This is controversial, but my, current understanding is that, you know, life is best thought of not as a bag of meat, or even a bag of elementary particles, but rather as a system which\ncan process information and retain its own complexity, even though nature is\nalways trying to mess it up, so, it's all about information processing. And that makes it a lot like something like a wave in the ocean, which is not, it's water molecules, right? The water molecules bob up and down, but the wave moves forward, it's an information pattern\nin the same way you, Lex, you're not the same atoms as during the first, - Time we talked, yeah.\n- Interview you did with me, you've swapped out most of them, but it's still you. And the information\npattern is still there, and if you could swap out your arms, and like whatever, you can still have this\nkind of continuity, it becomes much more sophisticated sort of way forward in time where the information lives on. I lost both of my parents\nsince our last podcast, and it actually gives me a lot of solace that this way of thinking about them, they haven't entirely died because a lot of mommy and daddy's, sorry, I'm getting a\nlittle emotional here, but a lot of their values, and ideas, and even jokes and so on, they haven't gone away, right? Some of them live on, I can carry on some of them, and they also live on a\nin a lot of other people. So in this sense, even with life 2.0, we can to some extent, already transcend our\nphysical bodies and our death. And particularly if you can\nshare your own information, your own ideas with many others like you do in your podcast, then you know, that's the closest immortality we can get with our bio bodies. - You carry a little bit of\nthem in you in some sense. - [Max] Yeah, yeah. - Do you miss them? Do you miss your mom and dad? - Of course, of course. - What did you learn about life from them? If we can take a bit of a tangent. - Oh, so many things. For starters, my fascination for math and the physical\nmysteries of our universe, I got a lot of that from my dad. But I think my obsession\nfor really big questions, and consciousness, and so on, that actually came mostly from my mom and what I got from both of them, which is very core part\nof really who I am, I think is this, just feeling comfortable with, not buying into what\neverybody else is saying, just doing what I think is right. They both very much just, you know, did their own thing, and sometimes they got flak for it and they did it anyway. - That's why you've always\nbeen in an inspiration to me. That you're at the top of your field and you're still willing to tackle the big\nquestions in your own way. You're one of one of the people that represents MIT best to me, you've always been an inspiration in that. So it's good to hear that you got that from your mom and dad. - Yeah, you're too kind. But yeah, I mean, the good reason to do science is because you're really curious, and you wanna figure out the truth. If you think, this is how it is and everyone else says, \"No, no, that's bullshit,\nand it's that way,\" you know, You stick with what you think is true, and even if everybody else\nkeeps thinking it's bullshit, there's a certain, I always root for the underdog, (Lex laughs)\nwhen I watch movies. And my dad once, one time for example, when I wrote one of my\ncraziest papers ever, talking about our universe\nultimately being mathematical, which we're not gonna get into today, I got this email from a quite\nfamous professor saying, \"This is not only all bullshit, but it's gonna ruin your career. You should stop doing this kind of stuff.\" I sent it to my dad. Do you know what he said? - [Lex] (laughs) What he say? - He replied with a quote from Dante. (Lex laughing) (Max speaking in Italian) \"Follow your own path\nand let the people talk.\" (Both laughing) Go dad! - [Lex] Yeah. - This is the kind of thing, you know, he's dead, but that attitude is not. - How did losing them as a man, as a human being change you? How did it expand your\nthinking about the world? How did it expand your thinking about, you know, this thing we're talking about, which is humans creating another living, sentient perhaps, being? - I think it, mainly do two things. One of them just going\nthrough all their stuff after they had passed away and so on, just drove home to me how\nimportant it is to ask ourselves, why are we doing this things we do? Because it's inevitable\nthat you look at some things they spent an enormous time on and you ask in hindsight, would they really have\nspent so much time on this? Would they have done something that was more meaningful? So I've been looking more\nin my life now and asking, you know, why am I doing what I'm doing? And I feel, it should either be something\nI really enjoy doing, or it should be something that I find really, really meaningful\nbecause it helps humanity, and if it's in none of\nthose two categories, maybe I should spend less\ntime on it, you know. The other thing is, dealing with death up in person like this, it's actually made me less afraid of, even less afraid of\nother people telling me that I'm an idiot, you know, which happens regularly, and just live my life,\ndo my thing, you know? And it's made it a\nlittle bit easier for me to focus on what I feel\nis really important. - What about fear of your own death? Has it made it more real that this is something that happens? - Yeah, it's made it extremely real, and you know, I'm next in line in our family now, right? It's me and my younger brother, but, they both handled it with such dignity, that was a true inspiration also. They never complained about things, and you know, when you're old and your body starts falling apart, it's more and more to complain about, they looked at what could they\nstill do that was meaningful, and they focused on that rather than wasting time talking about, or even thinking much about things they were disappointed in. I think anyone can make\nthemselves depressed if they start their morning by\nmaking a list of grievances. Whereas if you start your day\nwhen the little meditation and just the things you're grateful for, you basically choose to be a happy person. - Because you only have\na finite number of days, we should spend them, - [Max] Make it count. - Being grateful. - [Max] Yeah. - Well you do happen to\nbe working on a thing which seems to have potentially, some of the greatest impact\non human civilization of anything humans have ever created, which is artificial intelligence. This is, on the both\ndetailed technical level, and on the high philosophical\nlevel you work on. So you've mentioned to me that there's an open letter\nthat you're working on. - It's actually going live in a few hours. (Lex laughing) So I've been having late\nnights and early mornings. It's been very exciting, actually. In short, have you seen, \"Don't Look Up,\" the film? - Yes, yes. - I don't want to be the movie spoiler for anyone watching\nthis who hasn't seen it. But if you're watching this, you haven't seen it, watch it, because we\nare actually acting out, it's life imitating art. Humanity is doing exactly that right now, except it's an asteroid that\nwe are building ourselves. Almost nobody is talking about it. People are squabbling across the planet about all sorts of things, which seem very minor\ncompared to the asteroid that's about to hit us, right? Most politicians don't\neven this on the radar, they think maybe in 100 years or whatever. Right now we're at a fork on the road. This is the most important\nfork that humanity has reached in it's over 100,000 years on this planet. We're building effectively a new species that's smarter than us, it doesn't look so much like a species yet 'cause it's mostly not embodied in robots. But that's the technicality\nwhich will soon be changed. And this arrival of of\nartificial general intelligence that can do all our jobs as well as us, and probably shortly\nthereafter, superintelligence, which greatly exceeds\nour cognitive abilities. It's gonna either be the best thing ever to happen to humanity or the worst. I'm really quite confident that there is not that\nmuch middle ground there. - But it would be\nfundamentally transformative to human civilization. - Of course, utterly and totally. Again, we'd branded\nourselves as Homo sapiens 'cause it seemed like the basic thing, we're the king of the\ncastle on this planet, we're the smart ones, we can control everything else, this could very easily change. We're certainly not gonna be the smartest on the planet for very long if AI, unless AI progress just halts, and we can talk more about\nwhy I think that's true 'cause it's controversial. And then we can also talk about reasons we might think it's\ngonna be the best thing ever, and the reason we think it's going to be the end of humanity, which is of course, super controversial. But what I think we can, anyone who's working on advanced AI can agree on is, it's much like the film \"Don't Look Up,\" in that it's just really comical how little serious public\ndebate there is about it, given how huge it is. - So what we're talking\nabout is a development, of currently, things like GPT-4, and the signs it's showing\nof rapid improvement that may, in the near\nterm lead to development of superintelligent AGI, AI, general AI systems, and what kind of impact\nthat has on society. - [Max] Exactly. - When that thing achieves\ngeneral human-level intelligence, and then beyond that, general superhuman level intelligence. There's a lot of\nquestions to explore here. So one, you mentioned halt. Is that the content of the letter? is to suggest that maybe we should pause the development of these systems. - Exactly, so this is very controversial, from when we talked the first time, we talked about how I was involved in starting the Future of Life Institute, and we worked very hard on 2014, 2015, was the mainstream AI safety. The idea that there even could be risks and that you could do things about them. Before then, a lot of people thought it was just really kooky\nto even talk about it. And a lot of AI researchers felt, worried that this was too flaky, and could be bad for funding, and that the people had\ntalked about it were just not, didn't understand AI. I'm very, very happy with how that's gone, and that now, you know, it's completely mainstream, you go in any AI conference, and people talk about AI safety, and it's a nerdy technical\nfield full of equations and blah-blah. - [Lex] Yes. - As it should be, but there is this other thing, which has been quite taboo up until now, calling for slowdown. So what, we've constantly been\nsaying, including myself, I've been biting my tongue a lot, you know, is that, we don't need to slow down AI development. We just need to win this race, the wisdom race between\nthe growing power of the AI and the growing wisdom\nwith which we manage it. And rather than trying to slow down AI, let's just try to accelerate the wisdom, do all this technical work to figure out how you can actually ensure\nthat your powerful AI is gonna do what you want it to do. And have society adapt also with incentives and regulations so that these things get put to good use. Sadly, that didn't pan out. The progress on technical AI capabilities has gone a lot faster\nthan many people thought back when we started this in 2014. Turned out to be easier to build really advanced AI than we thought. And on the other side, it's gone much slower than we hoped with getting policymakers and others to actually put incentives\nin place to make, steer this in the good directions, maybe we should unpack it and talk a little bit about each, so. - [Lex] Yeah. - Why did it go faster than\na lot of people thought? In hindsight, it's exactly\nlike building flying machines. People spent a lot of time wondering about how do birds fly, you know. And that turned out to be really hard. Have you seen the TED\nTalk with a flying bird? - Like a flying robotic bird? - Yeah, it flies around the audience, but it took 100 years longer to figure out how to do that than for the Wright brothers\nto build the first airplane because it turned out there\nwas a much easier way to fly. And evolution picked\na more complicated one because it had its hands tied. It could only build a machine\nthat could assemble itself, which the Wright brothers\ndidn't care about that, they could only build a machine that use only the most common atoms\nin the periodic table, Wright Brothers didn't care about that, they could use steel, iron atoms, and it had to be built to repair itself, and it also had to be\nincredibly fuel efficient, you know, a lot of birds use less than half the fuel of a remote-controlled plane\nflying the same distance, For humans, just throw\na little more money, put a little more fuel in it, and there you go, 100 years earlier. That's exactly what's happening now with these large language models. The brain is incredibly complicated. Many people made the mistake, you're thinking we have to\nfigure out how the brain does human-level AI first before we could build in the machine, that was completely wrong. You can take an incredibly simple computational system called\na transformer network and just train it to do\nsomething incredibly dumb. Just read a gigantic amount of text and try to predict the next word. And it turns out, if you just throw a ton of compute at that and a ton of data, it gets to be frighteningly\ngood like GPT-4, which I've been playing with so much since it came out, right? And there's still some debate about whether that can get you all the way to full human level or not, but yeah, we can come back\nto the details of that and how you might get the human-level AI even if large language models don't. - Can you briefly, if it's just a small tangent, comment on your feelings about GPT-4? So just that you're impressed\nby this rate of progress, but where is it? Can GPT-4 reason? What are like the intuitions? What are human interpretable\nwords you can assign to the capabilities of GPT-4 that makes you so damn impressed with it? - I'm both very excited\nabout it and terrified. It's interesting mixture\nof emotions. (laughs) - All the best things in life\ninclude those two somehow. - Yeah, it can absolutely reason, anyone who hasn't played with it, I highly recommend doing\nthat before dissing it. It can do quite remarkable reasoning. I've had to do a lot of things, which I realized I couldn't\ndo that myself that well even, and it obviously does it dramatically faster than we do too, when you watch it type, and it's doing that well, servicing a massive number of\nother humans at the same time. The same time, it cannot reason as well as a human can on some tasks, it's obviously the limitations\nfrom its architecture. You know, we have in our heads, what in geek-speak is called\na recurrent neural network. There are loops, information can go from this neuron, to this neuron, to this neuron, and then back to this one, you can like ruminate on\nsomething for a while, you can self-reflect a lot. These large language models, they cannot, like GPT-4. It's a so-called transformer where it's just like a one-way street of information, basically. In geek-speak, it's called a\nfeed-forward neural network. And it's only so deep, so it can only do logic\nthat's that many steps and that deep, and it's not, so you can create problems\nwhich it will fail to solve, you know, for that reason. But the fact that it\ncan do so amazing things with this incredibly simple\narchitecture already, is quite stunning, and what we see in my lab at MIT when we look inside large language models to try to figure out how they're doing it, which, that's the key core\nfocus of our research, it's called mechanistic\ninterpretability in geek-speak. You know, you have this machine\nthat does something smart, you try to reverse engineer it, and see how does it do it. I think of it also as\nartificial neuroscience, (Lex laughs)\n'Cause that's exactly - I love it.\n- what neuroscientists do with actual brains. But here you have the\nadvantage that you can, you don't have to worry\nabout measurement errors. You can see what every\nneuron is doing all the time, and a recurrent thing\nwe see again and again, there's been a number of beautiful papers quite recently by a lot of researchers, and some of 'em are\nhere even in this area, is where when they figure\nout how something is done, you can say, \"Oh man, that's\nsuch a dumb way of doing it.\" And you read immediately\nsee how it can be improved. Like for example, there was this beautiful paper recently where they figured out how a large language model\nstores certain facts, like Eiffel Tower is in Paris, and they figured out\nexactly how it's stored and the proof of that they understood it was they could edit it. They changed some synapses in it, and then they asked it,\nWhere's the Eiffel Tower?\" And it said, \"It's in Rome.\" And then they asked,\n\"How do you get there? Oh, how do you get there from Germany?\" \"Oh, you take this train, the Roma Termini train station, and this and that,\" \"And what might you see\nif you're in front of it?\" \"Oh, you might see the Colosseum.\" So they had edited, - So they literally moved it to Rome. - But the way that it's\nstoring this information, it's incredibly dumb, if any fellow nerds listening to this, there was a big matrix, and roughly speaking, there are certain row and column vectors which encode these things, and they correspond very hand-wavingly to principle components and it would be much more\nefficient for as far as matrix, just store in the database, you know and, and everything so far, we've figured out how these things do are ways where you can see\nit can easily be improved. And the fact that this\nparticular architecture has some roadblocks built into it is in no way gonna\nprevent crafty researchers from quickly finding workarounds and making other kinds of architectures sort of go all the way, so. In short, it's turned\nout to be a lot easier to build close to human\nintelligence than we thought, and that means our runway as a species to get our shit together has has shortened. - And it seems like the scary thing about the effectiveness\nof large language models, so Sam Altman, I've recently\nhad conversation with, and he really showed that\nthe leap from GPT-3 to GPT-4 has to do with just a bunch of hacks, a bunch of little explorations\nwith smart researchers doing a few little fixes here and there. It's not some fundamental leap and transformation in the architecture. - And more data and more compute. - And more data and compute, but he said the big leaps has to do with not the data and the compute, but just learning this new discipline, just like you said. So researchers are going to\nlook at these architectures and there might be big\nleaps where you realize, \"Wait, why are we doing\nthis in this dumb way?\" And all of a sudden this\nmodel is 10x smarter. And that that can happen on any one day, on any one Tuesday or Wednesday afternoon. And then all of a sudden you have a system that's 10x smarter. It seems like it's such a new discipline, it's such a new, like we understand so little about why this thing works so damn well, that the linear improvement of compute, or exponential, but the steady improvement of compute, steady improvement of the data may not be the thing that\neven leads to the next leap. It could be a surprise little\nhack that improves everything. - Or a lot of little leaps here and there because so much of this\nis out in the open also, so many smart people are looking at this and trying to figure out\nlittle leaps here and there, and it becomes this sort\nof collective race where, a lot of people feel, \"If I don't take the\nleap someone else will,\" and it is actually very crucial\nfor the other part of it, why do we wanna slow this down? So again, what this open\nletter is calling for is just pausing all training of systems that are more powerful\nthan GPT-4 for six months. Just give a chance for the labs to coordinate\na bit on safety, and for society to adapt, give the right incentives to the labs. 'cause I, you know, you've interviewed a lot of\nthese people who lead these labs and you know just as well as I do that they're good people, they're idealistic people. They're doing this first and foremost because they believe that AI has a huge potential to help humanity. But at the same time they are trapped in this horrible race to the bottom. Have you read \"Meditations on Moloch\" by Scott Alexander? - [Lex] Yes. - Yeah, it's a beautiful\nessay on this poem by Ginsberg where he interprets it as\nbeing about this monster. It's this game theory\nmonster that pits people against each other in\nthis race to the bottom where everybody ultimately loses. And the evil thing about this monster is even though everybody\nsees it and understands, they still can't get\nout of the race, right? A good fraction of all the\nbad things that we humans do are caused by Moloch. And I like Scott Alexander's\nnaming of the monster. So we can, we humans can think of it as a thing. If you look at why do we have overfishing, why do we have more generally, the tragedy of the commons. Why is it that, so Liv Boeree, I don't know if you've\nhad her on your podcast. - Mhm, yeah. She's become a friend, yeah. - Great, she made this\nawesome point recently that beauty filters that a lot of female influencers feel pressure to use, are exactly Moloch in action again. First, nobody was using them, and people saw them\njust the way they were, and then some of 'em started using it, and becoming ever more plastic fantastic, and then the other ones\nthat weren't using it started to realize that, if they wanna to keep\ntheir their market share, they have to start using it too. And then you're in a situation\nwhere they're all using it, and none of them has any more market share or less than before. So nobody gained anything, everybody lost, and they have to keep becoming ever more plastic fantastic also, right? But nobody can go back to the old way because it's just too costly, right? Moloch is everywhere, and Moloch is not a new\narrival on the scene either. We humans have developed a lot\nof collaboration mechanisms to help us fight back against Moloch through various kinds of\nconstructive collaboration. The Soviet Union and the United States did sign a number of arms control treaties against Moloch who is trying to stoke them into unnecessarily risky\nnuclear arms races, et cetera, et cetera. And this is exactly what's\nhappening on the AI front. This time it's a little bit geopolitics, but it's mostly money, where there's just so\nmuch commercial pressure. You know, if you take any of these leaders of the top tech companies, if they just say, you know, \"This is too risky, I want\nto pause for six months.\" They're gonna get a lot of pressure from shareholders and others. They're like, \"Well\nyou know, if you pause, but those guys don't pause. We don't wanna get our lunch eaten.\" - [Lex] Yeah. - And shareholders even have the power to replace the executives\nin the worst case, right? So we did this open letter\nbecause we want to help these idealistic tech executives to do what their heart tells them, by providing enough public\npressure on the whole sector. Just pause, so that they can all pause in a coordinated fashion. And I think without the public pressure, none of them can do it alone. Push back against their shareholders no matter how goodhearted they are, 'cause Moloch is a really powerful foe. - So the idea is to, for the major developers\nof AI systems like this, so we're talking about Microsoft, Google, Meta, and anyone else. - Well OpenAI is very\nclose with Microsoft now, - With Microsoft, right, yeah.\n- of course, - And there there are\nplenty of smaller players. for example, Anthropic\nis is very impressive, there's Conjecture, there's many, many, many players, I don't wanna make a long list that sort of leave anyone out. And for that reason, it's so important that\nsome coordination happens, that there's external\npressure on all of them, saying, \"You all need the pause.\" 'Cause then, the people, the researchers in there\nat these organizations, the leaders who wanna\nslow down a little bit, they can say to their\nshareholders, you know, \"Everybody's slowing down\nbecause of this pressure and it's the right thing to do.\" - Have you seen in history, there examples what it's possible to pause the Moloch?\n- Yes, absolutely. And even like human cloning for example, you could make so much\nmoney on human cloning. Why aren't we doing it? Because biologists thought hard about this and felt like this is way too risky, they got together in the\nseventies in Asilomar, and decided even to stop a lot more stuff, also just editing the\nhuman germline, right? Gene editing that goes\nin to our offspring, and decided, \"Let's not do this because it's too unpredictable\nwhat it's gonna lead to,\" we could lose control over\nwhat happens to our species,\" so they paused. There was a ton of money to be made there, So it's very doable, but you need a public awareness\nof what the risks are, and the broader community\ncoming in and saying, \"Hey, let's slow down.\" And you know, another\ncommon pushback I get today, is that we can't stop in\nthe West because China. And in China undoubtedly, they also get told, \"We can't\nslow down because the West,\" because both sides think\nthey're the good guy. - [Lex] Yeah. - But look at human cloning, you know? Did China forge ahead with human cloning? There's been exactly one human cloning that's actually been done that I know of. It was done by a Chinese guy. Do you know where he is now? - [Lex] Where? - In jail. And you know who put him there? - [Lex] Who? - Chinese government. Not because Westerners said, \"China look, this is...\" No the Chinese government put him there 'cause they also felt, they like control, the Chinese government. If anything, maybe they're\neven more concerned about having control than Western governments, have no incentive of just losing control over where everything is going, and you can also see the Ernie Bot that was released by, I believe, Baidu recently, they got a lot of pushback\nfrom the government and had to rein it in,\nyou know, in a big way. I think once this basic message comes out that this isn't an arms\nrace, it's a suicide race, where everybody loses if anybody's AI goes out of control, it really changes the whole dynamic. It's not, and I'll say this again 'cause this is this very basic point I think a lot of people get wrong. Because a lot of people\ndismiss the whole idea that AI can really get very superhuman because they think there's something really magical about intelligence such that it can only\nexist in human minds, you know, because they believe that, they think it's gonna kind\nof get to just more or less \"GPT-4 plus plus,\" and then that's it. They don't see it as a suicide race. They think whoever gets that first, they're gonna control the world, they're gonna win. That's not how it's gonna be. And we can talk again about\nthe scientific arguments from why it's not gonna stop there. But the way it's gonna be, is if anybody completely loses control and you know, you don't care if someone manages to take over the world who really doesn't share your goals, you probably don't really\neven care very much about what nationality they have, you're not gonna like it\nmuch worse than today. If you live in Orwellian dystopia, what do you care who's created it, right? And if someone, if it goes farther, and we just lose control\neven to the machines, so that it's not us versus them, it's us versus it. What do you care who created\nthis unaligned entity which has goals different\nfrom humans, ultimately? And we get marginalized,\nwe get made obsolete, we get replaced. That's what I mean when I\nsay it's a suicide race, it's kind of like we're\nrushing towards this cliff, but the closer the cliff we get, the more scenic the views are, and the more money there is there, and the more, so we keep going, but we have to also stop\nat some point, right? Quit while we're ahead, And it's, it's a suicide race which cannot be won, but the way to really benefit from it is, to continue developing awesome\nAI a little bit slower. So we make it safe, make sure it does the\nthings that humans want, and create a condition\nwhere everybody wins. The technology has shown us that, you know, geopolitics\nand politics in general is not a zero sum game at all. - So there is some rate of\ndevelopment that will lead us as a human species to\nlose control of this thing. And the hope you have is that there's some\nlower level of development which will not allow us to lose control. This is an interesting thought you have about losing control, so if you have somebody, if you are somebody like Sundar Pichai or Sam Altman at the head\nof a company like this, you're saying if they develop an AGI, they too will lose control of it. So no one person can maintain control, no group of individuals\ncan maintain control. - If it's created very, very soon and is a big black box\nthat we don't understand like the large language models, yeah. Then I'm very confident\nthey're gonna lose control. But this isn't just me\nsaying it, you know, Sam Altman and Demis Hassabis have both said, they themselves acknowledge that, you know, there's really\ngreat risks for this and they want slow down once\nthey feel it gets scary. But it's clear that they're stuck in this, again, Moloch is forcing\nthem to go a little faster than they're comfortable with because of pressure from, just commercial pressures, right? To get a bit optimistic here, of course, this is a problem\nthat can be ultimately solved. To win this wisdom race, it's clear that what we\nhope that was gonna happen hasn't happened. The capability progress has gone faster than a lot of people thought, and the progress in the public sphere of policy making and so on, has gone slower than we thought. Even the technical AI\nsafety has gone slower. A lot of the technical safety research was kind of banking on\nthat large language models and other poorly understood systems couldn't get us all the way. That you had to build more\nof a kind of intelligence that you could understand. Maybe it could prove itself safe, you know, things like this, and I'm quite confident\nthat this can be done so we can reap all the benefits, but we cannot do it as quickly as this out of control\nexpress train we are on now is gonna get to AGI. That's why we need a\nlittle more time, I feel. - Is there something to be said, well like Sam Altman talked about, which is while we're in the pre-AGI stage, to release often and as\ntransparently as possible to learn a lot. So as opposed to being extremely cautious, release a lot, don't invest in a closed development where you focus on the AI safety. While it's somewhat \"dumb,\" quote-unquote, release as often as possible. And as you start to see signs\nof human-level intelligence and or superhuman level intelligence, then you put a halt on it. Well what a lot of safety researchers have been saying for many years is that the most dangerous\nthings you can do with an AI is first of all\nteach it to write code. - [Lex] Yeah. - Because that's the first step towards recursive self-improvement, which can take it from\nAGI to much higher levels. Okay? Oops, we've done that. And another thing high risk is connect it to the internet, let it go to websites, download stuff on its\nown and talk to people. Oops, we've done that already. You know Eliezer Yudkowsky, you said you interviewed\nhim recently, right? - [Lex] Yes, yep. - So he had this tweet\nrecently which said, gave me one of the best laughs in a while, where he is like, \"Hey, people used to\nmake fun of me and say, 'You're so stupid, Eliezer.' 'Cause you're saying you have to worry of obviously developers once they get to like really strong AI, first thing you're gonna do is like, never connect it to the internet, keep it in a box. where you know, you can\nreally study it safe.\" So he had written it in\nthe like in the meme form so it's like \"Then,\" and then that, and then, \"Now.\" (Lex laughing) \"LOL, let's make a chatbot.\" (both laughing) - [Lex] Yeah, yeah, yeah. - And the third thing is Stuart Russell. - [Lex] Yeah. - You know, amazing AI researcher. He has argued for a while\nthat we should never teach AI anything about humans. Above all, we should never let it learn about human psychology and\nhow you manipulate humans. That's the most dangerous kind\nof knowledge you can give it. Yeah, you can teach it\nall it needs to know about how to cure cancer\nand stuff like that. But don't let it read\nDaniel Kahneman's book about cognitive biases and all that. And then oops, \"LOL, you know, let's invent social media recommender algorithms\nwhich do exactly that.\" They get so good at knowing\nus and pressing our buttons that we are starting to create a world now where we just have ever more hatred, 'cause they've figured\nout that these algorithms, not for out of evil, but just to make money on advertising, that the best way to get more engagement, the euphemism, get people glued to their\nlittle rectangles, right? Is just to make them pissed off. - Well that's really interesting that a large AI system that's\ndoing the recommender system kind of task on social media, is basically just studying human beings because it's a bunch of\nus rats giving it signal, nonstop signal. It'll show a thing and\nthen we give signal, and whether we spread that\nthing, we like that thing, that thing increases our engagement, gets us to return to the platform, and it has that on the scale of hundreds of millions\nof people constantly. So it's just learning, and\nlearning, and learning, and presumably if the number of parameters in the neural network\nthat's doing the learning, and more end to end the learning is, the more it's able to\njust basically encode how to manipulate human behavior. - [Max] Exactly. - How to control humans at scale. - Exactly, and that is\nnot something you think is in humanity's interest. And right now it's mainly letting some humans manipulate other\nhumans for profit and power, which already caused a lot of damage, and then eventually that's a sort of skill that can make AI persuade\nhumans to let them escape whatever safety precautions we had put, you know, there was a really nice article in the New York Times\nrecently by Yuval Noah Harari and two co-authors\nincluding Tristan Harris from \"The Social Dilemma,\" and we have this phrase in there I love, It said, \"Humanity's first\ncontact with advanced AI was social media.\" And we lost that one. We now live in a country where there's much more hate in the world where there's much more hate, in fact. And in our democracy than\nwe're having this conversation, and people can't even agree on who won the last election, you know. And we humans often point fingers at other humans and say it's their fault, but it's really Moloch\nin these AI algorithms. We got the algorithms and then Moloch pitted the social media\ncompanies against each other so nobody could have a\nless creepy algorithm 'cause then they would lose out on revenue to the other company. - Is there any way to win that battle back if we just linger on this one battle that we've lost in terms of social media, is it possible to redesign social media, this very medium in which\nwe use as a civilization to communicate with each other, to have these kinds of conversation, to have discourse, to try to figure out how to solve the biggest problems in the world, whether that's nuclear war\nor the development of AGI. Is is it possible to do\nsocial media correctly? - I think it's not only\npossible, but it's necessary. Who are we kidding? That we're gonna be able to\nsolve all these other challenges if we can't even have a\nconversation with each other? It's constructive. The whole idea, the key idea of democracy is that you get a bunch of people together and they have a real conversation. The ones you try to foster on this podcast where you respectfully listen\nto people you disagree with. And you realize actually, you know, there are some things actually some common ground we have and let's, we both agree, let's not\nhave any nuclear wars, let's not do that, et cetera, et cetera. We're kidding ourselves that\nthinking we can face off the second contact with\never more powerful AI that's happening now with these large language\nmodels if we can't even have a functional conversation\nin the public space. That's why I started the\nImprove The News project, improvethenews.org. But I'm an optimist fundamentally, in that there is a lot of\nintrinsic goodness in people. And that what makes the difference between someone doing\ngood things for humanity and bad things is not some\nsort of fairytale thing, that this person was\nborn with the evil gene and this one is born with the good gene. No, I think it's whether we put, whether people find\nthemselves in situations that bring out the best in them or that bring out the worst in them. And I feel we're building an internet and a society that brings out the worst. - But it doesn't have to be that way. - [Max] No, it does not. - It's possible to create incentives and also create incentives\nthat make money. That both make money and\nbring out the best in people. - I mean, in the long term, it's not a good investment\nfor anyone, you know, to have a nuclear war, for example. And you know, is it a good investment for humanity if we just ultimately replace\nall humans by machines, and then we're so\nobsolete that eventually, there are no humans left? Well, it depends guess\nhow you do the math, But I would say by any\nreasonable economic standard, if you look at the future income of humans and there aren't any, you know, that's not a good investment. Moreover, like why can't we have a little bit of pride\nin our species, damn it? You know, why should we just build another species that gets rid of us? If we were Neanderthals, would we really consider it a smart move if we had really advanced\nbiotech to build Homo sapiens? You know, you might say, \"Hey Max, you know, yeah, let's build, these Homo sapiens, they're\ngonna be smarter than us, maybe they can help us, defend us better against predators and help fix up our\ncaves, make them nicer, we'll control 'em undoubtedly, you know?\" So then they build a couple, a little baby girl, little baby boy. They either, and then you have some wise\nold Neanderthal elder is like, \"Hmm, I'm scared that we're\nopening a Pandora's box here and that we're gonna\nget outsmarted by these super Neanderthal intelligences, and there won't be any Neanderthals left.\" But then you have a bunch of\nothers in the cave, right? \"You're such a Luddite scaremonger. Of course, they're gonna\nwant to keep us around 'cause we are their creators, and, you know, the smarter, I think the smarter they get, the nicer they're gonna get, they're gonna leave us. They're gonna want us around\nand it's gonna be fine, and besides look at these\nbabies, they're so cute. Clearly they're totally harmless.\" Those babies are exactly GPT-4. It's not, I wanna be clear, it's not GPT-4 that's terrifying. It's that GPT-4 is a baby technology, you know, and Microsoft even\nhad a paper recently out, titled something like, \"Sparkles of AGI.\" Well they were basically\nsaying this is baby AI, like these little Neanderthal babies, and it's gonna grow up. There's gonna be other\nsystems from the same company, from other companies, they'll be way more powerful, but they're gonna take all the things, ideas from these babies\nand before we know it, we're gonna be like\nthose last Neanderthals who were pretty disappointed when they realized that\nthey were getting replaced. - Well, this interesting point you make, which is of programming, it's entirely possible that GPT-4 is already the kind of system that can change everything\nby writing programs. - Yeah, it's because it's life 2.0, the systems I'm afraid\nof are gonna look nothing like a large language\nmodel, and they're not, but once it gets, once it or other people figure out a way of using this tech to make\nmuch better tech, right? It's just constantly\nreplacing its software. And from everything that we've seen about how these work under the hood, they're like the minimum\nviable intelligence. They do everything, you know, the dumbest way that still works, sort of. - [Lex] Yeah. - And so they're life 3.0, except when they replace their software, it's a lot faster than when\nyou decide to learn Swedish. Poof. (fingers snapping) And moreover, they think\na lot faster than us too. So when, you know, we don't think, have one logical step every nanosecond or few, or so, the way they do, and we can't also just\nsuddenly scale up our hardware massively in the cloud 'cause\nwe're so limited, right? So they are, and they are also life, can soon become a little\nbit more like life 3.0 in that if they need more hardware, hey, just rent it in the cloud, you know? \"How do you pay for it?\" \"Well, with all the services you provide.\" - And what we haven't seen yet, which could change a lot, is entire software systems. So right now programming is\ndone sort of in bits and pieces as an assistant tool to humans. But I do a lot of programming and with the kind of stuff\nthat GPT-4 is able to do, I mean, it's replacing a lot\nwhat I'm able to do, right? You still need a human in the loop to kind of manage the design of things, manage like, what are the prompts that generate the kind of stuff to do some basic adjustment of the codes, do some debugging, but if it's possible\nto add on top of GPT-4, kind of a feedback loop of self-debugging, improving the code, and then you launch that\nsystem onto the wild on the internet because\neverything is connected, and have it do things, have it interact with humans\nand then get that feedback, now you have this giant\necosystem of humans. That's one of the things that Elon Musk recently sort of tweeted as a case why everyone\nneeds to pay $7 or whatever for Twitter, - [Max] To make sure they're real. - Make sure they're real, we're now going to be living in a world where the bots are getting smarter, and smarter, and smarter\nto a degree where, you can't tell the difference between a human and a bot. - [Max] That's right. - And now you can have\nbots outnumber humans by 1 million to one. Which is why he's making a\ncase why you have to pay. To prove you're human, which is one of the only\nmechanisms to prove, which is depressing. - And yeah, I feel we have to remember, as individuals, we\nshould from time to time, ask ourselves why are we\ndoing what we're doing, right? And as a species, we need to do that too. So if we're building, as you say, machines that are outnumbering us, and more and more outsmarting us, and replacing us on the job market, not just for the dangerous\nand and boring tasks, but also for writing poems and doing art, and things that a lot of\npeople find really meaningful, we gotta ask ourselves, why? Why are we doing this? The answer is Moloch is\ntricking us into doing it. And it's such a clever trick that even though we see the trick, we still have no choice\nbut to fall for it, right? And also, thing you said about you using co-pilot AI tools to program faster, how many, what factor faster would\nyou say you code now? Does it go twice as fast? Or, - I don't really, because it's such a new tool. - [Max] Yeah. - I don't know if speed\nis significantly improved, but it feels like I'm a year away from being 5 to 10 times faster. - So if that's typical for programmers, then you're already seeing another kind of recursive self-improvement, right? Because previously, like a major generation of\nimprovement of the codes would happen on the\nhuman R and D time scale. And now if that's five times shorter, then it's gonna take five times less time than it otherwise would to develop the next level of these tools, and so on. So this is exactly the sort of beginning of an intelligence explosion. There can be humans in the\nloop a lot in the early stages, and then eventually humans\nare needed less and less and the machines can\nmore kind of go alone. But what you said there\nis just an exact example of these sort of things. Another thing which, I was kind of lying on my\npsychiatrist imagining, I'm on a psychiatrist couch here saying, \"Well what are my fears\nthat people would do with AI systems?\" So I mentioned three\nthat I had fears about many years ago, that they would do, namely teach it to code, connect it to the internet, and teach it to manipulate humans. A fourth one is building an API, (Lex chuckles) where code can control this\nsuper powerful thing, right? That's very unfortunate because one thing that systems like GPT-4\nhave going for them is that they are an oracle in the sense that they just answer questions. There's no robot connected to GPT-4. GPT-4 can't go and do stock trading based on its thinking. It is not an agent, and an intelligent agent is something that takes in information from the world, processes it, to figure out what action to take based on its goals that it has, and then does something back on the world. But once you have an API for, for example, GPT-4, nothing stops Joe Schmoe and a lot of other people\nfrom building real agents, which just keep making calls somewhere in some inner loop somewhere to these powerful oracle systems, which makes themselves much more powerful. That's another kind of\nunfortunate development, which I think we would've\nbeen better off delaying. I don't wanna pick on\nany particular companies, I think they're all under a\nlot of pressure to make money. - [Lex] Yeah. - And again, the reason we're\nwe're calling for this pause is to give them all cover to do what they know is the right thing, just slow down a little bit at this point. But everything we've talked about, I hope we'll make it clear\nto people watching this, you know, why these sort\nof human-level tools can cause a gradual acceleration. You keep using yesterday's technology to build tomorrow's technology. And when you do that over and over again, you naturally get an explosion. You know, that's the definition of an explosion in science, right? If you have two people, and they fall in love, now you have four people, and then they can make more babies, and now you have eight people, and then you have 16, 32, 64, et cetera. We call that a population explosion where it's just that each, if it's instead free neutrons\nin a nuclear reaction that if each one can make more than one, then you get an\nexponential growth in that, we call it a nuclear explosion. All explosions are like that, and an intelligence explosion, it's just exactly the same principle, that some amount of intelligence can make more intelligence than that, and then repeat. You always get exponentials. - What's your intuition why it does, you mentioned there's\nsome technical reasons why it doesn't stop at a certain point. What's your intuition? And do you have any\nintuition why it might stop? - It's obviously gonna stop when it bumps up against\nthe laws of physics. There are some things you just can't do no matter how smart you are, right? - Allegedly. 'Cause we don't know all the full laws of physics yet, right? - Seth Lloyd wrote a really cool paper on the physical limits on\ncomputation, for example. If you make it, put too much energy into it and the finite space will\nturn into a black hole, you can't move information around faster than the speed of\nlight, stuff like that. But it's hard to store way more than a modest number\nof bits per atom, et cetera. But, you know, those limits are just astronomically above, like 30 orders of magnitude\nabove where we are now. So, you know. Bigger difference, bigger\njump in intelligence than if you go from ant to a human. I think, of course what we want to do is have a controlled thing, in a nuclear reactor you put moderators in to make sure exactly it doesn't blow up out of control, right? When we do, experiments with biology\nand cells and so on, you know, we also try to make sure it doesn't get out of control. We can do this with AI too. The thing is, we haven't succeeded yet. And Moloch is exactly doing the opposite. Just fueling, just egging everybody on, \"Faster, faster, faster, or the other company is\ngonna catch up with you, or the other country is\ngonna catch up with you.\" We have to want to stop, and I don't believe in just asking people to look into their hearts\nand do the right thing. It's easier for others to say that, but like, if you are in this situation where your company is gonna get screwed by other companies that are not stopping, you're putting people in\na very hard situation, the right thing to do is change the whole\nincentive structure instead. And this is not an old, maybe I should say one\nmore thing about this, 'cause Moloch has been around as humanity's number\none or number two enemy since the beginning of civilization. And we came up with some\nreally cool countermeasures. Like first of all, already over 100,000 years ago, evolution realized that\nit was very unhelpful that people kept killing\neach other all the time. So it genetically gave us compassion and made it so that, like if you get two drunk dudes getting into a pointless bar fight, they might give each other black eyes, but they have a lot of inhibition towards just killing each other. That's a, And similarly, if you find\na baby lying on the street, when you go out for your\nmorning jog tomorrow, you're gonna stop and pick it up, right? Even though it maybe make you\nlate for your next podcast. So evolution gave us these genes that make our own egoistic incentives more aligned with what's good for the greater group\nwe're part of, right? And then as we got a\nbit more sophisticated and developed language, we invented gossip, which is also a fantastic\nanti-Moloch, right? 'Cause now, it really discourages\nliars, moochers, cheaters, because their own incentive\nnow is not to do this because word quickly gets around and then suddenly people\naren't gonna invite them to their dinners anymore or trust them. And then when we got\nstill more sophisticated in bigger societies, you know, we invented the legal system where even strangers who\ncouldn't rely on gossip and things like this\nwould treat each other, would have an incentive. Now those guys in the bar fights, even if someone is so drunk that he actually wants\nto kill the other guy, he also has a little thought\nin the back of his head that, you know, \"Do I really wanna\nspend the next 10 years eating like really crappy\nfood in a small room? I'm just gonna chill out,\" you know? And we similarly have tried to give these incentives to our corporations by having regulation and\nall sorts of oversight so that their incentives are\naligned with the greater good. We tried really hard, and the big problem\nthat we're failing now, is not that we haven't tried before, but it's just that the tech is growing, is developing much faster than the regulators been\nable to keep up, right? So regulators, it's kind of comical that\nthe European Union right now is doing this AI act, right? And in the beginning they had\na little opt-out exception that GPT-4 would be completely\nexcluded from regulation. Brilliant idea. - What's the logic behind that? - Some lobbyists pushed\nsuccessfully for this? So we were actually quite involved with the Future of Life Institute, Mark Brakel, Risto Uuk, Anthony Aguirre, and others, you know, we're quite\ninvolved with talking to, educating various people\ninvolved in this process about these general-purpose\nAI models coming, and pointing out that they\nwould become the laughing stock if they didn't put it in. So the French started pushing for it, it got put in to the draft, and it looked like all was good, and then there was a huge\ncounter push from lobbyists. Yeah, there were more\nlobbyists in Brussels from tech companies than from\noil companies, for example. And it looked like it might, this was gonna maybe get taken out again. And now GPT-4 happened, and I think it's gonna stay in. But this just shows, you know, Moloch can be defeated. But the challenge we're\nfacing is that the tech is generally much faster than\nwhat the policymakers are, and a lot of the policymakers also don't have a tech background, so it's, you know, we really need to work\nhard to educate them on what's taking place here. So we're getting this situation where the first kind of, so I define artificial intelligence just as non-biological\nintelligence, right? And by that definition, a company, a corporation is\nalso an artificial intelligence because the corporation isn't its humans, it's a system. If its CEO decides, if a CEO of a tobacco\ncompany decides one morning that she or he doesn't wanna\nsell cigarettes anymore, they'll just put another CEO in there. It's not enough to align the incentives of individual people or align individual computers'\nincentives to their owners, which is what technically,\nAI safety research is about. You also have to align the\nincentives of corporations with the greater good. And some corporations have\ngotten so big and so powerful very quickly that in many cases, their lobbyists instead\nalign the regulators to what they want rather\nthan the other way round. It's a classic regulatory capture. - Right, is the thing that\nthe slowdown hopes to achieve is give enough time to\nregulators to catch up, or enough time to the companies themselves to breathe and understand how to do AI safety correctly? - I think both, but I think that the vision, the path to success I see is first you give a breather actually to the people in these companies, their leadership who wants\nto do the right thing, and they all have safety teams and so on, on their companies, give them a chance to get\ntogether with the other companies, and the outside pressure can\nalso help catalyze that, right? And work out what is it that's, what are the reasonable\nsafety requirements one should put on future systems\nbefore they get rolled out. There are a lot of people also in academia and elsewhere outside of these companies who can be brought into this and have a lot of very good ideas. And then I think it's very\nrealistic that within six months, you can get these people coming up, so here's a white paper, here's what we all think it's reasonable. You know, you didn't, just because cars killed a lot of people, you didn't ban cars, but they got together a bunch of people and decided, you know, in order to be allowed to sell a car, it has to have a seatbelt in it. They're the analogous things that you can start requiring\na future AI systems so that they are safe. And once this heavy lifting, this intellectual work has been done by experts in the field,\nwhich can be done quickly, I think it's going to be quite easy to get policymakers to see, yeah, this is a good idea. And it's, you know, for the companies to fight Moloch, they want, and I believe Sam Altman has explicitly called for this, they want the regulators\nto actually adopt it so that their competition is gonna abide by it too, right? You don't want, you don't want to be\nenacting all these principles and then you abide by them, and then there's this one little company that doesn't sign onto it and then now they can\ngradually overtake you. Then the companies will get, be able to sleep secure knowing that everybody's playing\nby the same rules. - So do you think it's\npossible to develop guardrails that keep the systems from basically damaging\nirreparably humanity, while still enabling sort\nof the capitalist-fueled competition between companies as they develop how to best\nmake money with this AI? You think there's a\nbalancing that's possible? - Absolutely, I mean, we've seen that in many other sectors where you've had the free market produce quite good things without causing particular harm. When the guardrails are there\nand they work, you know, capitalism is a very\ngood way of optimizing for just getting the same\nthings done more efficiently. But it was good, you know, and like in hindsight,\nand I never met anyone, even on parties way over on the right, in any country who think it was a bad, thinks it was a terrible idea to ban child labor, for example. - Yeah, but it seems like\nthis particular technology has gotten so good so fast, become powerful to a\ndegree where you could see in the near term, the ability to make a lot of money. - [Max] Yeah. - And to put guardrails, to develop guardrails quickly\nin that kind of context seems to be tricky. It's not similar to cars or child labor, it seems like the opportunity\nto make a lot of money here very quickly is right here before us. - So again, there's this cliff. - Yeah, it gets quite scenic, (laughs) - [Max] The closer to the cliff you go, - Yeah. - The more money there is, the more gold ingots\nthere are on the ground you can pick up or whatever, if you want to drive there very fast, but it's not in anyone's incentive that we go over the cliff and it's not like\neverybody's in the wrong car. All the cars are connected\ntogether with a chain. So if anyone goes over, they'll start dragging\nthe others down too. And so ultimately it's in the selfish interests also of the\npeople in the companies to slow down when you just start seeing the contours of the cliff\nthere in front of you, right? And the problem is that, even though the people who\nare building the technology, and the CEOs, they really get it, the shareholders and\nthese other market forces, they are people who don't honestly, understand that the cliff is there, they usually don't. You have to get quite into the weeds to really appreciate how\npowerful this is and how fast. And a lot of people are\neven still stuck again in this idea that in this \"carbon chauvinism\"\nas I like to call it, that you can only have our\nlevel of intelligence in humans, that there's something magical about it. Whereas the people in the tech companies who build this stuff, they all realize that intelligence is information processing\nof a certain kind, and it really doesn't matter at all whether the information is\nprocessed by carbon atoms in neurons, in brains, or by silicon atoms in\nsome technology we build. So you brought up capitalism earlier, and there are a lot of\npeople who love capitalism and a lot of people who\nreally, really don't. And it struck me recently, that what's happening with capitalism here is exactly analogous to the way in which superintelligence\nmight wipe us out. Do you know why I studied\neconomics for my undergrad? Stockholm School of Economics, yay. (Lex laughing) - Well, no. No why, tell me. - So I was very interested in how you could use market forces to just get stuff done more efficiently, but give the right incentives to market so that it wouldn't do really bad things. So Dylan Hadfield-Menell, who's a professor and\ncolleague of mine at MIT, wrote this really interesting paper with some collaborators recently, where they proved mathematically that if you just take one goal\nthat you just optimize for, on and on, and on, indefinitely, that you think is gonna bring\nyou in the right direction, what basically always happens is, in the beginning, it will\nmake things better for you, but if you keep going, at some point, it's gonna start making\nthings worse for you again. And then gradually, it's gonna make it\nreally, really terrible. So just as a simple, the way I think of the proof is, suppose you want to go from\nhere back to Austin for example, and you're like, \"Okay, yeah, let's go south,\" but you put in exactly sort\nof the right direction. Just optimize that, as south as possible. You get closer and closer to Austin, but there's always some little error. So you're not going\nexactly towards Austin, but you get pretty close, but eventually, you\nstart going away again, and eventually, you're gonna\nbe leaving the solar system. - [Lex] (chuckles) Yeah. - And they proved, it's a beautiful mathematical proof, this happens generally, and this is very important for AI because, even though Stuart\nRussell has written a book and given a lot of talks\non why it's a bad idea to have AI just blindly\noptimize something, that's what pretty much\nall our systems do. - [Lex] Yeah. - We have something\ncalled the loss function that we're just minimizing, or reward function, we're\njust maximizing, and, capitalism is exactly like that too. We wanted to get stuff\ndone more efficiently, the people wanted. So introduce the free market. Things got done much more\nefficiently than they did in say, communism, right? And it got better. But then it just kept optimizing, and kept optimizing, and you got every bigger companies, and every more efficient\ninformation processing and now also very much powered by IT, and eventually a lot of\npeople are beginning to feel, \"Wait, we're kind of\noptimizing a bit too much. Like why did we just chop\ndown half the rainforest?\" You know, and why did suddenly these\nregulators get captured by lobbyists and so on? It's just the same optimization that's been running for too long. If you have an AI that actually\nhas power over the world and you just give it one goal, and just like keep optimizing that, most likely everybody's gonna be like, \"Yay, this is great.\" In the beginning things\nare getting better, but it's almost impossible to give it exactly the right\ndirection to optimize in. And then eventually all\nhell breaks loose, right? Nick Bostrom and others\nhave given examples that sound quite silly, Like what if you just want to like, tell it to cure cancer or something, and that's all you tell it, maybe it's gonna decide to\ntake over an entire continent just so we can get more\nsupercomputer facilities in there, and figure out how to\ncure cancer backwards, and then you're like, \"Wait, that's not what I wanted,\" right? And the issue with capitalism and the issue with runaway\nAI have kind of merged now, because that Moloch I talked about is exactly the capitalist Moloch that, we have built an economy that is optimizing for only one thing. Profit, right? And that worked great back when things were very inefficient, and then now it's getting done better, and it worked great as\nlong as the companies were small enough that they\ncouldn't capture the regulators. But that's not true anymore, but they keep optimizing, and now they realize that they can, these companies can make even more profit by building ever more powerful\nAI even if it's reckless, but optimize more and more,\nand more, and more, and more. So this is Moloch again showing up. And I just wanna, anyone here who has any concerns about late-stage capitalism\nhaving gone a little too far, you should worry about superintelligence 'cause it's the same\nvillain in both cases. It's Moloch. - And optimizing one objective\nfunction aggressively, blindly is going to take us there. - Yeah, we have to pause from time to time and look into our hearts and ask why are we doing this? Is this, am I still going towards Austin, or have I gone too far? You know, maybe we\nshould change direction. - And that is the idea behind\nthe halt for six months. Why six months? That seems like a very short period. Can we just linger and\nexplore different ideas here, because this feels like a really important moment in human history, where pausing would actually have a significant positive effect. - We said six months, because we figured the number one pushback that we're gonna get in the\nWest was like, \"But China?\" and everybody knows\nthere's no way that China is gonna catch up with the\nWest on this in six months. So that argument goes off the table and you can forget about\ngeopolitical competition and just focus on the real issue. That's why we put this. - That's really interesting. But you've are already made\nthe case that even for China, if you actually wanna\ntake on that argument, China too would not be\nbothered by a longer halt because they don't wanna lose control even more than the West doesn't. - That's what I think, yeah. - That's a really interesting argument. Like I have to actually\nreally think about that, which, the kind of thing people assume is if you develop an AGI, that OpenAI, if they're the ones\nthat do it, for example, they're going to win. But you're saying no, everybody loses. - Yeah, it's gonna get\nbetter and better and better, and then kaboom, we all lose. That's what's gonna happen. - When lose and win\nare defined on a metric of basically quality of\nlife for human civilization, and for Sam Altman. (laughs) Both. - To be blunt, my personal guess, you know, and people\ncan quibble with this, is that we're just gonna, there won't be any humans. That's it, that's what I mean by lose. You know, if you, we can see in history, once you have some species\nor some group of people who aren't needed anymore, doesn't usually work out\nso well for them, right? - [Lex] Yeah. - There were a lot of horses\nthat were used for traffic in Boston and then the car got invented and most of them got, yeah, well. (laughs) We don't need to go there. And if you look at humans, you know, right now, why did the\nlabor movement succeed? And after the Industrial Revolution? Because it was needed. Even though we had a lot of Molochs, and there was child labor\nand so on, you know, the company still needed to have workers, and that's why strikes\nhad power and so on. If we get to the point where most humans aren't needed anymore, I think it's quite naive to think that they're gonna still be treated well. You know, we say that. Yeah, yeah everybody's equal, and the government will\nalways protect them. But if you look in practice, groups that are very disenfranchised and don't have any actual\npower usually get screwed. And now in the beginning, so Industrial Revolution, we automated away muscle work, but that got, worked out pretty well eventually, because we educated ourselves and started working\nwith our brains instead and got usually more\ninteresting, better paid jobs. But now we're beginning\nto replace brain work. So we replaced a lot of boring stuff, like we got the pocket calculator, so you don't have people adding, multiplying numbers anymore at work. Fine, there were better\njobs they could get. But now GPT-4, you know, and the Stable Diffusion\nand techniques like this, they're really beginning to blow away some jobs that people really loved having. There was a heartbreaking\narticle post just yesterday on social media I saw, about this guy who was doing 3D modeling for gaming and he, and all of a sudden now\nhe got this new software he just sets prompts, and he feels this whole job that he loved just lost its meaning, you know? And I asked GPT-4 to rewrite \"Twinkle, Twinkle, Little Star\"\nin the style of Shakespeare, I couldn't have done such a good job. It was really impressive. You've seen a lot of the\nart coming out here, right? So I'm all for automating\naway the dangerous jobs and the boring jobs. But I think you hear a lot, some arguments which are too glib. Sometimes people say, \"Well that's all that's gonna happen. We're getting rid of the boring, tedious, dangerous jobs,\" it's just not true. There are a lot of really interesting jobs that are being taken away now. Journalism is gonna get crushed, coding is gonna get crushed. I predict the job market for programmers, the salaries are gonna start dropping. You know, if you said you\ncan code five times faster, you know, then you need five\ntimes fewer programmers, maybe there will be more output also, but then you'll still end up using fewer, needing fewer programmers than today. And I love coding, you know, I think it's super cool. So we need to stop and ask ourselves why again are we doing\nthis as humans, right? I feel that AI should be built\nby humanity for humanity, and let's not forget that. It shouldn't be by Moloch for Moloch, or what it really is now is\nkind of by humanity for Moloch, which doesn't make any sense. It's for us that we're doing it. And it would make a lot more sense if we build, develop,\nfigure out gradually, safely how to make all this tech, and then we think about\nwhat are the kind of jobs that people really don't want to have, you know, automate them all away. And then we ask what are the jobs that people really find meaning in, like maybe taking care of\nchildren in the daycare center, maybe doing art, et cetera, et cetera. And even if it were possible\nto automate that away, we don't need to do that, right? We built these machines. - Well it's possible that we redefine or rediscover what are the\njobs that give us meaning. So for me, the thing, it is really sad. Like I, (chuckles) half the time I'm excited,\nhalf the time I'm crying as I'm generating code because I kind of love programming. It's the act of creation, You have an idea, you design it, and then you bring it to life, and it does something. Especially if there's\nsome intelligence to it, it doesn't even have to have intelligence. Printing \"Hello world\" on screen. You made a little machine\nand it it comes to life. - [Max] Yeah. - And there's a bunch of\ntricks you learn along the way 'cause you've been doing\nit for many, many years. And then for to see AI be able to generate all the tricks you thought were special. I don't know, it's very, it's scary, it's almost painful. Like a loss of innocence maybe, like maybe when I was younger, I remember before I learned\nthat sugar is bad for you, you should be on a diet. I remember I enjoyed candy deeply, in a way I just can't anymore, that I know is bad for me. I enjoyed it unapologetically,\nfully, just intensely. And I lost that. Now, I feel like a little\nbit of that is lost, or being lost with programming, similar as it is for the 3D modeler no longer being able\nto really enjoy the art of modeling 3D things for gaming. I don't know what to make sense of that. Maybe I would rediscover\nthat the true magic of what it means to be humans is connecting with other humans, to have conversations like this, I don't know, to have sex, to eat food, to really intensify the value from conscious experiences, versus like creating other stuff. - You're pitching the rebranding again from Homo sapiens to Homo sentiens, the meaningful experiences. And just to a inject some\noptimism in this here, so we don't sound like it was a gloomers. You know, we can totally\nhave our cake and eat it. You hear a lot of totally bullshit claims that we can't afford having more teachers, have to cut the number of nurses, you know, that's just nonsense, obviously. With anything even quite far short of AGI, we can dramatically improve, grow the GDP, and produce this wealth\nof goods and services. It's very easy to create a world where everybody is better off than today. Including the richest people can be better off as well, right? It's not a zero sum game, you know, technology. Again, you can have two countries, like Sweden and Denmark had\nall these ridiculous wars century after century, and sometimes that Sweden\ngot a little better off 'cause it got a little bigger, and then Denmark got a\nlittle bit better off 'cause Sweden got a little bit smaller, but then technology came along and we both got just\ndramatically wealthier without taking away from anyone else, so it was just a total win for everyone. And AI can do that on steroids. if you can build safe AGI, if you can build superintelligence, basically all the limitations\nthat cause harm today can be completely eliminated. Right? It's a wonderful possibility. And this is not sci-fi, this is something which\nis clearly possible according to laws of physics, And we can talk about ways\nof making it safe also, but unfortunately that'll only happen if we steer in that direction, that's absolutely not the default outcome. That's why income\ninequality keeps going up. That's why the life expectancy in the US has been going down now, I think it's four years in a row. I just read a heartbreaking study from CDC about how something like\n1/3 of all the teenage girls in the US have been\nthinking about suicide. You know, like those are steps in totally the wrong direction and it's important to keep\nour eyes on the prize here that we can, we have the power now for the first time in the history of our species to harness artificial intelligence, to help us really flourish, and help bring out the\nbest in our humanity rather than the worst of it. To help us have really\nfulfilling experiences that feel truly meaningful. And you and I shouldn't sit here and dictate the future\ngenerations what they will be, let them figure it out. But let's give them a chance to live, and not foreclose all these\npossibilities for them, by just messing things up, right? - Well for that, we'll have to\nsolve the AI safety problem. It would be nice if we can linger on exploring that a little bit. So one interesting way to\nenter that discussion is, you tweeted, and Elon replied, you tweeted, \"Let's not just focus on whether GPT-4 will do more harm or\ngood on the job market, but also whether it's coding skills will hasten the arrival\nof superintelligence.\" That's something we've\nbeen talking about, right? So Elon proposed one\nthing in the reply saying, \"Maximum truth-seeking is my\nbest guess for AI safety.\" Can you maybe steel me on the case for, this objective function of truth and maybe make an argument\nagainst it in general, what are your different ideas to start approaching the\nsolution to AI safety? - I didn't see that reply actually. - [Lex] Oh, interesting. - But I really resonate with it because, AI is not evil. It caused people around the world to hate each other much more, but that's because we\nmade it in a certain way. It's a tool, we can use it for great\nthings and bad things, and we could just as well have AI systems, and this is part of my\nvision for success here. Truth-seeking AI that really\nbrings us together again, you know, why do people\nhate each other so much between countries and within countries is because they each\nhave totally different versions of the truth, right? If they all had the same truth that they trusted for good reason 'cause they could check it and verify it, and not have to believe in some self-proclaimed authority, right? They wouldn't be as nearly as much hate. There'd be a lot more\nunderstanding instead, and this is, I think something AI can\nhelp enormously with. For example, a little baby\nstep in this direction is this website called Metaculus where people bet and make\npredictions not for money, but just for their own reputation. And it's kind of funny actually, you treat the humans like you treat AI, as you have a loss function where they get penalized if they're super confident on something and then the opposite happens. - [Lex] Yeah. - Whereas if you're kind of humble, and then you're like, \"I think it's 51% chance\nthis is gonna happen,\" and then the other happens, you don't get penalized much, and what you can see is that some people are much better at predicting than others. They've earned your trust, right? One project that I'm working on right now is an outgrowth of Improve\nThe News foundation together with the Metaculus folks is, seeing if we can really\nscale this up a lot with more powerful AI. 'Cause I would love it, I would love for there to be like a really powerful\ntruth-seeking system where, that is trustworthy because it keeps being right about stuff. And people come to it and maybe look at its latest trust ranking of different pundits and\nnewspapers, et cetera. If they want to know why\nsome someone got a low score, they can click on it, and see all the predictions\nthat they actually made and how they turned out, you know, this is how we do it in science. You trust scientists like Einstein who said something everybody\nthought was bullshit, and turned out to be right, he get a lot of trust points, and he did it multiple times, even. I think AI has the power to really heal a lot of the rifts we're seeing\nby creating a trust system. It has to get away from this idea today with some fact checking site, which might themselves have an agenda and you just trust it\nbecause of its reputation, you want to have, so these sort of systems,\nthey earn in their trust and they're completely transparent. This I think would actually help a lot that can, I think, help heal the very\ndysfunctional conversation that humanity has about\nhow it's gonna deal with all its biggest challenges\nin in the world today. And then on the technical side, you know, another common\nsort of gloom comment I get from people are saying, \"We're just screwed, there's no hope.\" Is well, things like GPT-4 are way too complicated for a human to ever understand, and prove that they can be trustworthy. They're forgetting that AI can help us prove that things work, right? - [Lex] Yeah. - And there's this very\nfundamental fact that in math, it's much harder to come up with a proof than it is to verify that\nthe proof is correct. You can actually write a\nlittle proof-checking code, it's quite short, but you can as a human, understand, and then it can check the\nmost monstrously long proof ever generated even by your computer, and say, \"Yeah, this is valid.\" So right now, we have, this approach with virus-checking software that it looks to see if there's something, and if you should not trust it, and if it can prove to itself that you should not trust that\ncode, it warns you, right? What if you flip this around, and this is an idea I should give credit to Steve Omohundro for, so that it will only run\nthe code if it can prove, instead of not running it if it can prove that it's not trustworthy, it will only run it if it can prove that it's trustworthy. So it asks the code, \"Prove to me that you're gonna do what you say you're gonna do,\" and it gives you this proof, and you have a little proof\nthat you can check it. Now you can actually trust an AI that's much more intelligent\nthan you are, right? Because you, is its problem to come up with this proof that you could never have found, that you should trust it. - So this is the interesting point. I agree with you, but this is where Eliezer Yudkowsky might disagree with you. His claim, not with\nyou, but with this idea. his claim is superintelligent AI would be able to know how to\nlie to you with such a proof. - I have to lie to you and give me a proof that I'm gonna think is correct? - [Lex] Yeah. - But it's not me it's lying to you. That's to trick my proof checker, which is a piece of code. - So his general idea is\na superintelligent system can lie to a dumber proof checker. So you're going to have, as a system becomes more\nand more intelligent, there's going to be a threshold where a superintelligent system will be able to effectively lie to a slightly dumber AGI system. Like there's a, like he really focuses on this weak AGI to strong AGI jump, where the strong AGI can\nmake all the weak AGIs think that it's just one of them, but it's no longer that. And that leap is when it runs away. - Yeah, I don't buy that argument. I think no matter how\nsuperintelligent an AI is, it's never gonna be able to prove to me that there are only finitely\nmany primes, for example. (Lex chuckling) It just can't. And it can try to snow me by making up all sorts of\nnew weird rules of deduction, and say, \"Trust me, you know, the way your proof checker\nwork is too limited, and we have this new\nhyper math and it's true.\" But then I would just take the attitude, okay, I'm gonna forfeit some of these, the supposedly super cool technologies, I'm only gonna go with the ones that I can prove in my\nown trusted proof checker. Then I think it's fine. There's still, of course, this is not something anyone has successfully\nimplemented at this point, but I think it, I just give it as an example of hope, we don't have to do all\nthe work ourselves, right? This is exactly the sort of\nvery boring and tedious task that is perfect to outsource to an AI. And this is a way in which less powerful and less intelligent agents like us can actually continue to control and trust more powerful ones. - So build AGI systems that help us defend against other AGI systems. - Well for starters, begin with a simple\nproblem of just making sure that the system that you own or that's supposed to be loyal to you has to prove to itself\nthat it's always gonna do the things that you actually\nwant it to do, right? And if it can't prove it, maybe it's still gonna do it, but you won't run it. So you just forfeit some aspects of all the cool things AI can do. I bet your dollars to donuts, it can still do some\nincredibly cool stuff for you. - [Lex] Yeah. - There are other things too, that we shouldn't sweep under the rug. Like not every human agrees on exactly what direction we should\ngo with humanity, right? - Yes. - And you've talked a lot\nabout geopolitical things on your podcast to this effect, you know, but, I think that shouldn't\ndistract us from the fact that there are actually a lot of things that everybody in the\nworld virtually agrees on. That \"Hey, you know, like having a no humans on\nthe planet in a near future, nah, let's not do that\" right? You looked at something like the United Nations\nSustainable Development Goals. Some of 'em were quite a ambitious, and basically all the countries agree, US, China, Russia,\nUkraine, they all agree. So instead of quibbling\nabout the little things that we don't agree on, let's start with the things we do agree on and get them done. Instead of being so distracted by all these things we disagree on, that Moloch wins because frankly, Moloch going wild now, it feels like a war on life playing out in front of our eyes, if you just look at it\nfrom space, you know, we're on this planet, beautiful, vibrant ecosystem, now we start chopping\ndown big parts of it, even though nobody, most people thought that was a bad idea. Oh, we start doing ocean acidification, wiping out all sorts of species, oh, now we have all these close calls, we almost had a nuclear war, and we're replacing more\nand more of the biosphere with non-living things. We're also replacing in our social lives, a lot of the things which\nwere so valuable to humanity, a lot of social interactions now are replaced by people staring\ninto their rectangles, right? And I'm not a psychologist,\nI'm out of my depth here, but I suspect that part of\nthe reason why teen suicide and suicide in general in the US that record-breaking level\nis actually caused by, again, AI technologies and social media making people spend less time with actually just human interaction. We've all seen a bunch\nof good-looking people in restaurants staring into the rectangles instead of looking into\neach other's eyes, right? So that's also part of the war in life that we are replacing so many really life-affirming\nthings by technology. We're putting technology between us, that the technology that\nwas supposed to connect us is actually distancing us\nourselves from each other. And then we are giving\never more power to things which are not alive. These large corporations are\nnot living things, right? They're just maximizing profit. I wanna win the war on life. I think we humans, together with all our fellow\nliving things on this planet will be better off if\nwe can remain in control over the non-living things and make sure that they work for us. I really think it can be done. - Can you just linger\non this maybe high level of philosophical disagreement\nwith Eliezer Yudkowsky, in the hope you're stating. So he is very sure, he puts a very high probability, very close to one, depending on the day he puts it at one, that AI is going to kill humans. That there's just, he does not see a trajectory, which it doesn't end up\nwith that conclusion. What trajectory do you see\nthat doesn't end up there? And maybe can you see\nthe point he's making, and can you also see a way out? - First of all, I tremendously respect Eliezer Yudkowsky and his thinking. Second, I do share his view that there's a pretty large chance that we're not gonna make it as humans. There won't be any humans on the planet, in a not-too-distant future, and that makes me very sad. You know, we just had a little baby and I keep asking myself, you know, is, how old is he even gonna get, you know? And I ask myself, it feels, I said to my wife recently, it feels a little bit\nlike I was just diagnosed with some sort of cancer, which has some, you know, risk of dying from and some\nrisk of surviving, you know. Except this is a kind of cancer which can kill all of humanity. So I completely take\nseriously his concerns, I think, but absolutely, I don't\nthink it's hopeless. I think there is, first of all a lot of momentum now for the first time actually, since the many, many\nyears that have passed since I and many others\nstarted warning about this, I feel most people are getting it now. I was just talking to this guy in the gas station near our house the other day. And he's like, \"I think\nwe're getting replaced, and then I think...\" So that's positive that they're finally, we're finally seeing this reaction, which is the first step\ntowards solving the problem. Second, I really think that this vision of only running AIs, if the stakes are really high, they can prove to us that they're safe. It's really just virus\nchecking in reverse again, I think it's scientifically doable. I don't think it's hopeless, we might have to forfeit some of the technology that we could get if we were putting blind faith in our AIs, but we're still gonna get amazing stuff. - Do you envision a process\nwith a proof checker? Like something like GPT-4, GPT-5, will go through a process\nof rigorous interrogation? - No I think it's hopeless, That's like trying to\nproof-verify spaghetti. - [Lex] (laughs) Okay. - What I think, the vision I have for\nsuccess is instead that, you know, just like we human beings were able to look at our brains and distill out the key knowledge. Galileo, when his dad threw\nhim an apple when he was a kid, he was able to catch it\n'cause his brain could, in his funny spaghetti kind of way, you know, predict how\nparabolas are gonna move, his Kahneman System 1, right? But then he got older and he's like, \"Wait, this is a parabola. It's y equals x squared.\" I can distill this knowledge out and today you can easily\nprogram it into a computer and it can simulate not just that, but how to get to Mars and so on, right? I envision a similar process where we use the amazing\nlearning power of neural networks to discover the knowledge\nin the first place, but we don't stop with a\nblack box and use that. We then do a second round of AI where we use automated systems to extract out the knowledge,\nand see what is it, what are the insights it's had, okay? And then we put that knowledge into a completely different\nkind of architecture, or programming language or whatever, that's made in a way that it\ncan be both really efficient, and also is more amenable\nto very formal verification. That's my vision. I'm not sitting here saying, I'm confident 100% sure that\nit's gonna work, you know. But I don't think it's a chance, it's certainly not zero either, and it will certainly be possible to do for a lot of really cool AI applications that we're not using now. So we can have a lot of the\nfun that we're excited about if we do this. We are gonna need a little bit of time. And that's why it's good to pause and put in place requirements. One more thing also, I think, you know, someone might think, \"Well, 0% chance we're gonna survive, let's just give up,\" right? That's very dangerous, because there's no more\nguaranteed way to fail than to convince yourself\nthat it's impossible and not try, you know, when you study\nhistory and military history, the first thing you learn is that, that's how you do psychological warfare. You persuade the other\nside that it's hopeless so they don't even fight. And then of course you win, right? Let's not do this psychological\nwarfare on ourselves and say there's 100% percent probability we're all screwed anyway. And sadly, I do get that a little bit, sometimes from actually some young people who are like so convinced\nthat we're all screwed, that they're like, \"I'm just gonna play\ncomputer games and do drugs, 'cause we're screwed anyway, right?\" It's important to keep the hope alive because it actually has a causal impact, and makes it more likely\nthat we're gonna succeed. - It seems like the people that actually build solutions to the problem, seemingly impossible to solve problems are the ones that believe. - [Max] Yeah. - They're the ones who are the optimists. And it's like, it seems like there's some\nfundamental law to the universe where \"Fake it till you\nmake it,\" kind of works. Like believe it's possible\nand it becomes possible. - Yeah, was it Henry Ford who said that, if you tell yourself that\nit's impossible, it is. So let's not make that mistake. And this is a big mistake\nsociety is making, I think all in all, everybody's so gloomy, and the media also very biased towards if it bleeds, it leads, and gloom and doom, right? So most, visions of the future\nwe have are dystopian, which really demotivates people. We wanna really, really, really focus on the upside also to give people the willingness to fight for it. And for AI, you and I mostly talked\nabout gloom here again, but let's not forget that, you know, we have probably both lost someone we really cared about to some disease that we were told was incurable. Well it's not, there's no law of physics\nsaying we had to die of that cancer or whatever. Of course, you can cure it. And there's so many other things that we, with our human intelligence\nhave also failed to solve on this planet, which AI could also very\nmuch help us with, right? So if we can get this right, and just be a little more chill, and slow down a little\nbit so we get it right. It's mind-blowing how awesome\nour future can be, right? We talked a lot about stuff on Earth, it can be great, but even if you really get ambitious and look up into the skies, right? There's no reason we have\nto be stuck on this planet for the rest of the remaining, for billions of years to come. We totally understand\nnow that laws of physics let life spread out into\nspace to other solar systems, to other galaxies, and flourish for billions\nand billions of years. And this to me is a\nvery, very hopeful vision that really motivates me to fight. And coming back to it in the end, it's something you talked about again, you know, the struggle, how the human struggle\nis one of the things that's also really gives\nmeaning to our lives. If there's ever been an\nepic struggle, this is it. And isn't it even more epic\nif you're the underdog? If most people are telling\nyou this is gonna fail, it's impossible, right? And you persist and you succeed, right? And that's what we can do\ntogether as a species on this one. A lot of pundits are\nready to count this out. - Both in the battle to keep AI safe and becoming a multi-planetary species. - Yeah, and they're the same challenge. If we can keep AI safe, that's how we're gonna get\nmulti-planetary very efficiently. - I have some sort of technical questions about how to get it right. So one idea that I'm not even sure what the right answer is to is, should systems like GPT-4 be open sourced in whole or in part? Can you see the case for either? - I think the answer right now is no. I think the answer early on was yes. So we could bring in all the wonderful great thought process\nof everybody on this, but asking should we open source GPT-4 now is just the same as if you say, should we open source how to build really small nuclear weapons? Should we open source\nhow to make bioweapons? Should we open source\nhow to make a new virus that kills 90% of everybody who gets it? Of course we shouldn't. - So it's already that powerful. It's already that powerful\nthat we have to respect the power of the systems we've built. - The knowledge that you get from open sourcing everything we do now might very well be powerful enough that people looking at that can use it to build the things that are really threatening. Again, let's get it, remember OpenAI's GPT-4 is a baby AI, sort of baby, proto, almost little bit AGI, according to what Microsoft's\nrecent paper said, right? It's not that that we're scared of, what we're scared about is\npeople taking that who are, who might be a lot less responsible than the company that made it, right? And just go into town with it. That's why we wanna, it's an information hazard. There are many things which, yeah, are not open-sourced\nright now in society for very good reason. Like how do you make certain\nkind of very powerful toxins out of stuff you can buy in Home Depot? We don't open source\nthose things for a reason, and this is really no different. - [Lex] So- - And I'm saying that, I have to say it feels a bit weird, in a way, a bit weird to say it because MIT is like the cradle\nof the open source movement. And I love open source in general, power to the people, I say, but there's always gonna be some stuff that you don't open source, and you know, it's just\nlike you don't open source, so we have a three-month old baby, right? When he gets a little bit older, we're not gonna open source to him all the most dangerous things\nhe can do in the house, right? - But it does, it's a weird feeling because this is one of the\nfirst moments in history where there's a strong case to be made not to open source software. This is when the software\nhas become too dangerous. - Yeah, but it's not the first time that we didn't wanna\nopen source a technology. - Technology, yeah. Is there something to be said about how to get the release\nof such systems right, like GPT-4 and GPT-5? So OpenAI went through\na pretty rigorous effort for several months, you could say it could be longer, but nevertheless it's longer\nthan you would've expected of trying to test the system to see like what are the ways goes wrong to make it very difficult, well, somewhat difficult\nfor people to ask things, \"How do I make a bomb for $1?\" Or \"How do I say I hate a\ncertain group on Twitter in a way that doesn't get\nme blocked from Twitter, banned from Twitter.\" Those kinds of questions. So you basically use\nthe system to do harm. - [Max] Yeah. - Is there something you could say about ideas you have that's just, on looking having thought about\nthis problem of AI safety, how to release a system, how to test such systems when you have them inside the company. - Yeah, so a lot of people say that the two biggest risks\nfrom large language models are, it's spreading disinformation, harmful information of various types, and second being used for\noffensive cyberweapon. I think those are not\nthe two greatest threats. They're very serious threats, and it's wonderful that people\nare trying to mitigate them. A much bigger elephant in the room is how this is gonna disrupt our economy in a huge way, obviously, and maybe take away a lot\nof the most meaningful jobs. And an even bigger one is\nthe one we spent so much time talking about here that this becomes the bootloader\nfor the more powerful AI. - Write code, connected to the\ninternet, manipulate humans. - Yeah, and before we know\nit, we have something else, which is not at all a large language model that looks nothing like it, but which is way more intelligent and capable and has goals. And that's the elephant in the room. And obviously no matter how hard any of these companies have tried, that's not something that's easy for them to verify with large language models. And the only way to really\nlower that risk a lot would be to not let, for example, never let it read any code, not train on that, and not put it into an API, and to not Give it access\nto so much information about how to manipulate humans, so, but that doesn't mean you still can't make a ton of money on them, you know? We're gonna just watch now\nthis coming year, right? Microsoft is rolling\nout the new Office Suite where you go into Microsoft Word, and give it a prompt, and it write the whole text for you and then you edit it and then you're like, \"Oh, gimme a PowerPoint version of this,\" and it makes it. \"And now take the\nspreadsheet and blah blah.\" And you know, all of those things I think are, you can debate the economic impact of it and whether society is prepared to deal with this disruption. But those are not the things which, that's not the elephant of the room that keeps me awake at night\nfor wiping out humanity. And I think that's the biggest\nmisunderstanding we have. A lot of people think that we're scared of like automatic spreadsheets. That's not the case. That's not what Eliezer was\nfreaked out about either. - Is there in terms of\nthe actual mechanism of how AI might kill all humans. So something you've been outspoken about, you've talked about a lot. Is it autonomous weapon systems? So the use of AI in war, is that one of the things that's still you carry concern for as these systems become\nmore and more powerful? - I carry a concern for it, not that all humans are gonna\nget killed by slaughter bots, but rather just as express route into an Orwellian dystopia where it becomes much easier for very few to kill very many, and therefore it becomes very easy for very few to dominate very many, right? AI, if you wanna know how\nAI could kill all people, just ask yourself, we humans have driven a\nlot of species extinct. How do we do it? You know, we were smarter than them, usually we didn't do\nit even systematically by going around one-on-one, one after the other and stepping on them, or shooting them or anything like that. We just like chopped down their habitat 'cause we needed it for something else. In some cases we did it by putting more carbon dioxide in the atmosphere because of some reason that those animals didn't even understand, and now they're gone, right? So if you're an AI, and you just wanna figure something out, then you decide, you know, we just really need this space here to build more compute facilities. You know, if that's the\nonly goal it has, you know, we are just the sort of\naccidental roadkill along the way. And you could totally imagine, \"Yeah, maybe this oxygen\nis kind of annoying 'cause it cause more corrosion, so let's get rid of the oxygen.\" And good luck surviving after that. You know, I'm not particularly concerned that they would want to kill us just because that would\nbe like a goal in itself. you know, when we.. we've driven a number of the elephant species extinct. Right? It wasn't 'cause we didn't like elephants. The basic problem is you\njust don't want to give, you don't wanna cede\ncontrol over your planet to some other more intelligent entity that doesn't share your goals. It's that simple, and so, which brings us to another key challenge which AI safety research has been grappling with for a long time. Like, how do you make AI, first of all, understand our goals and then adopt our goals, and then retain them as\nthey get smarter, right? All three of those are really hard, right? Like a human child, first, they're just not smart enough to understand our goals. They can't even talk. And then eventually they're teenagers, and understand our goals just fine, but they don't share. (laughs) - [Lex] Yeah. - But there is fortunately\na magic phase in the middle where they're smart enough\nto understand our goals and malleable enough\nthat we can hopefully, with good parenting, teach\nthem right from wrong and instill good goals in them, right? So those are all tough\nchallenges with computers. And then, you know, even if you teach your kids\ngood goals when they're little, they might outgrow them too, and that's a challenge for\nmachines to keep improving. So these are a lot of hard,\nhard challenges we're up for, but I don't think any of\nthem are insurmountable. The fundamental reason why Eliezer looked so depressed when I last saw him was because he felt there\njust wasn't enough time. - Oh, that not that it was unsolvable, - Correct. - There's just not enough time. - He was hoping that humanity was gonna take this threat more seriously, so we would have more time, and now we don't have more time. That's why the open letter\nis calling for more time. - But even with time, the AI alignment problem, it seems to be really difficult. - Oh yeah. But it's also the most worthy problem, the most important problem\nfor humanity to ever solve. Because if we solve that one, Lex, that aligned AI can help us\nsolve all the other problems. - 'Cause it seems like it has to have constant\nhumility about its goal, constantly question the goal. Because as you optimize\ntowards a particular goal and you start to achieve it, that's when you have the\nunintended consequences, all the things you mentioned about. So how do you enforce and\ncode a constant humility as your ability become better, and better, and better, and better? - Professor Stuart Russell at Berkeley is also one of the driving\nforces behind this letter, he has a whole research\nprogram about this. I think of it as a AI humility, exactly. Although he calls it inverse\nreinforcement learning and other nerdy terms. But it's about exactly that. Instead of telling the AI, \"Here's this goal, go optimize the the bejesus out of it.\" You tell it, \"Okay, do\nwhat I want you to do, but I'm not gonna tell\nyou right now what it is I want you to do. You need to figure it out.\" So then you give the\nincentives to be very humble and keep asking you\nquestions along the way. Is this what you really meant? Is this what you wanted? And oh the other thing\nI tried didn't work, and seemed like it didn't work out right. Should I try it differently? What's nice about this is it's not just\nphilosophical mumbo-jumbo, it's theorems and technical\nwork that with more time, I think it can make a lot of progress, and there are a lot of\nbrilliant people now working on AI safety. We just need to give em a bit more time. - But also not that many\nrelative to skill of the prompt. - No, exactly. There should be at least this, just like every university worth its name has some cancer research going on in its biology department, right? Every university that\ndoes computer science should have a real effort in this area and it's nowhere near that. This is something I hope is changing now, thanks to the GPT-4, right? So I think if there's a silver lining to what's happening here, even though I think many people would wish it would've been rolled\nout more carefully, is that this might be the wake-up call that humanity needed, to really stop fantasizing about this being a hundred years off and stop fantasizing about this being completely\ncontrollable and predictable because it's so obvious, it's not predictable, you know? why is it that, I think it was ChatGPT that\ntried to persuade a journalist to divorce his wife, you know. It was not 'cause the\nengineers had built it, was like, (laughs mischievously) \"Let's put this in here, and screw a little bit with people.\" They hadn't predicted it at all. They built the giant black box trained to predict the next word and got all these emergent properties, and oops, it did this, you know. I think this is a very\npowerful wake-up call and anyone watching this who's not scared, I would encourage them to just play a bit more with these tools. They're out there now like GPT-4 and, so wake-up call is first step, once you've woken up, then gotta slow down a\nlittle bit the risky stuff to give a chance to\neveryone that has woken up to catch up with this on the safety front. - You know what's\ninteresting is, you know, MIT, that's computer science, but in general, but let's just even say\ncomputer science curriculum. How does the computer science\ncurriculum change now? You mentioned programming. - [Max] Yeah. - Like why would you be, when I was coming up, programming as a prestigious position. Like why would you be\ndedicating crazy amounts of time to become an excellent programmer? Like the nature of programming\nis fundamentally changing. - The nature of our\nentire education system is completely turned on its head. - Has anyone been able\nto like, load that in, and like think, because\nit's really turning, - I mean some English professors, some English teachers are\nbeginning to really freak out now. Right? Like they give an essay assignment and they get back all\nthis fantastic prose, like this is style of Hemmingway, and then they realize they\nhave to completely rethink and even, you know, just\nlike we stopped teaching, writing a script, is that what you say in English? - [Lex] Yeah, handwritten, yeah. - Yeah, when everybody started typing, you know, like so much of\nwhat we teach our kids today. - Yeah, I mean that's, everything is changing and\nit is changing very quickly. And so much of us understanding how to deal with the big\nproblems of the world is through the education system. And if the education system\nis being turned on its head, then what's next? It feels like having these\nkinds of conversations is essential to trying to figure it out. And everything's happening so rapidly. I don't think there's even, you're speaking of safety, the broad AI safety defined, I don't think most universities\nhave courses on AI safety. It's like a philosophy seminar. - Yeah, and like I'm an educator myself, so it pains me to say this, but I feel our education right now is completely obsoleted\nby what's happening. You know, you put a kid into first grade, and then you are envisioning like, and then they're gonna come out of high school 12 years later, and you've already pre-planned now what they're gonna learn, when you're not even sure if there's gonna be any\nworld left to come out to, like clearly you need to have a much more opportunistic education system that keeps adapting itself very rapidly as society re-adapts. The skills that were really useful when the curriculum was written, I mean how many of those skills are gonna get you a job in 12 years? I mean, seriously. - If we just linger on the\nGPT-4 system a little bit, you kind of hinted at it, especially talking about the importance of consciousness in in the\nhuman mind with Homo sentiens. Do you think GPT-4 is conscious? - Ah, I love this question. So let's define consciousness first because in my experience, like 90% of all arguments\nabout consciousness, (Lex chuckles) boil down to the two people arguing having totally different\ndefinitions of what it is, then they're just\nshouting past each other. I define consciousness\nas subjective experience. Right now I'm experiencing\ncolors and sounds, and emotions, you know, but does a self-driving\ncar experience anything? That's the question about whether it's conscious or not, right? Other people think you should define\nconsciousness differently, fine by me, but then maybe use a\ndifferent word for it. Or they can, I'm gonna use consciousness\nfor this at least, so, but if people hate the, yeah. So is GPT-4 conscious? Does GPT-4 have subjective experience? Short answer, I don't know, because we still don't know what it is that gives this wonderful\nsubjective experience that is kind of the\nmeaning of our life, right? Because meaning itself, the feeling of meaning is\na subjective experience. Joy is a subjective experience, love is a subjective experience, we don't know what it is, I've written some papers about this, a lot of people have. Giulio Tononi, a professor, has stuck his neck out the farthest and written down actually very\nbold mathematical conjecture for what's the essence of\nconscious information processing. He might be wrong, he might be right, but we should test it. He postulates that the consciousness has to do with loops in\nthe information processing. So our brain has loops. Information can go round and round, in computer science nerd-speak, you call it a recurrent neural network where some of the output\ngets fed back in again. And with his mathematical formulism, if it's a feed-forward neural network where information only\ngoes in one direction, like from your eye retina\ninto the back of your brain for example, that's not conscious. So he would predict\nthat your retina itself isn't conscious of anything, or a video camera. Now the interesting thing about GPT-4 is it's also just a one-way\nflow of information. So if Tononi is right, then GPT-4 is a very intelligent zombie, that can do all this smart stuff but isn't experiencing anything. And this is both a relief if it's true, and that you don't have to feel guilty about turning off GPT-4\nand wiping its memory whenever a new user comes along. I wouldn't like if someone did that to me, and neuralyze me like in \"Men In Black.\" But it's also creepy, that you can have a very high intelligence perhaps that is not conscious, because if we get replaced by machines, and while it's sad enough that\nhumanity isn't here anymore, 'cause I kind of like humanity, but at least if the\nmachines were conscious, I could be like, \"Well, but\nthey are our descendants and maybe they have our values\nand they are our children.\" But if Tononi is right and these are all transformers that are, not in the sense of Hollywood, but in the sense of these one-way\ndirection neural networks, so they're all the zombies, that's the ultimate zombie apocalypse now. We have this universe that goes on with great construction\nprojects and stuff, but there's no one experiencing anything. That would be like the\nultimate depressing future. So I actually think, as we move forward with\nbuilding more advanced AI, we should do more research on figuring out what kind of information processing actually it has experienced, because I think that's\nwhat it's all about. And I completely don't buy the dismissal that some people will say, \"Well this is all bullshit because consciousness\nequals intelligence.\" - [Lex] Right. - That's obviously not true. You can have a lot of conscious experience when you're not really\naccomplishing any goals at all. You're just reflecting on something, and you can sometimes, doing things that require intelligence probably without being conscious. - But I also worry that we humans, will discriminate against AI systems that clearly exhibit consciousness. That we will not allow AI\nsystems to have consciousness. We'll come up with theories\nabout measuring consciousness that will say this is a lesser being, and this was like, I worry about that because maybe, we humans will create something that is better than us humans, in the way that we find beautiful, which is they have a deeper subjective experience of reality. Not only are they smarter,\nbut they feel deeper. And we humans will hate them for it. As human history is shown, they'll be the \"other,\" we'll try to suppress it, they'll create conflict, they'll create war, all of this. I worry about this too. - Are you saying that we humans sometimes come up with\nself-serving arguments? No, we would never do that, would we? - Well that's the danger here is, even in this early stages, we might create something beautiful. And we'll erase its memory. - I was horrified as a kid when someone started boiling lobsters. I'm like, \"Oh my God, that's so cruel.\" And some grownup there\nback in Sweden said, \"Oh, it doesn't feel pain.\" I'm like, \"How do you know that?\" \"Oh, a scientist have shown that.\" And then there was a recent study where they show that lobsters\nactually do feel pain when you boil them. So they banned lobster\nboiling in Switzerland now. You have to kill them in\na different way first. Presumably, a scientific\nresearch boiled down to someone asked the\nlobster, \"Does it hurt?\" (both laughing) - Survey, self-report. - And we do the same thing with cruelty to farm animals also, all these self-serving\narguments for why they're fine. And yeah, so we should certainly, what I think step one is just be humble, and acknowledge that consciousness is not the same thing as intelligence. And I believe that consciousness still is a form of information processing where it's really information being aware of itself in a certain way, and let's study it and give\nourselves a little bit of time, and I think we will be able to figure out actually what it is that\ncauses consciousness. And then we can make\nprobably unconscious robots that do the boring jobs that we would feel immoral\nto give the machines. But if you have a companion robot taking care of your mom\nor something like that, she would probably want\nit to be conscious, right? So the emotions it seems\nto display aren't fake. All these things can be done in a good way if we give ourselves a little bit of time, and don't run, and take on this challenge. - Is there something you\ncould say to the timeline that you think about, about the development of AGI? Depending on the day, I'm sure that changes for you, but when do you think there would be a really\nbig leap in intelligence where you would definitively\nsay we have built AGI? Do you think it's one year from now, five years from now, 10, 20, 50? What's your gut say? - Honestly, for the past decade, I've deliberately given\nvery long timelines because I didn't want to fuel some kind of stupid Moloch race. - [Lex] Yeah. - But I think that cat has\nreally left the bag now. I think we might be very, very close. I don't think the Microsoft\npaper is totally off when they say that there\nare some glimmers of AGI. It's not AGI yet, it's not an agent, there's a lot of things they can't do. But I wouldn't bet very strongly against it happening very soon, that's why we decided\nto do this open letter. Because you know, if there's ever been a\ntime to pause, you know, it's today. - There's a feeling like this GPT-4 is a big transition\ninto waking everybody up to the effectiveness of these systems. And so the next version will be big. - Yeah, and if that next one isn't AGI, maybe the next next one will. And there are many companies\ntrying to do these things and the basic architecture of 'em is not some sort of\nsuper well-kept secret. So this is a time to... A lot of people have said for many years that there will come a time when we want to pause a little bit, that time is now. - You have spoken about and thought about nuclear war a lot. Over the past year, we\nseemingly have come closest to the precipice of nuclear war than, at least in my lifetime. - [Max] Mhm, yeah. - What do you learn about\nhuman nature from that? - It's our old friend Moloch again. It is really scary to see it where, America doesn't want\nthere to be a nuclear war. Russia doesn't want there to\nbe a global nuclear war either. We both know that it's just be another, if we just try to do it, if both sides try to launch first, it's just another suicide race, right? So why are we, why is it the way you said, that this is the closest\nwe've come since 1962? In fact, I think we've come closer now than even the Cuban Missile Crisis. It's 'cause of Moloch, You know, you have these other forces. On one hand you have the West saying that we have to\ndrive Russia out of Ukraine, it's a matter of pride. And we've staked so much on it that it would be seen as a huge loss of the credibility of the West if we don't drive Russia\nout entirely of the Ukraine. And on the other hand,\nyou have Russia who has, and you have the Russian leadership who knows that if they get\ncompletely driven out of Ukraine, you know, it might, it's not just gonna be\nvery humiliating for them, but they might, it often happens when countries lose wars that the things don't go so well for their leadership either. Like, you remember when Argentina invaded the Falkland Islands? The military junta that\nordered that, right? People are cheering on\nthe streets at first when they took it, and then when they got their\nbutt kicked by the British, you know what happened to those guys? They were out. And I believe those who are still alive are in jail now, right? So you know, the Russian\nleadership is entirely cornered where they know that just\ngetting driven out of Ukraine is not an option, and, so this to me, is a\ntypical example of Moloch. You have these incentives\nof the two parties where both of them are\njust driven to escalate more and more, right? If Russia starts losing in\nthe conventional warfare, the only thing they cam do since their back's against the wall, is to keep escalating. And the West has put\nitself in the situation now where we're sort of already\ncommitted to drive Russia out. So the only option the West has, is to call Russia's bluff and\nkeep sending in more weapons. This really bothers me because Moloch can sometimes drive competing parties to do something which is ultimately just really bad for both of them. And you know, what makes me even more\nworried is not just that I, it's difficult to see an ending, a quick peaceful ending to this tragedy that doesn't involve\nsome horrible escalation, but also that we\nunderstand more clearly now just how horrible it would be. There was an amazing\npaper that was published in Naturefood this August, by some of the top researchers who've been studying nuclear\nwinter for a long time, and what they basically did was they combined climate models with food and agricultural models, so instead of just saying, \"Yeah, you know, it gets\nreally cold, blah blah blah,\" they figured out actually\nhow many people would die in different countries. And it's pretty mind-blowing, you know? So basically what happens, you know, is that the thing that\nkills the most people is not the explosions,\nit's not the radioactivity, it's not the EMP mayhem, it's not the rampaging mobs foraging food, no, it's the fact that\nyou get so much smoke coming up from the burning\ncities into the stratosphere that it spreads around the\nEarth from the jetstreams. So in typical models you\nget like 10 years or so where it's just crazy cold during the first year after the war, and in their models, the temperature drops in Nebraska and in the Ukraine bread baskets, you know, by like 20 Celsius or so, if I remember. No yeah, 20, 30 Celsius depending on where you are. 40 Celsius in some places, which is, you know, 40 Fahrenheit to 80 Fahrenheit colder than\nwhat it would it normally be. So, you know, I'm not good at farming but, (Lex laughing) if it's snowing, if it drops below freezing pretty much on most days in July and then like, that's not good. So they worked out, they put this into their farming models and what they found\nwas really interesting. The countries that get the most hard hit are the ones in the northern hemisphere. So in the US, in one model they had, they had about 99% of all\nAmericans starving to death, in Russia, and China, and Europe, also about 99%, 98% starving to death. So you might be like, \"Oh, it's kind of poetic justice that both the Russians and the Americans, 99% of them have to pay for it, 'cause it was their bombs that did it.\" But you know, that doesn't particularly\ncheer people up in Sweden or other random countries that have nothing to do with it, right? And it, I think it hasn't entered the mainstream, not understanding very much\njust like how bad this is. Most people, especially a lot of people in decision-making positions still think of nuclear weapons as something that makes you powerful, scary but powerful. They don't think of it as something where, \"Yeah, just to within a percent or two, you know, we're all just\ngonna starve to death and- - And starving to death is, the worst way to die. As Holodomor, as all the\nfamines in history show the torture involved in that. - Probably brings out\nthe worst in people also. When people are desperate\nlike this, it's not, so some people, I've have heard some people say that if that's what's gonna happen, they'd rather be at ground zero and just get vaporized, you know? But I think people\nunderestimate the risk of this because they aren't afraid of Moloch. They think, \"Oh, it's just gonna be, 'cause humans don't want this, so it's not gonna happen.\" That's the whole point of Moloch. That things happen that nobody wanted. - And that applies to nuclear weapons, and that applies to AGI. - Exactly. And it applies\nto some of the things that people have gotten most upset with capitalism for also, right? Where everybody was just\nkind of trapped, you know. It's not that if some\ncompany does something that causes a lot of harm, not that the CEO is a bad person, but she or he knew that, you know, that all the other companies\nwere doing this too. So Moloch is, is a formidable foe, I wish someone would make good movies so we can see who the real enemy is, so we don't, 'cause we're not fighting\nagainst each other, Moloch makes us fight against each other. That's what Moloch's superpower is. The hope here is any kind of technology or the mechanism that\nlets us instead realize that we're fighting\nthe wrong enemy, right? - It's such a fascinating battle. - It's not us versus them, it's us versus it, yeah. - Yeah, we are fighting\nMoloch for human survival. We as a civilization. - Have you seen the\nmovie \"Needful Things\"? It's a Stephen King novel. I love Stephen King, and Max von Sydow, a Swedish actor, is playing the guy. It's brilliant, I just thought, I hadn't\nthought about that until now, but that's the closest I've\nseen to a movie about Moloch. I don't wanna spoil the film for anyone who wants to watch it. But basically, it's about\nthis guy who turns out to, you can interpret him as\nthe devil or whatever, but he doesn't actually ever\ngo around and kill people or torture people, or go\nburning coal or anything. He makes everybody fight each other, makes everybody fear each other, hate each other, and then kill each other. So that's the movie\nabout Moloch, you know. - Love is the answer, that seems to be, one of the ways to fight\nMoloch is by compassion, by seeing the common humanity. - Yes, yes. And to not sound, so we don't sound like a bunch of Kumbaya tree\nhuggers here, right? (Lex laughing) We're not just saying\n\"Love and peace, man.\" We're trying to actually help people understand the true facts\nabout the other side, and feel the compassion because, it's that truth makes you\nmore compassionate, right? So that's why I really like using AI for truth and for\ntruth-seeking technologies. that can as a result, you know, will get us more love than hate. And even if you can't get love, you know, let's settle for some understanding which already gives compassion. If someone is like, you know, \"I really disagree with you Lex, but I can see where you're coming from. You're not a bad person\nwho needs to be destroyed, but I disagree with you and I'm happy to have an\nargument about it,\" you know? That's a lot of progress compared to where we are at\n2023 in the public space, wouldn't you say? - If we solve the AI safety problem, as we've talked about, and then you, Max Tegmark, who has been talking\nabout this for many years, get to sit down with the AGI, with the early AGI system\non a beach with a drink, (Max chuckles) What would you ask her? What kind of question would you ask? What would you talk about? Something so much smarter than you, would you be afraid?\n- I knew you were gonna get me with a really zinger of a question. That's a good one. - Would you be afraid\nto ask some questions? - No, I'm not afraid of the truth. (Lex laughing) I'm very humble. I know I'm just a meat bag\nwith all these flaws, you know? But yeah, I mean, we talked a lot\nabout the Homo sentiens, I've really already tried that\nfor a long time with myself. And that is what's really valuable about being alive for me, is that I have these\nmeaningful experiences. It's not that I'm good at this, or\ngood at that or whatever. There's so much I suck at, and... - So you're not afraid for the system to show you just how dumb you are. - No, no. In fact, my son reminds me of that pretty frequently. (laughs) - You could find out how dumb\nyou are in terms of physics, how little we humans understand. - I'm cool with that. I think, so I can't waffle my way\nout of this question, it's a fair one and it's tough. I think, given that I'm a\nreally, really curious person, that's really the\ndefining part of who I am, I'm so curious. I have some physics questions. (Lex laughing) I love to understand. I have some questions about consciousness, about the nature of reality, I would just really, really\nlove to understand also. I can tell you one for example, that I've been obsessing\nabout a lot recently. So I believe that, so suppose Tononi is right. and suppose there are some\ninformation processing systems that are conscious and some that are not. Suppose you can even make\nreasonably smart things like GPT-4 that are not conscious, but you can also make them conscious. Here is the question that\nkeeps me awake at night. Is it the case that the\nunconscious zombie systems that are really intelligent\nare also really efficient? Sorry, really inefficient? So that when you try to\nmake things more efficient, we will naturally be a pressure to do, they become conscious. I'm kind of hoping that\nthat's correct, and I, do you want me to give you, you can hand-wave the argument for it?\n- Yes, please. - You know like, In my lab again, every time we look at how these large language\nmodels do something, we see that they do them\nin really dumb ways, and you could make it make it better. If you, we have loops in our computer\nlanguage for a reason, the code would get way, way longer if you weren't allowed to use them, right? It's more efficient to have the loops and in order to have self-reflection whether it's conscious or not, right? Even an operating system knows\nthings about itself, right? You need to have loops already, right? So I think this is, I'm waving my hands a lot, but I suspect that, the most efficient way of implementing a given level of intelligence, has loops in it, the self-reflection, and will be conscious. - Isn't that great news? - Yes, if it's true, it's wonderful. 'Cause then we don't have to fear the ultimate zombie apocalypse. And I think if you look\nat our brains, actually. Our brains are part\nzombie and part conscious. When I open my eyes, I immediately take all these pixels that hit on my retina, right? And I'm like, \"Oh, that's Lex.\" But I have no freaking clue\nof how I did that computation. It's actually quite complicated, right? It was only relatively recently, we could even do it well\nwith machines, right? You get a bunch of information processing happening in my retina and then it goes to the\nlateral geniculate nucleus in my thalamus, and the area V1, V2, V4, and the fusiform face area here, that Nancy Kanwisher at MIT invented, and blah, blah, blah, blah, blah. And I have no frigging clue\nhow that worked, right? It feels to me subjectively, like my conscious module just\ngot a little email saying, \"Facial processing task\ncomplete, it's Lex.\" - [Lex] Yeah. - And I'm gonna just go with that, right? So this fits perfectly\nwith Tononi's model, because this was all one-way\ninformation processing mainly. And it turned out for\nthat particular task, that's all you needed. And it probably was kind of the\nmost efficient way to do it. But there were a lot of other things that we associated with\nhigher intelligence and planning, and so on, and so forth, where you kind of wanna have loops and be able to ruminate and self-reflect, and introspect, and so on. Where my hunch is that\nif you want to fake that with a zombie system that\njust all goes one way, you have to like unroll those loops, and it gets really, really long, and it's much more inefficient. So I'm actually hopeful that AI, if in the future we have all these various sublime and interesting\nmachines that do cool things, and are aligned with us, that they will be at least, they will also have consciousness for kind of these things that we do. - That great intelligence is also correlated to great consciousness, or a deep kind of consciousness. - Yes, so that's a happy thought for me 'cause the zombie apocalypse really, is my worst nightmare of all. It would be like adding insult to injury, not only did we get replaced, but we frigging replaced\nourselves by zombies, like, how dumb can we be? - That's such a beautiful vision, and that's actually a provable one. That's one that we humans\ncan intuitively prove that those two things are correlated, as we start to understand what\nit means to be intelligent, and what it means to be conscious, which these systems, early AGI-like systems\nwill help us understand. And I just wanna say one more thing, which is super important. Most of my colleagues, when I started going\non about consciousness tell me that it's all bullshit and I should stop talking about it. I hear a little inner voice from my father and from my mom saying, \"Keep talking about it,\"\n'cause I think they're wrong. And the main way to\nconvince people like that, that they're wrong if they\nsay that consciousness is just equal to intelligence, is to ask them what's wrong with torture? Or why are you against torture? if it's just about, you know, these particles moving this\nway rather than that way, and there is no such thing\nas subjective experience, what's wrong with torture? I mean, do you have a\ngood comeback to that? - No, it seems like suffering. Suffering imposed unto\nother humans is somehow deeply wrong in a way that intelligence doesn't quite explain. - And if someone tells me, well, you know, it's just an illusion, consciousness, whatever, you know. I would like to invite them the next time they're having surgery, to do it without anesthesia. Like what is anesthesia really doing? If you have it, you can have a local\nanesthesia when you're awake. I have that when they\nfixed my shoulder, right? It's super entertaining. What was that that it did? it just removed my subjective\nexperience of pain. It didn't change anything about what was actually\nhappening in my shoulder, right? So if someone says, \"That's all bullshit,\" Skip the anesthesia, that's my advice. This is incredibly central. - It could be fundamental to whatever this thing we have going on here. - It is fundamental because we're, what we feel that's so fundamental, is suffering and joy, and\npleasure, and meaning, and, those are all subjective\nexperiences there. And let's not, those are the elephant in the room, that's what makes life worth living. And that's what can make it horrible if it's just a bunch of suffering. So let's not make the mistake of saying that that's all bullshit. - And let's not make the mistake of not instilling the AI systems with that same thing\nthat makes us special. - [Max] Yeah. - Max, it's a huge honor\nthat you would sit down to me the first time on the first\nepisode of this podcast. It's a huge honor you\nsit down with me again and talk about this, what I think is the most important topic, the most important problem that we humans have to\nface and hopefully solve. - Yeah, well, the honor is all mine and I'm so grateful to you for making more people aware of this fact that humanity has reached\nthe most important fork in the road ever in its history. And let's turn in the correct direction. - Thanks for listening\nto this conversation with Max Tegmark. To support this podcast. Please check out our\nsponsors in the description. And now let me leave you with some words from Frank Herbert. \"History is a constant race between invention and catastrophe.\" Thank you for listening, and hope to see you next time."
    }
}