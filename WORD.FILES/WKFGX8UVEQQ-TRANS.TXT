{
  "transcript": "open aai recently published a paper on the governance of agentic AI systems and this video will continue to be critical of open AI this is a big step in the right direction so the tldr of this paper is that they Define agentic systems as those that are able to perform complex tasks in complex environments with minimal supervision so those are the three primary criteria that open aai uses to Define agentic systems and this paper is about many kind of ways to approach this in terms of governance what to look for such as the ability to spend money uh they talk about you know unintended consequences of misinterpreting instructions the example that they gave is kind of funny which is uh imagine that you tell an AI agent that you want a Japanese cheesecake and it misunderstands and actually like buys you a ticket to Japan and so it ends up costing you like $2,500 when actually what you meant was a Japanese recipe now obviously models generally don't do that if you were to ask chat GPT for Japanese cheesecake right now it's not going to say oh well you should go to Japan I I hear that they have good cheesecake in Kyoto but the point being is that you do kind of have this monkeys paw possibility of misunderstanding instructions I did talk about he ability to understand and think through things particularly optimizing for costs in my book Symphony of thought these models are already capable of thinking through contingencies failure modes those sorts of things but you need to build those into the systems because is unless you tell the model to think through those things it's not going to so it's more of a prompting strategy and a meta prompting strategy anyways my point here though is that open aai is moving towards what I would consider the right direction and what I really want them to do and not just open Ai and I'll unpack why it's structurally impossible for one company such as open aai to get this right um and that's actually why they publish this paper they said that like in the introduction it literally says they want this to be a touch point for conversations so here I am boosting the signal and having the conversation so they're inching towards full autonomy talking about full agentic behavior and and that sort of thing I hope that they get to that and I suspect that the reason that open aai is not talking about full autonomy yet is due to the Overton window people are just not ready for it uh by and large especially when you look at the fact that out there in the wide world most people are still just not even aware of open Ai and GPT and then you've got you know bozos on the internet saying well it failed at this one corner case and therefore it's useless and I'm like whatever okay so when I say full autonomy when I say you know we should be researching full autonomy in AI what do I mean there are three primary criteria that I am talking about when I mentioned full autonomy right now so that is self-directing self-correcting and self-improving so self-directing means that you have a machine or you know AI model or whatever robot that makes all of its own decisions and judgment calls it sets all of its own goals and objectives now it should probably do that within the framework of some decision thing like an ethical framework or decision-making framework which I've done some work on that I call it theistic imperatives but I don't want to make this about my research because there's plenty of ways to skin that cat um but self-directing so self-correcting means that it should be able to detect and rectify errors of all kinds whether that's strategic logical errors technical errors flaws in Computing that sort of thing so basically self-awareness um and I don't mean I don't mean self-awareness in the philosophical sense I don't mean philosophical sentience what I mean is that we need to have systems and we need to understand the protocols and guidelines and Frameworks of what what kinds of data streams do these models and machines and robots need to be paying attention to on their in internally in order to find and rectify those flaws we already have some research into this in terms of prompting strategies so this is like a tree of decisions and those sorts of things so we're researching it from a prompt engineering perspective but of course models are not the the entire stack you know and having a model check itself is not necessarily the best way to go so we probably need to be researching ways to have multiple models checking each other which seems like it's the way that it's going anyways because of mixture of experts but there's also other other layers of the stack so there's the training data there is the hardware the software the models themselves and then if you have some models watching other models who's watching the Watchers do you have a circle of models all checking each other that's the that's kind of what I mean by self-correcting and then finally self-improving so self-improving means I and machines and robots that are capable of improving every layer of the stack Hardware software and models and that includes the training that and training data and that sort of thing so if we research those three pillars and we we come up with principles of okay how do we make sure that AI robots machines are able to do self-direct ction self-correction and self-improvement all in a stable and predictable trajectory we should be fine fewer and fewer people kind of worried about the control problem there are still doomers out there that say like AI is going to kill everyone but I think the I think the cycle has kind of moved on so it's like okay the Overton window has shifted saying AI is here how do we how do we do it safely so those are three criteria that I want to see being researched more deliberately kind of in a coherent framework there are a couple problems that I have with open ai's current approach um and so the first is the duality of intelligence so duality of intelligence basically means that neutering models is not a sustainable path for the long term uh the more intelligent a model is the more destructive potential it has um intelligence is just intrinsically dangerous and so you might continuously find edge cases and Corner cases but there's also for every every way that you try and hamstring a model there's going to be a new way to jailbreak it and it might actually get harder to curtail these abilities as models get smarter but also that assumes that good actors are going to be implementing this stuff as plenty of Point people have pointed out open source models are only a few months behind closed Source models and the first thing that happens is they're all jailbroken so you know yeah putting putting guard rails on a commercially available API great that is not a permanent or long-term solution outside of a commercial deployment and so when you you know we have to assume that in the future there are going to be super intelligent open source models that are fully jailbroken and uncensored that's just a fact of life that we're going to have to deal with which means we should be researching how to create self-detecting or self-directing self-correcting and self-improving models now so that we know how to do that and then also there is the uncertainty of corrigibility so this is something that I do agree with some of the doomers which is good luck controlling something that is a million times more intelligent than you yes right now they're just inert machines yes right now they rely on you know billion dooll data centers and you know super expensive gpus and a tremendous amount of power so we still have the power switch we should not assume that we will have the power switch forever into the future and so the combination of The Duality of intelligence and the uncertainty of corrigibility means that like right now while we have control is when we need to be researching full autonomy because in the long run what's going to protect us from an evil AI That's going haywire or maybe even not an evil AI but just something that is malfunctioning we need something that is benevolent that is good that is stable to help you know kind of level the playing field basically there's three primary concerns that I have with respect to failure to research full autonomy right now and this is why I'm making this video is because the clock is ticking and so there are three major dangers that emerge from failure to research full autonomy right now while we have the kill switch first inevitable technological progress It's it's barreling forward full steam ahead research is not just going fast it is accelerating as more and more Nations participate in this and this is like just think about like okay Russia China America Germany you know everyone is going to be working more towards fully autonomous AI systems we already know that the military is so like why are the rest of us not researching this because in order to understand this like it's just it's just going to keep going faster so number two is the lack of preparedness and oversight basically if we don't understand full autonomy now while it's in its infancy we will not have the necessary Frameworks guidelines and best practices in order to understand full autonomy by the time it's too late so the best time to start researching full autonomy is yesterday the second best time to start researching it is today and finally the the other biggest danger is just missed opportunities for Safety Research again uh you know as these more powerful AI systems emerge that are intrinsically more agentic that are intrinsically smarter we might not know what safety features to build into them or how that will upset the competitive landscape as Sam Alman recently said in an interview they got a lot wrong that kind of the story that he told was you know they tried the closed safety testing they did they did six months of safety testing and they missed a whole lot of stuff with you know the release of GPT 3.5 and GPT 4 there's problems with this cloistered research mentality for instance you know when you have closed door testing you're going to miss a lot of a lot of opportunities you're going to not see things that you know that the market does and this is what Sam Alman said which is once they released it you know all the users were far more creative and far more divergently thinking than they anticipated and so part of solving autonomy has to be in public it has to be done as an ongoing negotiation as an ongoing conversation because you have to release something and see what happens and so this incrementalism is the way to go forward and I think open AI has converged on that but I just want to kind of drive home the point that if they have then I would I would like to reinforce that uh blind spots so you know Silicon Valley Tech culture small teams and then the hubris of inventors everyone has blind spots including me including Sam Alman including Bill Gates like and you all of you watching we all have blind spots which means that it is just structurally impossible for a small closed door team to research full autonomy correctly and then finally there's incentive structures open AI Google Microsoft the Department of Defense everyone has a set of incentive structures and no matter how much we all believe that we are doing good work and that we are benevolently you know trying to better Humanity we all have incentive structures which sometimes are perverse incentive structures we are obligated to do things in a certain way we're beholden to investors to Partners to other stakeholders to our employers to whatever and and this is not this is not something that is unique to open AI or me or the government or whatever what I mean is that everyone has blind spots everyone has incentives and and so in that respect it is structurally impossible to do this in a closed door setting so what I want to do is close the video out by talking about the economic incentive so I've I've been talking about all the dangers with researching failing to research full autonomy so let's talk about like the advantages so first in adversarial environments whether that's cyber security or military or even just commercial environments people who stick the landing of full autonomy are going to have a huge Advantage so imagine you're the first company to have fully autonomous AI employees you're going to have a huge advantage over those that don't there's a pretty good profit motive to figure out how to do that self-directing self-correcting and self-improving right now another Advantage is decision fatigue the current research trajectory says that humans are going to need need to be in the loop humans are going to need to be supervising and guiding the machines but that sounds like a really boring job can you imagine just like pushing a button every day saying like Okay yes I approve that yes I approve that okay go do this go do that you know like I don't know like that just seems suboptimal that seems like not a good use of human brain power especially as AI gets better at supervising itself or supervising each other so we need to be be very deliberately trying to get humans out of the loop because decision fatigue is a very thing especially when you have machines that that don't get tired and humans do get tired very quickly especially when when you're engaging executive thought so the current plan of you know agentic machines that humans are just kind of steering you know like puppeteers that's not a sustainable pattern not just commercially but just from a psychological and neuroscience perspective that is not a good use of human executive function and then the the last kind of point about economic advantages or incentives to research full autonomy is shorter feedback Cycles so I've I've made a couple videos talking about polymorphic applications so imagine that you can shorten the feedback cycle of of your software platform of your work streams of everything else and you just get humans out of the loop Al together and these and your your AI agents whether they're software coders building you know you know your your software as a service platform whether they are you know working on self-driving that sort of stuff humans are slow and as these machines get better and faster then I think that it would behoove us to get humans out of the loop and this will accelerate Innovation and productivity even more cu the the you know humans are the biggest bottleneck now that says like okay well where do humans fit in that's a topic for another video but what I'm saying is there are tremendous competitive advantages economic advantages and safety advantages to studying full autonomy right now so if you take one thing away from this video that's what I want you to take away which is we need to be studying and researching and testing and experimenting with full machine autonomy ideally yesterday but today is a good second place so thanks for watching like subscribe etc etc you know the drill cheers",
  "duration": 14,
  "comments": []
}
