{
  "transcript": "in this video I'll demonstrate how to integrate a production ready third party API into your custom GPT we'll avoid repet or any other playground environment instead we'll focus on deploying a real serverless API using Firebase functions this ensures that you can scale to virtually any number of requests without ever worrying about your server shutting down and if you'd like to adapt this code for your own custom tools or logic I will show you exactly how to do that at the end of the video let's begin in this tutorial we'll connect Google analytics data API to our custom GPT to answer questions about my website's traffic this is a very common project that I often receive as an AI developer datadriven decisions are the key to success in 21st century but the problem is that most of these apis are not straightforward to work with first you have to create a correct cury then you have to know how to receive the results and third you have to know how to interpret them before you can make use of any of this data and GPT models streamline this entire process like never before instead of spending hours to analyze your data you can simply ask a question and get an instant response saving you or your client hours of time to get started create a new repo on GitHub and clone it then open it in your terminal and install the Firebase CLI you can do this with npm by running npm DG install Firebase tools if you don't have npm installed then guess what ask Chad next run Firebase init functions command to initialize Firebase functions in your current folder Firebase CLI manages all of the deployment aspects for you so you can focus solely on coding additionally they provide a generous free usage quota that in my three years of working with them I still haven't been able to exceed I do hope that this will change soon though when prompted in the terminal select create new project and enter your project ID and project name if needed wait until Google Cloud resources are created and then when asked about about which language you would like to use to write your functions select Chinese just kidding select python select yes to install the dependencies and now you should see a functions folder in your root directory which means that we are ready to start coding in requirements.txt file add two new dependencies the first one is the instructor Library which allows us to Define openi functions as identic schemas and the second one is Google analytics data package the second package is going to differ based on your own requirements you can chat with GPT 4 to find out which one you need depending on the API that you're going to use Now activate your virtual environment and run pip install dasr requirements.txt command open the main.py file uncomment the example function and rename it into something more descriptive since we are developing an internal API which means it is not intended to be used by anyone outside of your organization we can use a simple hardcoded token authentication creating a public app that uses o off and manages refresh and access tokens with custom gpts involves more complexity so if you're interested in learning about how to allow anyone to connect their accounts to your app on GPT store let me know in the comments nevertheless I believe that internal applications hold even greater potential compared to those on the GPT store and I will explain exactly why at the end of this video now we will add a simple DB token Global variable above this function to use for authentication which you can generate by asking chat GPT to generate a random token make sure not to publicly Commit This repo or use an environment variable instead to prevent your token from being exposed now to check authentication we will simply retrieve the token from the authorization header and then compare it with the database token that we have just defined if they don't match we will return a 401 response to initialize the Google analytics client import the beta analytics data client and then use the from service account file function I know that working with Google apis can be really confusing at first but luckily the general process is the same for almost all of them first you have to go to the Google Cloud console and then select your project then you have to find the API that you want to connect out of hundreds of options by the way then you have to enable this API then click on manage and get your service account Key by clicking on create credentials under the credentials tab enter your service account name and then click done click on this service account go to keys and click add key click create new key and then choose Json and click create after that go back to your project folder and drop this key into your fun functions directory again please make sure not to publicly commit this key on GitHub because I don't want to be implicated if someone hacks your organization on gcp then add the path to your service key into the from service account file function after that just one more step is required before your service account can access yours or your client's data through the API which is adding it to the user to the account itself since we are using Google analytics I'm going to open analytics.google.com and select the account that I want to analyze then I'm going to click on admin on the bottom left corner and click on account access management after that I'm going to click on add user under the plus icon and enter my service account email address from the Google Cloud console which typically starts with your service account name and ends with I AMG service.com this is the tricky part because the Curious will be performed by your service account not yourself so you have to make sure that it also has the necessary permissions for our use case the analyst roll should be more than enough you can also add your service account to as many Google apis as needed next go back to the admin on analytics console copy the property ID from the property details and insert it into our Cloud function as a variable in this example we will analyze just one property but you can use multiple properties if needed and allow GPT to decide which one to use this will allow you to analyze data across multiple websites or streams the next step is to go ahead and Define our openi function schema just like in the last tutorial we'll do this with the instructor library because it is a lot more convenient then Json we'll need a few models starting with the main one G4 cury params model I will not go too much into the details here because the schema will be different for each application or API endpoint that you create instead I will show you how to add a custom schema for your own purposes at the end of this video basically any dock strings or parameter descriptions will be passed to your custom GPT by using the instructor Library which it will then use to decide how to call your function therefore you should provide clear explanations of each functions purpose in the dock string and the role of each parameter in the field description other models in this schema like Dimension schema metric schema and date range schema Define the types for some of the properties in my main schema for example date ranges is a list of date range schemas because there can be multiple in this API check out the GitHub repoint description for more details but the most important part in this file is below the if name equals main statement this piece of code converts our function definitions into the open API schema that GP Builder expects after defining your functions you can simply run this file to get your open API schema printed in the terminal but for now let's finish the main logic in our main endpoint you can get your data from the request by using request.get Json method we'll then unpack it into our data model and set it as a variable another great benefit of using an instructor library is that if the request parameters passed by your GPT do not Feit your schema which often happens you will receive an error we can then catch this error with the accept statement and return it back back to GPT allowing it to correct itself on the next request if needed but if the data is good and it passes validation we can proceed to constructing a request body for the Google analytics data API this entails unpacking all the parameters into Google analytics classes and then calling the Run report and point itself the final step is to Simply return the data from the report as a string we are now ready to deploy our function to do this simply run Firebase deploy D- only functions command and wait until the deployment is complete that's it your first endpoint is now live and it can scale to virtually millions of users in production you should also see your endpoint URL at the very bottom copy this URL and go back to the schema. py file then replace it under service in your schema now run the schema file from the terminal again copy the output and let's jump into the GPT Builder to test everything out I'm going to name my GPT j4 data analyst in instructions I will tell it to curate the provided endpoint and interpret the data for the user then I'm going to hit add and insert our schema the last step is to add authentication which you can do by clicking the edit button and then selecting the API key type paste your DB token into the API key field select the bearer off type and hit save again let me know in the comments if you'd like me to explore a more complex or off option for public applications we are now ready to test the first question I'm going to ask my GPT is how many visitors were on my website this week as you can see the model immediately curies the endpoint and after allowing it to submit the request it now gets the data and interprets it for us I had a few active users this week most likely from YouTube hopefully we can increase this number soon next I'm going to ask the primary location of my website visitors for the year 2023 the main location appears to be United States which I'm not surprised about but this is already an extremely valuable insight and finally I'll ask how much time the users are spending on average on my site sometimes the model might not get the first request right but since we're passing back the arrow message it immediately corrects itself and then tells us that the engagement rate is moderate honestly I have no idea where to find this information on Google analytics myself so this is already extremely useful I'm going to save this GPT to use privately in the future if you want to share this GPT with your own organization or clients enter privacy policy URL it can be any random link or you can generate it with chpt and host it on a not taken app like notion and then click publish to only people with the link and share it now as I promised before I'm going to quickly guide you and show you how to add your own custom logic and create more endpoints for your custom GPT for example you can make an action to shorten URLs you can create a web scraper or you can connect to any database or a CRM system the possibilities here are endless the first step to do so would be to create a new function inside the main.py file and name it accordingly for example I going to do a simple calculator that adds two numbers then I'm going to Define my schema by copying the schema. P file into the root directory again I will name my data model add two numbers and the description for the model would be that this function simply adds two numbers the parameters would obviously be number one and number two both required integers without default values to make one or all of your parameters optional you can wrap them in an optional type from the type in default python library and provide default values then I'm going to replace the previous model in the name equals main block below now I just have to finish the main Logic for our endpoint in the main.py file the process is the same as before we unpack the request parameters into our model catch and return arrow if there are any and if not execute our logic and return the data as a response now we can run the deploy command again copy our new function URL into the schema example file under servers print our schema and paste it into our new action for our custom GPT that's it now it should be able to add two numbers without printing a block of unnecessary text in conclusion custom gpts is a huge opportunity but you have to approach it strategically if you think that you have a unique idea for the GPT store I can assure you that a thousand other people have the ex exact same idea and if it's really good open AI will most likely release it themselves on the other hand if you are developing a custom solution tailored for a specific business in my opinion you are much more likely to succeed open a rapper startups rarely satisfy all my clients business needs most of them require custom logic or guard rails to ensure that only specific data can be accessed or modified for example you can allow the model to store delet information in a client CRM while at the same time curing a Salesforce API to calculate the total revenue that the site can bring in all in one function without the need to create any user interface any longer so let me know in the comments what you want to see next I do read all of them and I try to respond as much as possible more videos on agent swarms certainly coming soon so thank you for watching and don't forget to subscribe",
  "duration": 12,
  "comments": []
}
